Master: 192.168.0.134
Slave 1: 192.168.0.132
Slave 2: 192.168.0.133



thierry@Ubuntu-thierry:~$ ssh thierry@192.168.0.134
The authenticity of host '192.168.0.134 (192.168.0.134)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.134' (ECDSA) to the list of known hosts.
thierry@192.168.0.134's password: 
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-72-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Dec 27 15:14:16 UTC 2019

  System load:  0.03              Users logged in:        0
  Usage of /:   50.4% of 9.78GB   IP address for enp0s3:  10.0.2.15
  Memory usage: 10%               IP address for enp0s9:  192.168.0.134
  Swap usage:   0%                IP address for docker0: 172.17.0.1
  Processes:    101

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

0 packages can be updated.
0 updates are security updates.


Last login: Fri Dec 27 15:13:24 2019
thierry@k8s-master:~$ sudo su
[sudo] password for thierry: 
root@k8s-master:/home/thierry# cd ~
root@k8s-master:~# kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.134
W1227 15:18:50.344974    4381 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL "https://dl.k8s.io/release/stable-1.txt": Get https://dl.k8s.io/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
W1227 15:18:50.345124    4381 version.go:102] falling back to the local client version: v1.17.0
W1227 15:18:50.345448    4381 validation.go:28] Cannot validate kube-proxy config - no validator is available
W1227 15:18:50.345474    4381 validation.go:28] Cannot validate kubelet config - no validator is available
[init] Using Kubernetes version: v1.17.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.0.134]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.134 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.0.134 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
W1227 15:19:20.253038    4381 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[control-plane] Creating static Pod manifest for "kube-scheduler"
W1227 15:19:20.258892    4381 manifests.go:214] the default kube-apiserver authorization-mode is "Node,RBAC"; using "Node,RBAC"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 17.507210 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.17" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node k8s-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 2693ts.tkmiic8k3ngfa96l
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
    --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb 
root@k8s-master:~# 
root@k8s-master:~# exit
exit
thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
thierry@k8s-master:~$ ll .kube/
total 16
drwxrwxr-x 2 thierry thierry 4096 Dec 27 15:25 ./
drwxr-xr-x 5 thierry thierry 4096 Dec 27 15:25 ../
-rw------- 1 thierry thierry 5449 Dec 27 15:25 config
thierry@k8s-master:~$ cat .kube/config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1USXlOekUxTVRreE5sb1hEVEk1TVRJeU5ERTFNVGt4Tmxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTEwxCmhWaGxVSjNnWGQ2VUdhTCtrL0VnaVc3cWtFbEVEcUltRFAzWmhCdEtsZzBUalZKMy91MTYxUjZ2eStielJYT3AKTitXeGh1OERKN0ZzZTJZWTIrUVZ1elpSZEtoOUUzOHZzSVVoWWZiNitsamJqaWNqUE1FeXVuaHZDZnpjekg3NApRcktUdDFsclFhS0haWnZybkpXOFk2dCtxWUhxRVdBRDdveEpkZzREaEtqbHVlQU9YRTdZVTkvY0pmZE9PZFZ3ClA3SnZxdjQ4WXdDRkU0YXBxUDErdEZjREU4ZVNjbS9MdjFYUUtDNUdwdTRJbEdySitlUnAya2RvWFdSZlUzejcKK1MwRFFxOTJodW1pZFR6WGtDeVFqQkVNWDh0VGZwaUNwNW1PMkxxY3NLMkRhbElGSTdTbkRrWUZSS3hjTzJnLwpwVFQ2aW9WUVl5VWFLMlVhQ2JzQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLczlyZytZZE1kRDB0eUJVM0Y1MktkdDNsV1cKWXFJKytXVkFTYlA3M05jd0UwNER5dHdyOG5sODUydWxWZ3ErV2t3ckFPMXBuOUM2R3U1amZRTUNURWU3WFB0Tgo5ZDc5U2JQS2c5QWNJbktCQVkzcVB2VjY2M05qOWljckl2L2YvRnJlbnRkenVlNEFyUUNiTzRoWWRLc0Y3NWxTCmJQM0k2cWVpNVNaWlpPMHY4NnVDcFpFM3FXeGY3b01US1RnRlhMdHBaOGppeHdHaWpGNzlCSHFpZWkyZDRBS3gKdWFvL2hJSWpSMkphYUhYQmRQbXd5RmQxajRlSXFLeUZQOUpxVjkzT1lYeU1NQ0dEQSs2ZC9ldDM1a2RCYmhvZgpmWnJ5QVp4ZEw0K3BWLzhGOExHQThDR1ZmLzd6dUcrRUpUZk1Sa0JDdjUzSzZ6dlJmcmdLcFl6clFVVT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    server: https://192.168.0.134:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJSnNLYTdJUjNDM013RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB4T1RFeU1qY3hOVEU1TVRaYUZ3MHlNREV5TWpZeE5URTVNVGxhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXhxaUJJUkhBd3NoUnBKRnEKMi9YMTNHR0Z0NHhGdk9QNXdGN2QzQ0R3c0RoWjBvR2FZYnFIaDFPOFBuYlNXTWdUNTZzQngzUzk2NWkzM09mTApyVUpxRUVqWXVTdU52dDVzZU5CQ1ZLTnBxR00yK1p5dkdnYW42ZC9rWW5kVHU0b1lCNHZMZXJrTGQyc1A0L05xClJvWkE1S1RuU2tha2hPK0E1OHBMSUp5RzhyWW4rcFpkalhhb0IwVGFRSjVMdTgzS09rTVNMcnFZVUhwdml6TEUKUnBSaHZUakQxSDdWRlI4VnBSMkxxeXlqbTRlNVhnT1B6bElFejlzUzRyZkRUMCt3V0hyK1Z2OGtpOTErZmF2RApmWjNJWnVXL3drVzRkR2ZBdWNiMUphbXpYZk1XbGRULzFhc1Nhbk1QUkxnSUVNTDRJRkJ4SXVnQzB0VDlrUjdSCkRRdWRid0lEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFHY1V3WWpOczRRTnU3ZkV1MHhLU21xWDRIdmxBSjJwK241awpXeC9FTG51MHBSNGV4R01nbWNsV000R3FhdG9ESHc1c0pVb0RjVHhBM3llUlNkZTQzK3dldTdISWltZlFKMXhYClp3YnpEb1ZyWVdrVm4vWkhPVVZXTm96QVFxQWVrQmZ5VG1JQVNiK1lpaTlTNUtmZ2NJVFN5SFBWUzRpV3RBcDIKWHZZODZnYitxOXBzc3g2bENlT05UcWRWc3hkclZRQ0JhbWRtN3JaeFVEQWRsVVNvL3dBaXBJdnNkbi85ampuMgozMHRpMTRxRHoranprMEpWcm9TNGR2eE0zOGNYRENaamlmQkwvZjVyc2YwUHVHcEQwWnAzUlNsUkVidXpXRVZPClA2WnNBaFhSWjROZnVSTXBMYjI5TVF2VGtVM1JSRzBiTGdrNUNSSm05SXZoUUMrZG80VT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: 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
thierry@k8s-master:~$ 

thierry@Ubuntu-thierry:~$ scp /projects/get-started/kubernetes/yaml/kube-flannel.yaml thierry@192.168.0.134:~/.
thierry@192.168.0.134's password: 
kube-flannel.yaml                                100%   14KB  10.8MB/s   00:00 


thierry@k8s-master:~$ ll
total 52
drwxr-xr-x 5 thierry thierry  4096 Dec 27 15:34 ./
drwxr-xr-x 3 root    root     4096 Dec  5 22:55 ../
-rw------- 1 thierry thierry   209 Dec 23 21:20 .bash_history
-rw-r--r-- 1 thierry thierry   220 Apr  4  2018 .bash_logout
-rw-r--r-- 1 thierry thierry  3771 Apr  4  2018 .bashrc
drwx------ 2 thierry thierry  4096 Dec  5 22:58 .cache/
drwx------ 3 thierry thierry  4096 Dec  5 22:58 .gnupg/
drwxrwxr-x 2 thierry thierry  4096 Dec 27 15:25 .kube/
-rw-r--r-- 1 thierry thierry   807 Apr  4  2018 .profile
-rw-r--r-- 1 thierry thierry     0 Dec  5 22:59 .sudo_as_admin_successful
-rw-r--r-- 1 thierry thierry 14416 Dec 27 15:34 kube-flannel.yaml
thierry@k8s-master:~$ kubectl apply -f kube-flannel.yaml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-bmkd4             0/1     Pending    0          16m   <none>      <none>       <none>           <none>
kube-system   coredns-6955765f44-kjbdn             0/1     Pending    0          16m   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-qzbwg          0/1     Init:0/1   0          20s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-c9mmr                     1/1     Running    0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          16m   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-bmkd4             0/1     Pending   0          16m   <none>      <none>       <none>           <none>
kube-system   coredns-6955765f44-kjbdn             0/1     Pending   0          16m   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-qzbwg          1/1     Running   0          31s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-c9mmr                     1/1     Running   0          16m   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          16m   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-bmkd4             1/1     Running   0          17m   10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-kjbdn             1/1     Running   0          17m   10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          17m   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          17m   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          17m   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-qzbwg          1/1     Running   0          81s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-c9mmr                     1/1     Running   0          17m   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          17m   10.0.2.15    k8s-master   <none>           <none>


thierry@Ubuntu-thierry:~$ mkdir -p $HOME/.kube
thierry@Ubuntu-thierry:~$ scp -r thierry@192.168.0.134:~/.kube/config ~/.kube/
thierry@192.168.0.134's password: 
config                                                                                                                                                                    100% 5449     6.4MB/s   00:00    
thierry@Ubuntu-thierry:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
[sudo] password for thierry: 
thierry@Ubuntu-thierry:~$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
thierry@Ubuntu-thierry:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.134:6443
KubeDNS is running at https://192.168.0.134:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   20m   v1.17.0
thierry@Ubuntu-thierry:~$ 
thierry@Ubuntu-thierry:~$ scp /projects/get-started/kubernetes/yaml/dashboard*
/projects/get-started/kubernetes/yaml/dashboard-v200b8-recommended.yaml: Not a directory
thierry@Ubuntu-thierry:~$ scp /projects/get-started/kubernetes/yaml/dashboard* thierry@192.168.0.134:~/
thierry@192.168.0.134's password: 
dashboard-adminrole.yaml                        100%  270   629.8KB/s   00:00
dashboard-adminuser.yaml                        100%  100   279.3KB/s   00:00
dashboard-v200b8-recommended.yaml               100% 7568    12.3MB/s   00:00


thierry@k8s-master:~$ ll
total 68
drwxr-xr-x 5 thierry thierry  4096 Dec 27 15:41 ./
drwxr-xr-x 3 root    root     4096 Dec  5 22:55 ../
-rw------- 1 thierry thierry   209 Dec 23 21:20 .bash_history
-rw-r--r-- 1 thierry thierry   220 Apr  4  2018 .bash_logout
-rw-r--r-- 1 thierry thierry  3771 Apr  4  2018 .bashrc
drwx------ 2 thierry thierry  4096 Dec  5 22:58 .cache/
drwx------ 3 thierry thierry  4096 Dec  5 22:58 .gnupg/
drwxrwxr-x 4 thierry thierry  4096 Dec 27 15:36 .kube/
-rw-r--r-- 1 thierry thierry   807 Apr  4  2018 .profile
-rw-r--r-- 1 thierry thierry     0 Dec  5 22:59 .sudo_as_admin_successful
-rw-r--r-- 1 thierry thierry   270 Dec 27 15:41 dashboard-adminrole.yaml
-rw-r--r-- 1 thierry thierry   100 Dec 27 15:41 dashboard-adminuser.yaml
-rw-rw-r-- 1 thierry thierry  7568 Dec 27 15:41 dashboard-v200b8-recommended.yaml
-rw-r--r-- 1 thierry thierry 14416 Dec 27 15:34 kube-flannel.yaml

thierry@k8s-master:~$ kubectl apply -f dashboard-v200b8-recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
kube-system            coredns-6955765f44-bmkd4                     1/1     Running             0          24m    10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running             0          24m    10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running             0          24m    10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running             0          24m    10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running             0          24m    10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running             0          8m7s   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running             0          24m    10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running             0          24m    10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   0/1     ContainerCreating   0          11s    <none>       k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        0/1     ContainerCreating   0          11s    <none>       k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system            coredns-6955765f44-bmkd4                     1/1     Running             0          24m     10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running             0          24m     10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running             0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running             0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running             0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running             0          8m11s   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running             0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running             0          24m     10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   0/1     ContainerCreating   0          15s     <none>       k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running             0          15s     10.244.0.4   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system            coredns-6955765f44-bmkd4                     1/1     Running   0          24m     10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running   0          24m     10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running   0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running   0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running   0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running   0          8m19s   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running   0          24m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running   0          24m     10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   1/1     Running   0          23s     10.244.0.5   k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running   0          23s     10.244.0.4   k8s-master   <none>           <none>
thierry@k8s-master:~$ 

thierry@k8s-master:~$ kubectl apply -f dashboard-adminuser.yaml
serviceaccount/admin-user created
thierry@k8s-master:~$ kubectl apply -f dashboard-adminrole.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

thierry@k8s-master:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-z67cg
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 233e08ca-097d-4635-b2ac-36d32b15c079

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkNQRldHb3pITVBiZzlhdlRmbDJRNjdPZ0E5YkhYbTlvT2RQRmVBR1phXzQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXo2N2NnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMzNlMDhjYS0wOTdkLTQ2MzUtYjJhYy0zNmQzMmIxNWMwNzkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.nnDOFLaeC_a63paCCZd7Rn6Wqxrz7k2kzY7uoUzraAG09ozGVk_bnOGq47nU7YQeCGLiz1RSOfZuVICeeQUICSogidosnMl7QRlCSDOn_ob-zyos4yqZbfUWRTJK0r7Df0S5A8UUBN3OQ4in24rQ0TpuqLNY3txbUE_Q25bPWm31hc6zxeRw7stAoa7_9wqptFCHu8mfhrM7M_o0RjvfcVRqanxK21qC6k9Vfc2sdFc2ibJGyByi9nuxEnvDBXtbk_vK4Kvp9HRg_fCixP7nChyoo2QHajJPjRRHe_-4WsDi0IJ5Qtw8japuxaTSMs1ne1-wxCoEfnWk_uEqDJu82g
ca.crt:     1025 bytes
namespace:  20 bytes

BEFORE LAUNCHING A PROXY ON A DIFFERENTE TERMINAL:

thierry@k8s-master:~$ curl http://localhost:8070/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
curl: (7) Failed to connect to localhost port 8070: Connection refused

WE LAUNCH A PROXY ON A DIFFERENT TERMINAL:

thierry@Ubuntu-thierry:~$ ssh thierry@192.168.0.134
thierry@192.168.0.134's password: 
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-72-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

  System information as of Fri Dec 27 15:48:56 UTC 2019

  System load:  0.47              Users logged in:        1
  Usage of /:   60.6% of 9.78GB   IP address for enp0s3:  10.0.2.15
  Memory usage: 38%               IP address for enp0s9:  192.168.0.134
  Swap usage:   0%                IP address for docker0: 172.17.0.1
  Processes:    145               IP address for cni0:    10.244.0.1

 * Overheard at KubeCon: "microk8s.status just blew my mind".

     https://microk8s.io/docs/commands#microk8s.status

0 packages can be updated.
0 updates are security updates.


Last login: Fri Dec 27 15:14:16 2019 from 192.168.0.23
thierry@k8s-master:~$ kubectl proxy -p 8070
Starting to serve on 127.0.0.1:8070


AND WE TRY TO CONNECT AGAIN:

thierry@k8s-master:~$ curl http://localhost:8070/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}

Obviously, we cann access the web server, but it fails for some reason...

WE NOW LAUNCH A PROXY FROM A DIFFERENT TERMINAL ON THE HOST:

thierry@Ubuntu-thierry:~$ kubectl proxy -p 8080
Starting to serve on 127.0.0.1:8080

thierry@Ubuntu-thierry:~$ curl http://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}

Quite consistently, it fails...




root@k8s-slave1:~# kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
>     --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb
W1227 16:29:41.035197    2219 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@k8s-slave1:~# 

thierry@k8s-slave2:~$ sudo su
[sudo] password for thierry: 
root@k8s-slave2:/home/thierry# cd ~
root@k8s-slave2:~# kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
>     --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb
W1227 16:29:51.078557    2309 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Downloading configuration for the kubelet from the "kubelet-config-1.17" ConfigMap in the kube-system namespace
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

root@k8s-slave2:~# 


thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   69m   v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   0s    v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   10s   v1.17.0
k8s-slave2   NotReady   <none>   1s    v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   29s   v1.17.0
k8s-slave2   Ready      <none>   20s   v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   70m   v1.17.0
k8s-slave1   Ready    <none>   31s   v1.17.0
k8s-slave2   Ready    <none>   22s   v1.17.0



thierry@Ubuntu-thierry:~$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1
deployment.apps/kubernetes-bootcamp created
thierry@Ubuntu-thierry:~$ kubectl get pods
NAME                                   READY   STATUS    RESTARTS   AGE
kubernetes-bootcamp-69fbc6f4cf-k2t92   1/1     Running   0          4m26s
thierry@Ubuntu-thierry:~$ kubectl get deployments
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
kubernetes-bootcamp   1/1     1            1           8m25s
thierry@Ubuntu-thierry:~$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES
kubernetes-bootcamp-69fbc6f4cf-k2t92   1/1     Running   0          17m   10.244.2.2   k8s-slave2   <none>           <none>
thierry@Ubuntu-thierry:~$ curl http://localhost:8080/version
{
  "major": "1",
  "minor": "17",
  "gitVersion": "v1.17.0",
  "gitCommit": "70132b0f130acc0bed193d9ba59dd186f0e634cf",
  "gitTreeState": "clean",
  "buildDate": "2019-12-07T21:12:17Z",
  "goVersion": "go1.13.4",
  "compiler": "gc",
  "platform": "linux/amd64"
}
thierry@Ubuntu-thierry:~$ export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
thierry@Ubuntu-thierry:~$ echo Name of the Pod: $POD_NAME
Name of the Pod: kubernetes-bootcamp-69fbc6f4cf-k2t92

thierry@Ubuntu-thierry:~$ kubectl describe pods
Name:         kubernetes-bootcamp-69fbc6f4cf-k2t92
Namespace:    default
Priority:     0
Node:         k8s-slave2/10.0.2.15
Start Time:   Fri, 27 Dec 2019 17:44:58 +0100
Labels:       app=kubernetes-bootcamp
              pod-template-hash=69fbc6f4cf
Annotations:  <none>
Status:       Running
IP:           10.244.2.2
IPs:
  IP:           10.244.2.2
Controlled By:  ReplicaSet/kubernetes-bootcamp-69fbc6f4cf
Containers:
  kubernetes-bootcamp:
    Container ID:   docker://aff46533626e6cfdff6b3ecef622e33eed0f00260465a6924d1f1620cca6c447
    Image:          gcr.io/google-samples/kubernetes-bootcamp:v1
    Image ID:       docker-pullable://gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 27 Dec 2019 17:45:13 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hk4xc (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-hk4xc:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-hk4xc
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age   From                 Message
  ----    ------     ----  ----                 -------
  Normal  Scheduled  29m   default-scheduler    Successfully assigned default/kubernetes-bootcamp-69fbc6f4cf-k2t92 to k8s-slave2
  Normal  Pulling    29m   kubelet, k8s-slave2  Pulling image "gcr.io/google-samples/kubernetes-bootcamp:v1"
  Normal  Pulled     29m   kubelet, k8s-slave2  Successfully pulled image "gcr.io/google-samples/kubernetes-bootcamp:v1"
  Normal  Created    29m   kubelet, k8s-slave2  Created container kubernetes-bootcamp
  Normal  Started    29m   kubelet, k8s-slave2  Started container kubernetes-bootcamp

thierry@Ubuntu-thierry:~$ curl http://localhost:8080/api/v1/namespaces/default/pods/$POD_NAME/proxy/
Error trying to reach service: 'dial tcp 10.244.2.2:80: i/o timeout'


thierry@Ubuntu-thierry:~$ kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
default                kubernetes-bootcamp-69fbc6f4cf-k2t92         1/1     Running   0          36m    10.244.2.2   k8s-slave2   <none>           <none>
kube-system            coredns-6955765f44-bmkd4                     1/1     Running   0          121m   10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running   0          121m   10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running   0          121m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running   0          121m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running   0          121m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-ctm9c                  1/1     Running   1          51m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running   0          105m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-x52cp                  1/1     Running   0          51m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-proxy-45csc                             1/1     Running   0          51m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running   0          121m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-z4vlc                             1/1     Running   0          51m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running   0          121m   10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   1/1     Running   0          97m    10.244.0.5   k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running   0          97m    10.244.0.4   k8s-master   <none>           <none>
thierry@Ubuntu-thierry:~$ kubectl describe node k8s-master
Name:               k8s-master
Roles:              master
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=k8s-master
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/master=
Annotations:        flannel.alpha.coreos.com/backend-data: {"VtepMAC":"66:05:13:96:ba:16"}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.0.2.15
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 27 Dec 2019 16:19:36 +0100
Taints:             node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  k8s-master
  AcquireTime:     <unset>
  RenewTime:       Fri, 27 Dec 2019 18:21:40 +0100
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 27 Dec 2019 18:19:51 +0100   Fri, 27 Dec 2019 16:19:31 +0100   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 27 Dec 2019 18:19:51 +0100   Fri, 27 Dec 2019 16:19:31 +0100   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 27 Dec 2019 18:19:51 +0100   Fri, 27 Dec 2019 16:19:31 +0100   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 27 Dec 2019 18:19:51 +0100   Fri, 27 Dec 2019 16:36:44 +0100   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  10.0.2.15
  Hostname:    k8s-master
Capacity:
  cpu:                2
  ephemeral-storage:  10252564Ki
  hugepages-2Mi:      0
  memory:             2041140Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  9448762967
  hugepages-2Mi:      0
  memory:             1938740Ki
  pods:               110
System Info:
  Machine ID:                 d8435cc8370a4b149f43d8f1ced04a0d
  System UUID:                2E2746B3-3C5C-4553-8D59-01197F972A0C
  Boot ID:                    9c1f8882-82b8-46a5-8fac-cc6073bb577d
  Kernel Version:             4.15.0-72-generic
  OS Image:                   Ubuntu 18.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://18.9.7
  Kubelet Version:            v1.17.0
  Kube-Proxy Version:         v1.17.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6955765f44-bmkd4                      100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     121m
  kube-system                 coredns-6955765f44-kjbdn                      100m (5%)     0 (0%)      70Mi (3%)        170Mi (8%)     121m
  kube-system                 etcd-k8s-master                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m
  kube-system                 kube-apiserver-k8s-master                     250m (12%)    0 (0%)      0 (0%)           0 (0%)         122m
  kube-system                 kube-controller-manager-k8s-master            200m (10%)    0 (0%)      0 (0%)           0 (0%)         122m
  kube-system                 kube-flannel-ds-amd64-qzbwg                   100m (5%)     100m (5%)   50Mi (2%)        50Mi (2%)      105m
  kube-system                 kube-proxy-c9mmr                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         121m
  kube-system                 kube-scheduler-k8s-master                     100m (5%)     0 (0%)      0 (0%)           0 (0%)         122m
  kubernetes-dashboard        dashboard-metrics-scraper-76585494d8-8qztj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m
  kubernetes-dashboard        kubernetes-dashboard-5996555fd8-2gfcr         0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests     Limits
  --------           --------     ------
  cpu                850m (42%)   100m (5%)
  memory             190Mi (10%)  390Mi (20%)
  ephemeral-storage  0 (0%)       0 (0%)
Events:              <none>
thierry@Ubuntu-thierry:~$ 

thierry@Ubuntu-thierry:~$ kubectl get namespace
NAME                   STATUS   AGE
default                Active   129m
kube-node-lease        Active   129m
kube-public            Active   129m
kube-system            Active   129m
kubernetes-dashboard   Active   104m
thierry@Ubuntu-thierry:~$ echo $POD_NAME
kubernetes-bootcamp-69fbc6f4cf-k2t92
thierry@Ubuntu-thierry:~$ curl http://localhost:8080/api/v1/namespace/default/pods/$POD_NAME/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}


thierry@Ubuntu-thierry:~$ kubectl create deployment hello --image=gcr.io/google-samples/node-hello:1.0 
deployment.apps/hello created

thierry@Ubuntu-thierry:~$ kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS              RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
default                hello-7747bc55f-t6ddl                        0/1     ContainerCreating   0          36s    <none>       k8s-slave1   <none>           <none>
default                kubernetes-bootcamp-69fbc6f4cf-k2t92         1/1     Running             0          77m    10.244.2.2   k8s-slave2   <none>           <none>
kube-system            coredns-6955765f44-bmkd4                     1/1     Running             0          162m   10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running             0          162m   10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running             0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running             0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running             0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-ctm9c                  1/1     Running             1          92m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running             0          146m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-x52cp                  1/1     Running             0          93m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-proxy-45csc                             1/1     Running             0          92m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running             0          162m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-z4vlc                             1/1     Running             0          93m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running             0          163m   10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   1/1     Running             0          138m   10.244.0.5   k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running             0          138m   10.244.0.4   k8s-master   <none>           <none>
thierry@Ubuntu-thierry:~$ kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
default                hello-7747bc55f-t6ddl                        1/1     Running   0          61s    10.244.1.2   k8s-slave1   <none>           <none>
default                kubernetes-bootcamp-69fbc6f4cf-k2t92         1/1     Running   0          78m    10.244.2.2   k8s-slave2   <none>           <none>
kube-system            coredns-6955765f44-bmkd4                     1/1     Running   0          163m   10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running   0          163m   10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running   0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running   0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running   0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-ctm9c                  1/1     Running   1          93m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running   0          147m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-x52cp                  1/1     Running   0          93m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-proxy-45csc                             1/1     Running   0          93m    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running   0          163m   10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-z4vlc                             1/1     Running   0          93m    10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running   0          163m   10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   1/1     Running   0          139m   10.244.0.5   k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running   0          139m   10.244.0.4   k8s-master   <none>           <none>


thierry@Ubuntu-thierry:~$ curl http://localhost:8080/api/v1/pods/ | grep kubernetes-bootcamp
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0        "name": "kubernetes-bootcamp-69fbc6f4cf-k2t92",
        "generateName": "kubernetes-bootcamp-69fbc6f4cf-",
        "selfLink": "/api/v1/namespaces/default/pods/kubernetes-bootcamp-69fbc6f4cf-k2t92",
          "app": "kubernetes-bootcamp",
            "name": "kubernetes-bootcamp-69fbc6f4cf",
            "name": "kubernetes-bootcamp",
            "image": "gcr.io/google-samples/kubernetes-bootcamp:v1",
            "name": "kubernetes-bootcamp",
            "image": "gcr.io/google-samples/kubernetes-bootcamp:v1",
            "imageID": "docker-pullable://gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af",
100  107k    0  107k    0     0  8260k      0 --:--:-- --:--:-- --:--:-- 8260k
thierry@Ubuntu-thierry:~$ curl /api/v1/namespaces/default/pods/kubernetes-bootcamp-69fbc6f4cf-k2t92
curl: (3) <url> malformed
thierry@Ubuntu-thierry:~$ curl http://localhost:8080/api/v1/namespaces/default/pods/kubernetes-bootcamp-69fbc6f4cf-k2t92
{
  "kind": "Pod",
  "apiVersion": "v1",
  "metadata": {
    "name": "kubernetes-bootcamp-69fbc6f4cf-k2t92",
    "generateName": "kubernetes-bootcamp-69fbc6f4cf-",
    "namespace": "default",
    "selfLink": "/api/v1/namespaces/default/pods/kubernetes-bootcamp-69fbc6f4cf-k2t92",
    "uid": "a189ecfb-d15c-4821-a85d-f0f78e84ebff",
    "resourceVersion": "12202",
    "creationTimestamp": "2019-12-27T16:44:58Z",
    "labels": {
      "app": "kubernetes-bootcamp",
      "pod-template-hash": "69fbc6f4cf"
    },
    "ownerReferences": [
      {
        "apiVersion": "apps/v1",
        "kind": "ReplicaSet",
        "name": "kubernetes-bootcamp-69fbc6f4cf",
        "uid": "93668656-01ef-437f-8088-b5e863a0289b",
        "controller": true,
        "blockOwnerDeletion": true
      }
    ]
  },
  "spec": {
    "volumes": [
      {
        "name": "default-token-hk4xc",
        "secret": {
          "secretName": "default-token-hk4xc",
          "defaultMode": 420
        }
      }
    ],
    "containers": [
      {
        "name": "kubernetes-bootcamp",
        "image": "gcr.io/google-samples/kubernetes-bootcamp:v1",
        "resources": {
          
        },
        "volumeMounts": [
          {
            "name": "default-token-hk4xc",
            "readOnly": true,
            "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount"
          }
        ],
        "terminationMessagePath": "/dev/termination-log",
        "terminationMessagePolicy": "File",
        "imagePullPolicy": "IfNotPresent"
      }
    ],
    "restartPolicy": "Always",
    "terminationGracePeriodSeconds": 30,
    "dnsPolicy": "ClusterFirst",
    "serviceAccountName": "default",
    "serviceAccount": "default",
    "nodeName": "k8s-slave2",
    "securityContext": {
      
    },
    "schedulerName": "default-scheduler",
    "tolerations": [
      {
        "key": "node.kubernetes.io/not-ready",
        "operator": "Exists",
        "effect": "NoExecute",
        "tolerationSeconds": 300
      },
      {
        "key": "node.kubernetes.io/unreachable",
        "operator": "Exists",
        "effect": "NoExecute",
        "tolerationSeconds": 300
      }
    ],
    "priority": 0,
    "enableServiceLinks": true
  },
  "status": {
    "phase": "Running",
    "conditions": [
      {
        "type": "Initialized",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-27T16:44:58Z"
      },
      {
        "type": "Ready",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-27T16:45:14Z"
      },
      {
        "type": "ContainersReady",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-27T16:45:14Z"
      },
      {
        "type": "PodScheduled",
        "status": "True",
        "lastProbeTime": null,
        "lastTransitionTime": "2019-12-27T16:44:58Z"
      }
    ],
    "hostIP": "10.0.2.15",
    "podIP": "10.244.2.2",
    "podIPs": [
      {
        "ip": "10.244.2.2"
      }
    ],
    "startTime": "2019-12-27T16:44:58Z",
    "containerStatuses": [
      {
        "name": "kubernetes-bootcamp",
        "state": {
          "running": {
            "startedAt": "2019-12-27T16:45:13Z"
          }
        },
        "lastState": {
          
        },
        "ready": true,
        "restartCount": 0,
        "image": "gcr.io/google-samples/kubernetes-bootcamp:v1",
        "imageID": "docker-pullable://gcr.io/google-samples/kubernetes-bootcamp@sha256:0d6b8ee63bb57c5f5b6156f446b3bc3b3c143d233037f3a2f00e279c8fcc64af",
        "containerID": "docker://aff46533626e6cfdff6b3ecef622e33eed0f00260465a6924d1f1620cca6c447",
        "started": true
      }
    ],
    "qosClass": "BestEffort"
  }
}
thierry@Ubuntu-thierry:~$ 






