#############################################################################
#                                                                           #
# Learn Kubernetes Basics - Part 2 - Create a cluster                       #
#                                                                           #
#############################################################################


Let's now refine a bit few concepts on the node, and get these concepts in 
action in setting up and deploying a cluster!


====================================
1 - Naming conventions & unique data
====================================


In this section, we will build step by step the infrastructure on which we 
will deploy a Kubernetes cluster. We intentionaly make every step manual, so 
that you can appreciate the full process at leat once in your lifetime. In 
real production world, most of these steps are automated and you would simply 
"push a button".

The steps are:
    - building an initial  VM image ("K8s BARE") with all the prerequisites 
      installed (Ubuntu OS, docker, kubernetes)
    - renaming a VM to be the master node ("K8s master - cluster not 
      deployed"), derived from "K8S BARE"
    - renaming two 'slave' VMs ("K8s slave - cluster not deployed") derived 
      from "K8s BARE"
    - initialize and deploy a Kubernetes on the master, and join the two 
      slaves into the cluster (all VMs are then called "cluster deployed")
    - setup a dashboard and access it


Note: in order to save time, you can jump directly as the section 2.4 and set 
    up the cluster from VM images which are properly configured. The VM images 
    are named:
        2019-12-23 - K8s master - not configured
        2019-12-23 - K8s slave 1 - not configured
        2019-12-23 - K8s slave 2 - not configured



###
###      WARNING
###
### Few informations will be unique to each deployement and you need to 
### carefully copy and save these informations in order to adapt the commands 
### as shown in the tutorial (i.e. replace the tutorial's info with yours) and 
### run your own version of the commands:
###
### 1) the ip@ of the VMs
###
### In the tutorial, I used (copied from a real execution):
###    master:  192.168.0.134
###    slave 1: 192.168.0.132
###    slave 2: 192.168.0.133
### The ip@ granted to each VM by the host machine will vary from execution of
### this tutorial to another, so please collect the ip@as you will start the 
### VMs and keep this information for the whole duration of the tutorial.
###
### 2) the token required for a node to join into the cluster
###
### In the following examples, I used:
###
### kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
    --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb 
###
### YOU WILL NEED TO USE YOUR OWN INFORMATION AS SHOWN IN SECTION 4
###

=========================
2 - Building the first VM
=========================

(you can skip this phase and use directly the "K8s BARE" image)

Nothing fancy here: we've built from the Ubuntu Server 18.04 LTS, with:
  - two CPUs (it is mandatory to run a Kubernetes master)
  - 2048 MB memory
  - two network interfaces:
       * one NAT (which is default setting) for internet connectivity
       * one configured in a "Bridge Adapter" mode, pointing on the most 
         relevant network adapter (in my case: the Ethernet card on the 
         desktop, or the wifi card on the laptop)

(image "K8s BARE - 0 - Description")
(image "K8s BARE - 1 - Memory")
(image "K8s BARE - 2 - CPU")
(image "K8s BARE - 3 - 1st Network interface")
(image "K8s BARE - 4 - 2nd Network interface")

In order to make this VM a generic basis on which we can run any type of k8s
nodes, we then need to log as root and (in this order):
  - to install docker
  - to disable the swap
  - to install kubernetes

For convenience, in this tutorial, we will configure the VMs by connecting 
into them via ssh from terminals running on the host machine, using the 
default admin profile (login=thierry, password=thierry) (Secure, huh!!!). 
There are few advantages doing so, namely the fact that the prompt then tells 
on which machine you are actually logged, which is very convenient when you 
will have 3 VMs running at the same time.

To do so, you need to identify the ip@ of the VM: it will be displayed on 
your first loging into the VM from the VM window:

(image "K8s BARE - 5 - Login from the VM")

Open a terminal and - as a regular user on the host machine, ssh into the VM:
(in my case, the hostname of my laptop is 'laptop' and my login is 'tso')

tso@laptop:~$ ssh thierry@192.168.0.134

The first time you connect on a remote machine, you're asked to confirm: 
confirm by typing 'yes' and then enter the password:

The authenticity of host '192.168.0.134 (192.168.0.134)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.134' (ECDSA) to the list of known hosts.
thierry@192.168.0.134's password: 

You are now connected on the VM and your prompt will show it:

(K8s BARE - 6 - Login from a terminal)

thierry@k8s_node:~$

You are logged as 'thierry' (the name of the account on the VM) and not 
anymore as 'tso' (the name of the account on the host machine) and you are on 
'k8s_node'* and not on 'laptop' anymore.

*: the initial name of the machine is whatever you have specified when 
building the fist image from the Ubuntu OS in VirtualBox. I used the generic 
name 'k8s_node' because the same VM will later be used to configure both 
master and slave nodes.


We must do the following steps as 'root':

thierry@k8s_node:~$ sudo su

The prompt becomes 'root@k8s_node:/#' and we can continue the next steps.

We will install docker: collect the certificates, add the signing key to the 
local apt repository, add the docker repository... and finally install docker:

root@k8s_node:/# apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common
root@k8s_node:/# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb https://download.docker.com/linux/debian stretch stable"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y docker-ce

Then we must disable the swap: we do so immediately with the "swappoff" 
command:

root@k8s_node:/# swapoff -a

and we also prevent the swap to be re-activated at next boot, by editing the 
fstab file (/etc/fstab) and disable the line indicating the swap partition 
by commenting it (i.e. add a '#' at the beginning of the line). Thus, the 
original file changes from:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
/swap.img	none	swap	sw	0	0

to:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
#/swap.img	none	swap	sw	0	0


We will then install Kubernetes: collect the certificates, add the signing 
key to the local apt repository, add the google kubernetes repository... and 
finally install kubernetes:

root@k8s_node:/# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y kubelet kubeadm kubectl


Here we are !!!
The generic VM is setup, and we can use this basis for crafting teh master 
and slave nodes. As a matter of precaution, you should log out, go in 
VirtualBox and shutdown the VM (softly), take a snapshot of the VM and export 
it as and appliance under the name of  "K8s BARE".


=======================================
3 - Building the master and slave nodes
=======================================

In this section, we will configure one master node and two slave nodes 
from "K8s BARE". To do so, we will clone K8s_BARE (which VirtualBox does very
easily).

In order to reduce the memory footprint of the tutorial, choose 'linked clone'
rather than 'full clone', since each VM virtual disk is more than 3.5GB (and
the laptop does not have infinite storage).

(K8s BARE - 7 - Cloning)

And you should get something looking like this:

(K8s BARE - 8 - Cloned machines)

Start the three VMs in VirtualBox, and as shown in the previous section, 
log in, identify the ip@, and connect on these VMs from terminals running on 
the host machine.

Since almost all prerequisites were already installed in the previous phase, 
the task here merely consists in renaming the machines so that a) you know on
which machine you are logged, and b) you prevent naming conflicts within the
kubernetes cluster later on (since every node must have a unique name in the 
cluster).

So we will edit two files: /etc/hosts and /etc/hostname

Log into the first VM (in our case 'ssh thierry@192.168.0.134) and edit the
file /etc/hostname: replace the previous host name (k8s-node) with the new
host name 'k8s-master'.

Then edit the file /etc/hosts and add the new name 'k8s-master' after
'127.0.0.1   localhost'.

After edition, it should look like this:

thierry@k8s-master:~$ cat /etc/hostname 
k8s-master

thierry@k8s-master:~$ cat /etc/hosts
127.0.0.1 localhost k8s-master
127.0.1.1 k8s_node

Log out and log in again, and the prompt should look like:

thierry@k8s-master:~$ 

Then do so on the two other machines (respectively at 192.168.0.132 and 
192.168.0.133) with the new host names 'k8s-slave1' and 'k8s-slave2'. The 
prompt on these two machines should look like:

thierry@k8s-slave1:~$ 

and

thierry@k8s-slave2:~$ 

Interestingly, the prompt will always tell you on which machine and which 
account you are a given moment in time, which will be important in order not 
to do too many mistakes while we will configure the Kubernetes cluster.

That's it! We now have 3 machines running on the laptop, with all the 
prerequisites installed and the right configuration to make them a master 
and two slaves.

We can test the connectivity between the machines on the laptop (ping from 
one machine to the other) to check that there is no problem, and then go to
the next section.


==============================================
4 - Configure the Master and start the cluster
==============================================

To start with, we will initiate the cluster on the Master node.

SSH onto the Master node and log as root. To initiate the cluster, we will use 
'kubeadm' with the 'init' command. We specify two arguments:
    - specify the 'internal cluster network' which will enable the nodes to 
      communicate one with the other. This internal network uses the second 
      network interface declared in VirtualBox (the one in bridge mode, and
      use a dedicated internal IP range (10.244.0.0-255 in this example).
    - specifiy that the master node must advertise its own IP@ in order to 
      enable the slave nodes to first connect to the master node.

root@k8s-master:/# kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.0.134

The whole setup can take a bit of time, and it should conclude with displaying 
the following message (truncated):

[init] Using Kubernetes version: v1.17.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[...]
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
    --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb 


Here you are: your Kubernetes Master has initialized successfully!
You can see that the message displayed during the initialisation process 
includes three very important directives:
    - you need to create on each machine from which you will manage the cluster 
      (in our case, on the host and on the master) a .kube directory and 
      configuration file to be able to operate the cluster as a regular user 
      (i.e. not root)
    - you need to actually deploy the network which will connect all the pods 
      together, and you will have to choose one of the possible network 
      configurations.
    - you are given the information needed for a slave node to join the 
      cluster by giving the right token and the public key of the master.

So, let's follow these directives.


4.1 - Copy the .kube/config file on the Master
==============================================

The next step is to configure the 'regular' user (i.e. not root) on the 
master node who will manage the cluster, and on the host machine. First we 
logout as 'root' but we stay logged as 'thierry' on the master, and as a 
'thierry' we enter the commands given in the initialization message:

thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Here you are, the config file is properly copied in the .kube directory, with 
appropriate rights:

thierry@k8s-master:~$ ll .kube/
total 16
drwxrwxr-x 2 thierry thierry 4096 Dec 27 15:25 ./
drwxr-xr-x 5 thierry thierry 4096 Dec 27 15:25 ../
-rw------- 1 thierry thierry 5449 Dec 27 15:25 config
thierry@k8s-master:~$ cat .kube/config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FUR [...] S0tLQo=
    server: https://192.168.0.134:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FUR [...] S0tLQo=
    client-key-data: LS0tLS1CRUdJTiBSU0EgUFJJVkFURSBLRVktL [...] S0tCg==

(all tokens are truncated)

As you can see, the config file contains teh credentials required to act as an 
admin on the cluster. Once again, this would not happen this way in a 
production environment...


4.2 - Deploy a Kubernetes network model
=======================================

We now must deploy the network within the cluster. For the sake of simplicity, 
we will use the 'flannel' network configuration (not because it is better than 
others, but because I know it is simple to deploy and make work).

The configuration file for the 'flannel' network can be directly retrieved 
from the kubernetes github repository, and a copy is available in the 'yaml' 
sub-directory.

From the host machine, we copy the configuration file on the Master node:

tso@laptop:~$ scp /projects/get-started/kubernetes/yaml/kube-flannel.yaml thierry@192.168.0.134:~/.
thierry@192.168.0.134's password: 
kube-flannel.yaml                                100%   14KB  10.8MB/s   00:00

Then back from the master, we deploy this network configuration:

thierry@k8s-master:~$ kubectl apply -f kube-flannel.yaml
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

Once you have received this message, the master will need a bit of time to 
process the deployment as it will actually spawn new pods in charge of 
operating the network. You can see the progress by asking several times to 
show the pods running on the master with the command:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide

See the results at several seconds distance, and observe that the pods status
evolve as their get from 'Pending' to 'ContainerCreating' and finally to 
'Running'.

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          14s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          2m58s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running             0          20s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running             0          3m4s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Running   0          3m7s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Running   0          3m7s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          23s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m7s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             1/1     Running   0          3m16s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             1/1     Running   0          3m16s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          32s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>


This is actually interesting as it reveals one key nature of Kubernetes: it
will enable you to run containerized applications, but it relies itself on 
launching containers (i.e. containerized applications) to do its job. As your
can see above, it launches a DNS application wich will serve to connect pods
within the cluster: this DNS application is itself running in a container, 
managed as a pod.

Looking at the list of the pods, you see that Kubernetes launched several 
applications: an API server, a proxy, a scheduler, a datastore (etcd), two
DNS... and a controller manager. We will see abit later all these functions.


4.3 - Remotely manage the cluster as a regular user from the host
=================================================================

Now we need to enable the host machine to actually manage the Kubernetes 
cluster, and in order to do so, we must get back to the host and create the 
same .kube directory with the same config file (using the same commands as the
ones we used to create the directory and file on the master node). Log again as 
'tso' on the laptop, and remote copy this .kube directory from the master to 
the laptop:

tso@laptop:~$ mkdir -p $HOME/.kube
tso@laptop:~$ scp -r thierry@192.168.0.134:~/.kube/config ~/.kube/
tso@laptop:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Here you are: you now can manage the Kubernetes cluster from the host machine. 
To check that the cluster is alive, you can run the version command:

tso@laptop:~$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}

and view the cluster details:

tso@laptop:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.63:6443
KubeDNS is running at https://192.168.0.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

tso@laptop:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   9m      v1.17.0
tso@laptop:~$ kubectl get services -o wide

As you can see, the master is still alone; you have not yet joined a slave 
into the cluster.


=======================
5 - Setup the dashboard
=======================

Dashboard is a web-based Kubernetes user interface. You can use Dashboard to 
deploy containerized applications to a Kubernetes cluster, troubleshoot your 
containerized application, and manage the cluster resources. You can use 
Dashboard to get an overview of applications running on your cluster, as well 
as for creating or modifying individual Kubernetes resources (such as 
Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, 
initiate a rolling update, restart a pod or deploy new applications using a 
deploy wizard.

Dashboard also provides information on the state of Kubernetes resources in 
your cluster and on any errors that may have occurred.

(image: 'dashboard-general_view')

As Kubernetes exposes all its capability via REST APIs, the dashboard 
application uses these APIs and shows graphically the information (exactly 
the same way 'kubectl' does it with the CLI). The dashboard is itself a 
containerized application running as a 'service' on the K8s cluster: it is 
both using the cluster as its own infrastructure, and use the cluster's APIs 
to monitor  and manage it.


5.1 - Deploy the dahsboard UI
=============================

The Dashboard UI is not deployed by default. To deploy it, run the following 
command:

First copy the configuration files from the 'yaml' sub-directory to the 
Master, with teh following command:

tso@laptop:~$ scp /projects/get-started/kubernetes/yaml/dashboard* thierry@192.168.0.134:~/
thierry@192.168.0.134's password: 
dashboard-adminrole.yaml                        100%  270   629.8KB/s   00:00
dashboard-adminuser.yaml                        100%  100   279.3KB/s   00:00
dashboard-v200b8-recommended.yaml               100% 7568    12.3MB/s   00:00

You can verify that the 3 files are well copies on the home directory on the 
Master:

thierry@k8s-master:~$ ll
total 68
drwxr-xr-x 5 thierry thierry  4096 Dec 27 15:41 ./
drwxr-xr-x 3 root    root     4096 Dec  5 22:55 ../
-rw------- 1 thierry thierry   209 Dec 23 21:20 .bash_history
-rw-r--r-- 1 thierry thierry   220 Apr  4  2018 .bash_logout
-rw-r--r-- 1 thierry thierry  3771 Apr  4  2018 .bashrc
drwx------ 2 thierry thierry  4096 Dec  5 22:58 .cache/
drwx------ 3 thierry thierry  4096 Dec  5 22:58 .gnupg/
drwxrwxr-x 4 thierry thierry  4096 Dec 27 15:36 .kube/
-rw-r--r-- 1 thierry thierry   807 Apr  4  2018 .profile
-rw-r--r-- 1 thierry thierry     0 Dec  5 22:59 .sudo_as_admin_successful
-rw-r--r-- 1 thierry thierry   270 Dec 27 15:41 dashboard-adminrole.yaml
-rw-r--r-- 1 thierry thierry   100 Dec 27 15:41 dashboard-adminuser.yaml
-rw-rw-r-- 1 thierry thierry  7568 Dec 27 15:41 dashboard-v200b8-recommended.yaml
-rw-r--r-- 1 thierry thierry 14416 Dec 27 15:34 kube-flannel.yaml


thierry@k8s-master:~$ kubectl apply -f dashboard-v200b8-recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created

The configuration YAML file is also available in the 'yaml' directory, under 
the name 'dashboard-v200b8-recommended.yaml'.

You can check that the dashboard is actually running by checking the pods from
the host machine:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system            coredns-6955765f44-md48q                     1/1     Running   0          18m     10.244.0.3   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-wklk8                     1/1     Running   0          18m     10.244.0.2   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running   0          19m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running   0          19m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running   0          19m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-5wktw                  1/1     Running   0          5m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-flannel-ds-amd64-8t4wm                  1/1     Running   0          11m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-9cpvn                  1/1     Running   0          5m9s    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-proxy-b5tvt                             1/1     Running   0          5m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-proxy-hfdfh                             1/1     Running   0          18m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-w2c4m                             1/1     Running   0          5m9s    10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running   0          19m     10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-9dqkl   1/1     Running   0          24s     10.244.1.2   k8s-slave1   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-bwcjx        1/1     Running   0          24s     10.244.2.2   k8s-slave2   <none>           <none>

Here you can see that a pod is running with the name 'dashboard'. However, 
even if the dashboard is actually running, you still cannot access it. To get 
there, we first need to setup a 'user' with the proper privileges and roles.


5.2 - Create a sample user to access the dashboard
==================================================

To protect your cluster data, Dashboard deploys with a minimal RBAC ('Role 
Based Access Control') configuration by default. Currently, Dashboard only 
supports logging in with a Bearer Token. To create a token for this demo, you 
can follow this guide, and create a sample user.


    Note: This is ok for this tutorial, but it is NOT the right way to proceed 
        in a production environment. In order to go fast and easily to a 
        visible outcome, we create here a user whi will inherit administrative 
        privileges, which is dangerous in a real environment. Like many 
        shortcuts in this tutorial, please do not assume that this is 
        representative of a 'real' environment.

We will create the user it by writing a yaml manifest describing the user, 
and then a second manifest describing its roles and provoleges. We will then 
apply these two manifests to the cluster via the kubectl command.

A user's role/privilege is always linked to a set of resources: as usual in Kubernetes, we actually 

First we create the 'Service Account', which mean the account (= the user 
profile) which will be related to the service (here the dashboard, which is a 
Kubernetes service).
    - The name of the account is 'admin-user'.
    - We define a 'namespace' on which is it valid: its valid eclare it valid in the 
namespace 'kubernetes-dashboard' first.

(file: dashboard-adminuser.yaml)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard

Then run the command:
thierry@k8s-master:~$ kubectl apply -f dashboard-adminuser.yaml
serviceaccount/admin-user created

5.3 - Bind the role of the sample user to the Cluster admin
===========================================================

In most cases after provisioning our cluster using kops or kubeadm or any 
other popular tool, the ClusterRole admin-Role already exists in the cluster. 
We can use it and create only ClusterRoleBinding for our ServiceAccount.

    NOTE: apiVersion of ClusterRoleBinding resource may differ between 
        Kubernetes versions. Prior to Kubernetes v1.8 the apiVersion was rbac.authorization.k8s.io/v1beta1.

(file: dashboard-adminrole.yaml)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

Then run the command:
thierry@k8s-master:~$ kubectl apply -f dashboard-adminrole.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created


5.4 - Collect the Bearer Token
==============================

Now we need to find THE token we can use to log in. Execute following command:

thierry@k8s-master:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-z67cg
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 233e08ca-097d-4635-b2ac-36d32b15c079

Type:  kubernetes.io/service-account-token

Data
====
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkNQRldHb3pITVBiZzlhdlRmbDJRNjdPZ0E5YkhYbTlvT2RQRmVBR1phXzQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXo2N2NnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMzNlMDhjYS0wOTdkLTQ2MzUtYjJhYy0zNmQzMmIxNWMwNzkiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.nnDOFLaeC_a63paCCZd7Rn6Wqxrz7k2kzY7uoUzraAG09ozGVk_bnOGq47nU7YQeCGLiz1RSOfZuVICeeQUICSogidosnMl7QRlCSDOn_ob-zyos4yqZbfUWRTJK0r7Df0S5A8UUBN3OQ4in24rQ0TpuqLNY3txbUE_Q25bPWm31hc6zxeRw7stAoa7_9wqptFCHu8mfhrM7M_o0RjvfcVRqanxK21qC6k9Vfc2sdFc2ibJGyByi9nuxEnvDBXtbk_vK4Kvp9HRg_fCixP7nChyoo2QHajJPjRRHe_-4WsDi0IJ5Qtw8japuxaTSMs1ne1-wxCoEfnWk_uEqDJu82g
ca.crt:     1025 bytes
namespace:  20 bytes

Copy the token (the long line of meaningless characters) because you will need 
it to log into the dashboard, when we will access it via the browser.


5.5 - Connect to the Dashboard web server
=========================================

In order to connect to the dashboard, you simply need to access the URL 
http://server@ip:server@port/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/

However, the server runs on the cluster's internal network, and you cannot 
access directly this network: you need to connect via a proxy provided by 
Kubernetes.

Since the web server is running on the Master (since we have activated only 
one node in the cluster so far), let's try to connect directly from the 
Master (knowing that the default port is probably 80):

thierry@k8s-master:~$ curl http://localhost/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
curl: (7) Failed to connect to localhost port 80: Connection refused

As expected, the connection is refused: we will run a proxy from a different 
terminal window (because teh proxy runs in exclusive mode in that window) and 
we will try again to connect from the Master:

From another terminal:

thierry@Ubuntu-thierry:~$ ssh thierry@192.168.0.134
thierry@192.168.0.134's password: 
Welcome to Ubuntu 18.04.3 LTS (GNU/Linux 4.15.0-72-generic x86_64)
...
Last login: Fri Dec 27 15:14:16 2019 from 192.168.0.23
thierry@k8s-master:~$ kubectl proxy -p 8070
Starting to serve on 127.0.0.1:8070

Now we will try again to access the dashboard from the Master, back on the 
previous terminal and using the proxied port 8070:

thierry@k8s-master:~$ curl http://localhost:8070/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}

Well, we don't exactly get the expected result (i.e. we see a 404 code which 
is never a great news when connecting to a web server) but at least the 
connection is accepted. We've done well actually, but we still to work a bit 
more since this is not yet so useful, is it? We need to see what exactly 
happens in a browser to better understand.

Actually, since the Master VM is built from a 'server' OS (i.e. Ubuntu without 
the UI and all related components, in a pure CLI mode), we cannot run a 
browser directly from the Master. We need to do so from the Host. So we will 
kill the proxy running on the Master (beware: run only ONE proxy at any moment 
in time), and launch the same Kubernetes  proxy from a new terminal on the 
Host, and tehn we will try to connect from the Host again:

From a different terminal:

tso@laptop:~$ kubectl proxy -p 8080
Starting to serve on 127.0.0.1:8080

And then back in our 'good old Host' terminal:

tso@laptop:~$ curl http://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {
    
  },
  "status": "Failure",
  "message": "the server could not find the requested resource",
  "reason": "NotFound",
  "details": {
    
  },
  "code": 404
}

Yes! The connection is accepted: we now can launch a browser from the Host and
see what happens. Do it now, and you will see the login page of the dashboard:
    - tick the 'Token' radio button and paste the token which you copied 
        earliers (the one which we got from the 'get secret' and 'describe' 
        commands).
    - sign in

and you are now on the welcome page of the dashboard: feel free navigating on 
the pages, since it will give you detailed information about the cluster. 
Namely, you can go on the 'Overview'and you will see that there is only one 
service running right now, the 'Kubernetes' service (i.e. Kubernetes is 
monitoring itself via the dashboard, the same wat it will monitor your 
applications).

If you go on the 'Nodes' tab, you will see that 'k8s-master' is the only node 
running so far. Click on its name, and you get a detailed status of the node: 
go down and see the 'Allocation' section which shows the resources consummed 
(vs the sizing of the node), and further down the 'Pods' section shows the 
various components of the Kubernetes control plane which are running just to 
keep the cluster active.

If you go on the 'Pods' tab, you will see that it is still empty. This tab 
only shows the 'user's' Pods, the ones activated in order to run your 
applications. You don't see here the control plane components (as shown 
above).




===============================================
6 - Enroll the two slave nodes into the cluster
===============================================

It's now time to join the two slaves into the cluster. To do so, we need to 
log into the slave nodes (with ssh), as 'root', and use the token and key 
which were displayed at the end of the cluster initialisation message:

root@k8s-slave1:/# kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
    --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb 

and then:

root@k8s-slave2:/# kubeadm join 192.168.0.134:6443 --token 2693ts.tkmiic8k3ngfa96l \
    --discovery-token-ca-cert-hash sha256:3f151eb58176dd4473f58b1d8db39b9aa64b13a06f4accc6d60b68a0a5612beb 

Come back to the laptop and check the status of the cluster. Check the number 
of nodes as they get enlisted and active into the cluster:

thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   69m   v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   0s    v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   10s   v1.17.0
k8s-slave2   NotReady   <none>   1s    v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   70m   v1.17.0
k8s-slave1   NotReady   <none>   29s   v1.17.0
k8s-slave2   Ready      <none>   20s   v1.17.0
thierry@Ubuntu-thierry:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   70m   v1.17.0
k8s-slave1   Ready    <none>   31s   v1.17.0
k8s-slave2   Ready    <none>   22s   v1.17.0

Now, let's look at the pods running on the cluster: you can see that some 
pods are now running on the slave nodes as well:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system            coredns-6955765f44-bmkd4                     1/1     Running   0          74m     10.244.0.2   k8s-master   <none>           <none>
kube-system            coredns-6955765f44-kjbdn                     1/1     Running   0          74m     10.244.0.3   k8s-master   <none>           <none>
kube-system            etcd-k8s-master                              1/1     Running   0          74m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-apiserver-k8s-master                    1/1     Running   0          74m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-controller-manager-k8s-master           1/1     Running   0          74m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-ctm9c                  1/1     Running   1          4m23s   10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-flannel-ds-amd64-qzbwg                  1/1     Running   0          58m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-flannel-ds-amd64-x52cp                  1/1     Running   0          4m32s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-proxy-45csc                             1/1     Running   0          4m23s   10.0.2.15    k8s-slave2   <none>           <none>
kube-system            kube-proxy-c9mmr                             1/1     Running   0          74m     10.0.2.15    k8s-master   <none>           <none>
kube-system            kube-proxy-z4vlc                             1/1     Running   0          4m32s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system            kube-scheduler-k8s-master                    1/1     Running   0          74m     10.0.2.15    k8s-master   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-8qztj   1/1     Running   0          50m     10.244.0.5   k8s-master   <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-5996555fd8-2gfcr        1/1     Running   0          50m     10.244.0.4   k8s-master   <none>           <none>


YES!!! The cluster is up and running and the master is actually scheduling 
pods on the slave nodes. At this moment, the pods are only Kubernetes' 
components, but this is already showing the principles by which Kubernetes 
will schedule your own application on the pods.

Go to the browser, at the 'Nodes' tab, and you will see the three active nodes 
with their respective details. Click on one of the salves and observe the 
'Allocation' and teh 'Pods' section: the slave is significantly less loaded 
than teh master, because it carries only a fraction of the control plane tasks
compared to the master. Quite logically, the slaves are primarily focused on 
running your application, and not Kubernetes itself: the repartition of the 
tasks amongst teh node aims at freeing up as much as possible of the slave's 
resources to make them available to... you.


