#############################################################################
#                                                                           #
# Learn Kubernetes Basics - Part 2 - Create a cluster                       #
#                                                                           #
#############################################################################


Let's now refine a bit few concepts on the node, and get these concepts in 
action in setting up and deploying a cluster!


======================================
2.1 - Naming conventions & unique data
======================================


In this section, we will build step by step the infrastructure on which we 
will deploy a Kubernetes cluster. We intentionaly make every step manual, so 
that you can appreciate the full process at leat once in your lifetime. In 
real production world, most of these steps are automated and you would simply 
"push a button".

The steps are:
    - building an initial  VM image ("K8s BARE") with all the prerequisites 
      installed (Ubuntu OS, docker, kubernetes)
    - renaming a VM to be the master node ("K8s master - cluster not 
      deployed"), derived from "K8S BARE"
    - renaming two 'slave' VMs ("K8s slave - cluster not deployed") derived 
      from "K8s BARE"
    - initialize and deploy a Kubernetes on the master, and join the two 
      slaves into the cluster (all VMs are then called "cluster deployed")
    - setup a dashboard and access it


Note: in order to save time, you can jump directly as the section 2.4 and set 
    up the cluster from VM images which are properly configured. The VM images 
    are named:
        2019-12-23 - K8s master - not configured
        2019-12-23 - K8s slave 1 - not configured
        2019-12-23 - K8s slave 2 - not configured



###
###      WARNING
###
### Few informations will be unique to each deployement and you need to 
### carefully copy and save these informations in order to adapt the commands 
### as shown in the tutorial (i.e. replace the tutorial's info with yours) and 
### run your own version of the commands:
###
### 1) the ip@ of the VMs
###
### In the tutorial, I used (copied from a real execution):
###    master:  192.168.0.118
###    slave 1: 192.168.0.119
###    slave 2: 192.168.0.120
### The ip@ granted to each VM by the host machine will vary from execution of
### this tutorial to another, so please collect the ip@as you will start the 
### VMs and keep this information for the whole duration of the tutorial.
###
### 2) the token required for a node to join into the cluster
###
### In the following examples, I used:
###
### $ kubeadm join 192.168.0.118:6443 --token xieq98.t6n7bkdyhqwrsb79 \
    --discovery-token-ca-cert-hash sha256:6a37ccc0523cf5b060ae80246f06e457bae8298cf9e50f9ea56fbd7d13bad041
###
### YOU WILL NEED TO USE YOUR OWN INFORMATION AS SHOWN IN SECTION 2.4
###

===========================
2.2 - Building the first VM
===========================

(you can skip this phase and use directly the "K8s BARE" image)

Nothing fancy here: we've built from the Ubuntu Server 18.04 LTS, with:
  - two CPUs (it is mandatory to run a Kubernetes master)
  - 2048 MB memory
  - two network interfaces:
       * one NAT (which is default setting) for internet connectivity
       * one configured in a "Bridge Adapter" mode, pointing on the most 
         relevant network adapter (in my case: the Ethernet card on the 
         desktop, or the wifi card on the laptop)

(image "K8s BARE - 0 - Description")
(image "K8s BARE - 1 - Memory")
(image "K8s BARE - 2 - CPU")
(image "K8s BARE - 3 - 1st Network interface")
(image "K8s BARE - 4 - 2nd Network interface")

In order to make this VM a generic basis on which we can run any type of k8s
nodes, we then need to log as root and (in this order):
  - to install docker
  - to disable the swap
  - to install kubernetes

For convenience, in this tutorial, we will configure the VMs by connecting 
into them via ssh from terminals running on the host machine, using the 
default admin profile (login=thierry, password=thierry) (Secure, huh!!!). 
There are few advantages doing so, namely the fact that the prompt then tells 
on which machine you are actually logged, which is very convenient when you 
will have 3 VMs running at the same time.

To do so, you need to identify the ip@ of the VM: it will be displayed on 
your first loging into the VM from the VM window:

(image "K8s BARE - 5 - Login from the VM")

Open a terminal and - as a regular user on the host machine, ssh into the VM:
(in my case, the hostname of my laptop is 'laptop' and my login is 'tso')

tso@laptop:~$ ssh thierry@192.168.0.118

The first time you connect on a remote machine, you're asked to confirm: 
confirm by typing 'yes' and then enter the password:

The authenticity of host '192.168.0.118 (192.168.0.118)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.118' (ECDSA) to the list of known hosts.
thierry@192.168.0.118's password: 

You are now connected on the VM and your prompt will show it:

(K8s BARE - 6 - Login from a terminal)

thierry@k8s_node:~$

You are logged as 'thierry' (the name of the account on the VM) and not 
anymore as 'tso' (the name of the account on the host machine) and you are on 
'k8s_node'* and not on 'laptop' anymore.

*: the initial name of the machine is whatever you have specified when 
building the fist image from the Ubuntu OS in VirtualBox. I used the generic 
name 'k8s_node' because the same VM will later be used to configure both 
master and slave nodes.


We must do the following steps as 'root':

thierry@k8s_node:~$ sudo su

The prompt becomes 'root@k8s_node:/#' and we can continue the next steps.

We will install docker: collect the certificates, add the signing key to the 
local apt repository, add the docker repository... and finally install docker:

root@k8s_node:/# apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common
root@k8s_node:/# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb https://download.docker.com/linux/debian stretch stable"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y docker-ce

Then we must disable the swap: we do so immediately with the "swappoff" 
command:

root@k8s_node:/# swapoff -a

and we also prevent the swap to be re-activated at next boot, by editing the 
fstab file (/etc/fstab) and disable the line indicating the swap partition 
by commenting it (i.e. add a '#' at the beginning of the line). Thus, the 
original file changes from:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
/swap.img	none	swap	sw	0	0

to:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
#/swap.img	none	swap	sw	0	0


We will then install Kubernetes: collect the certificates, add the signing 
key to the local apt repository, add the google kubernetes repository... and 
finally install docker:

root@k8s_node:/# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y kubelet kubeadm kubectl


Here we are !!!
The generic VM is setup, and we can use this basis for crafting teh master 
and slave nodes. As a matter of precaution, you should log out, go in 
VirtualBox and shutdown the VM (softly), take a snapshot of the VM and export 
it as and appliance under the name of  "K8s BARE".


=========================================
2.3 - Building the master and slave nodes
=========================================

In this section, we will configure one master node and two slave nodes 
from "K8s BARE". To do so, we will clone K8s_BARE (which VirtualBox does very
easily).

In order to reduce the memory footprint of the tutorial, choose 'linked clone'
rather than 'full clone', since each VM virtual disk is more than 3.5GB (and
the laptop does not have infinite storage).

(K8s BARE - 7 - Cloning)

And you should get something looking like this:

(K8s BARE - 8 - Cloned machines)

Start the three VMs in VirtualBox, and as shown in the previous section, 
log in, identify the ip@, and connect on these VMs from terminals running on 
the host machine.

Since almost all prerequisites were already installed in the previous phase, 
the task here merely consists in renaming the machines so that a) you know on
which machine you are logged, and b) you prevent naming conflicts within the
kubernetes cluster later on (since every node must have a unique name in the 
cluster).

So we will edit two files: /etc/hosts and /etc/hostname

Log into the first VM (in our case 'ssh thierry@192.168.0.63) and edit the
file /etc/hostname: replace the previous host name (k8s-node) with the new
host name 'k8s-master'.

Then edit the file /etc/hosts and add the new name 'k8s-master after
'127.0.0.1   localhost'.

After edition, it should look like this:

thierry@k8s-master:~$ cat /etc/hostname 
k8s-master

thierry@k8s-master:~$ cat /etc/hosts
127.0.0.1 localhost k8s-master
127.0.1.1 k8s_node

Log out and log in again, and the prompt should look like:

thierry@k8s-master:~$ 

Then do so on the two other machines (respectively at 192.168.0.116 and 
192.168.0.117) with the new host names 'k8s-slave1' and 'k8s-slave2'. The 
prompt on these two machines should look like:

thierry@k8s-slave1:~$ 

and

thierry@k8s-slave2:~$ 

Interestingly, the prompt will always tell you on which machine and which 
account you are a given moment in time, which will be important in order not 
to do too many mistakes while we will configure the Kubernetes cluster.

That's it! We now have 3 machines running on the laptop, with all the 
prerequisites installed and the right configuration to make them a master 
and two slaves.

We can test the connectivity between the machines on the laptop (ping from 
one machine to the other) to check that there is no problem, and then go to
the next section.


=====================================
2.4 - Configure and start the cluster
=====================================

To start with, we will initiate the cluster on the master node.

SSH onto the master node and log as root. To initiate the cluster, we will use 
'kubeadm' with the 'init' command. We specify two arguments:
    - specify the 'internal cluster network' which will enable the nodes to 
      communicate one with the other. This internal network uses the second 
      network interface declared in VirtualBox (the one in bridge mode, and
      use a dedicated internal IP range (10.244.0.0-255 in this example).
    - specifiy that the master node must advertise its own IP@ in order to 
      enable the slave nodes to first connect to the master node.

root@k8s-master:/# kubeadm init --pod-network-cidr=10.244.0.0/16 \
      --apiserver-advertise-address=192.168.0.118
      
The whole setup can take a bit of time, and it shoudl conclude with displaying 
the following message (truncated):

[init] Using Kubernetes version: v1.17.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[...]
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.118:6443 --token xieq98.t6n7bkdyhqwrsb79 \
    --discovery-token-ca-cert-hash sha256:6a37ccc0523cf5b060ae80246f06e457bae8298cf9e50f9ea56fbd7d13bad041



Here you are: your Kubernetes master has initialized successfully!
You can see that the message displayed during the initialisation process 
includes three very important directives:
    - you need to create on each machine from which you will manage the cluster 
      (in our case, on the host and on the master) a .kube directory and 
      configuration file to be able to operate the cluster as a regular user 
      (i.e. not root)
    - you need to actually deploy the network which will connect all the pods 
      together, and you will have to choose one of the possible network 
      configurations.
    - you are given the information needed for a slave node to join the 
      cluster by giving the right token and the public key of the master.

So, let's follow these directives.


2.4.1 - create the .kube/config file
====================================

The next step is to configure the 'regular' user (i.e. not root) on the 
master node who will manage the cluster, and on the host machine. First we 
logout as 'root' but we stay logged as 'thierry' on the master, and as a 
'thierry' we enter the commands given in the initialization message:

thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config



2.4.2 - Deploy a Kubernetes network model
=========================================

We now must deploy the network within the cluster. For the sake of simplicity, 
we will use the 'flannel' network configuration (not because it is better than 
others, but because I know it is simple to deploy and make work).

The configuration file for the 'flannel' network can be directly retrieved 
from the kubernetes github repository, and a copy is available in the 'yaml' 
sub-directory.

thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

Once you have received this message, the master will need a bit of time to 
process the deployment as it will actually spawn new pods in charge of 
operating the network. You can see the progress by asking several times to 
show the pods running on the master with the command:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide

See the results at several seconds distance, and observe that the pods status
evolve as their get from 'Pending' to 'ContainerCreating' and finally to 
'Running'.

tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          14s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          2m58s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running             0          20s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running             0          3m4s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Running   0          3m7s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Running   0          3m7s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          23s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m7s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             1/1     Running   0          3m16s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             1/1     Running   0          3m16s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          32s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>


This is actually interesting as it reveals one key nature of Kubernetes: it
will enable you to run containerized applications, but it relies itself on 
launching containers (i.e. containerized applications) to do its job. As your
can see above, it launches a DNS application wich will serve to connect pods
within the cluster: this DNS application is itself running in a container, 
managed as a pod.

Looking at the list of the pods, you see that Kubernetes launched several 
applications: an API server, a proxy, a scheduler, a datastore (ETCD), two
DNS... and a controller manager. We will see abit later all these functions.


2.4.3 - Remotely manage the cluster as a regular user on the laptop
===================================================================

Now we need to enable the host machine to actually manage the Kubernetes 
cluster, and in order to do so, we must get back to the host and create the 
same .kube directory with the same config file (using the same commands as the
ones we used to create the directory and file on the master node). Log again as 
'tso' on the laptop, and remote copy this .kube directory from the master to 
the laptop:

tso@laptop:~$ mkdir -p $HOME/.kube
tso@laptop:~$ scp -r thierry@192.168.0.124:~/.kube/config ~/.kube/
tso@laptop:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Here you are: you now can manage the Kubernetes cluster from the host machine. 
To check that the cluster is alive, you can run the version command:

tso@laptop:~$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}

and view the cluster details:

tso@laptop:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.63:6443
KubeDNS is running at https://192.168.0.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

tso@laptop:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   9m      v1.17.0
tso@laptop:~$ kubectl get services -o wide

As you can see, the master is still alone; you have not yet joined a slave 
into the cluster.


2.4.3 - Enroll the two slave nodes into the cluster
===================================================

It's now time to join the two slaves into the cluster. To do so, we need to log 
into the slave nodes (with ssh), as 'root', and use the token and key which 
were displayed at the end of the cluster initialisation message:

root@k8s-slave1:/# kubeadm join 192.168.0.63:6443 --token fwt4d1.u6h91bmd5c3uko6e \
    --discovery-token-ca-cert-hash sha256:2e801168b30996a3f2a1dfb2b447a36b359f2522b2f36f9f9983468b4f278b94 

and then:

root@k8s-slave2:/# kubeadm join 192.168.0.63:6443 --token fwt4d1.u6h91bmd5c3uko6e \
    --discovery-token-ca-cert-hash sha256:2e801168b30996a3f2a1dfb2b447a36b359f2522b2f36f9f9983468b4f278b94 


Come back to the laptop and check the status of the cluster. Check the number 
of nodes as they get enlisted and active into the cluster:

tso@laptop:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   5m40s   v1.17.0
tso@laptop:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m46s   v1.17.0
k8s-slave1   NotReady   <none>   3s      v1.17.0
tso@laptop:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m49s   v1.17.0
k8s-slave1   NotReady   <none>   6s      v1.17.0
k8s-slave2   NotReady   <none>   3s      v1.17.0
tso@laptop:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m53s   v1.17.0
k8s-slave1   NotReady   <none>   10s     v1.17.0
k8s-slave2   NotReady   <none>   7s      v1.17.0
tso@laptop:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   7m25s   v1.17.0
k8s-slave1   Ready    <none>   42s     v1.17.0
k8s-slave2   Ready    <none>   39s     v1.17.0

Now, let's look at the pods running on the cluster: you can see that some 
pods are now running on the slave nodes as well:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             1/1     Running   0          7m47s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             1/1     Running   0          7m47s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-4kg8s          1/1     Running   0          80s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          5m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ljggs          1/1     Running   0          84s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-8c7rf                     1/1     Running   0          84s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-9454v                     1/1     Running   0          80s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          7m47s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>



YES!!! The cluster is up and running and the master is actually scheduling 
pods on the slave nodes. At this moment, the pods are only Kubernetes' 
components, but this is already showing the principles by which Kubernetes 
will schedule your own application on the pods.


=========================
2.5 - Setup the dashboard
=========================

Dashboard is a web-based Kubernetes user interface. You can use Dashboard to 
deploy containerized applications to a Kubernetes cluster, troubleshoot your 
containerized application, and manage the cluster resources. You can use 
Dashboard to get an overview of applications running on your cluster, as well 
as for creating or modifying individual Kubernetes resources (such as 
Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, 
initiate a rolling update, restart a pod or deploy new applications using a 
deploy wizard.

Dashboard also provides information on the state of Kubernetes resources in 
your cluster and on any errors that may have occurred.

(image: 'dashboard-general_view')

As Kubernetes exposes all its capability via REST APIs, the dashboard 
application uses these APIs and shows graphically the information (exactly 
the same way 'kubectl' does it with the CLI). The dashboard is itself a 
containerized application running as a 'service' on the K8s cluster: it is 
both using the cluster as its own infrastructure, and use the cluster's APIs 
to monitor  and manage it.


2.5.1 - Deploying the dahsboard UI
==================================

The Dashboard UI is not deployed by default. To deploy it, run the following 
command:

thierry@k8s-master:~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml

The configuration YAML file is also available in the 'yaml' directory, under 
the name 'dashboard-v200b8-recommended.yaml'.

You can check that the dashboard is actually running by checking the pods from
the host machine:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide
************ PRINT HERE THE RESULT  **********************
************ PRINT HERE THE RESULT  **********************
************ PRINT HERE THE RESULT  **********************
************ PRINT HERE THE RESULT  **********************

Here you can see that a pod is running with the name 'dashboard'. However, 
even if the dashboard is actually running, you still cannot access it. To get 
there, we first need to setup a 'user' with the proper privileges and roles.


2.5.2 Creating a sample user to access the dashboard
====================================================

To protect your cluster data, Dashboard deploys with a minimal RBAC ('Role 
Based Access Control') configuration by default. Currently, Dashboard only 
supports logging in with a Bearer Token. To create a token for this demo, you 
can follow this guide, and create a sample user.


    Note: This is ok for this tutorial, but it is NOT the right way to proceed 
        in a production environment. In order to go fast and easily to a 
        visible outcome, we create here a user whi will inherit administrative 
        privileges, which is dangerous in a real environment. Like many 
        shortcuts in this tutorial, please do not assume that this is 
        representative of a 'real' environment.

We will create the user it by writing a yaml manifest describing the user, 
and then a second manifest describing its roles and provoleges. We will then 
apply these two manifests to the cluster via the kubectl command.

A user's role/privilege is always linked to a set of resources: as usual in Kubernetes, we actually 

First we create the 'Service Account', which mean the account (= the user 
profile) which will be related to the service (here the dashboard, which is a 
Kubernetes service).
    - The name of the account is 'admin-user'.
    - We define a 'namespace' on which is it valid: its valideclare it valid in the 
namespace 'kubernetes-dashboard' first.

(file: dashboard-adminuser.yaml)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard

Then run the command:
thierry@k8s-master:~$ kubectl apply -f dashboard-adminuser.yaml
serviceaccount/admin-user created

2.2 Create ClusterRoleBinding

In most cases after provisioning our cluster using kops or kubeadm or any 
other popular tool, the ClusterRole admin-Role already exists in the cluster. 
We can use it and create only ClusterRoleBinding for our ServiceAccount.

    NOTE: apiVersion of ClusterRoleBinding resource may differ between 
        Kubernetes versions. Prior to Kubernetes v1.8 the apiVersion was rbac.authorization.k8s.io/v1beta1.

(file: dashboard-adminrole.yaml)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

Then run the command:
thierry@k8s-master:~$ kubectl apply -f dashboard-adminrole.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created



  
2.3 Bearer Token

Now we need to find THE token we can use to log in. Execute following command:

For Bash:

thierry@k8s-master:~$ kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-7nl7z
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 22db92cc-4878-4faf-ab4e-1523a396f560

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkV0aG5CR3hWUXo4cVdOaFBKa2xtekdhQ05LVlNvV1hGYm1jcHlaSG1hT1kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTdubDd6Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMmRiOTJjYy00ODc4LTRmYWYtYWI0ZS0xNTIzYTM5NmY1NjAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.e6qIABcTZgRlEpbKPig-RsoJJ8UMoBrvocjZ8saj7OpEE9Z49tqBvzHCsp3uzX6gHg5ZNvOwcwpfJvG3QWClqJDFESF2tEm48C-KVei7Wo1Y3xwTVp3Q9tVP09TRwMdzfIqFP_2MkifhFBLkyJ4eDb179RUYLPfM2MKueCy5UoowGiBpqEoHqovxVB1lob4rG2-Y4ugrlwDEcPgOdYQ8dhA0qgBiHwSTMxFnDpuXatl1UutsoSU1HX2DYUevs5U5Y_UF76EI8SRmR23vCmJOt7QhgDcUwekHMUNUwmiF_LqyqUG6jrYSUxzxQQoDzKqtOskq08enBjbTEYu-6MSOsg


Test the availability from the master node:

run the proxy from a different terminal:
thierry@k8s-master:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001


thierry@k8s-master:~$ curl http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard/version:/proxy/
<!--
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <title>Kubernetes Dashboard</title>
  <link rel="icon" type="image/png" href="assets/images/kubernetes-logo.png"/>
  <meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="styles.dd2d1d3576191b87904a.css"></head>

<body>
  <kd-root></kd-root>
<script src="runtime.380dd4d7ab4891f91b7b.js" defer=""></script><script src="polyfills-es5.65f1e5151c840cf04c3e.js" nomodule="" defer=""></script><script src="polyfills.8623bbc9d68876cdaaaf.js" defer=""></script><script src="scripts.7d5e232ea538f2c0f8a7.js" defer=""></script><script src="main.3036e86b43f81b098e24.js" defer=""></script></body>

</html>


Donc je teste depuis la machine hote:

Dans un terminal different:

tso@laptop:~$ kubectl proxy -p 8080
Starting to serve on 127.0.0.1:8080


Et de retour dasn mon terminal de départ:

tso@laptop:~$ curl http://localhost:8080/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
<!--
Copyright 2017 The Kubernetes Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <title>Kubernetes Dashboard</title>
  <link rel="icon" type="image/png" href="assets/images/kubernetes-logo.png"/>
  <meta name="viewport" content="width=device-width">
<link rel="stylesheet" href="styles.dd2d1d3576191b87904a.css"></head>

<body>
  <kd-root></kd-root>
<script src="runtime.380dd4d7ab4891f91b7b.js" defer=""></script><script src="polyfills-es5.65f1e5151c840cf04c3e.js" nomodule="" defer=""></script><script src="polyfills.8623bbc9d68876cdaaaf.js" defer=""></script><script src="scripts.7d5e232ea538f2c0f8a7.js" defer=""></script><script src="main.3036e86b43f81b098e24.js" defer=""></script></body>

</html>


Ca marche !!!!!



Plusieurs images:


