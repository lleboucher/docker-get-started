#!/usr/bin/env bash


###############################################################################
#
# Learn Kubernetes Basics - Part 0 - pre-requisites
#
###############################################################################


This tutorial mixes elements from the official Kubernetes site 'get started' 
sections (many of these), and also from Dada's blog (specifically for setting 
up the Kubernetes clusters on VMs running on a laptop):
    https://kubernetes.io/docs/tutorials/kubernetes-basics/
    https://www.dadall.info/article658/preparer-virtualbox-pour-kubernetes

Here are the identified pre-requisites to run this tutorial and actually
learn something from this experience:

    - have a linux laptop, with a 'admin' account (i.e. need to have
      the sudo privilege). Ubuntu will be perfect for beginners.
    - have curl, git and virtualbox installed

Also, you will find several resources in the directory:

    - flannel.yml - a kubernetes network model
    - server.js   - a node.js example application used as of part 3
    - Dockerfile  - to build the container for the application
    - load-balancer-example.yaml - to dsitribute teh app on several pods
    - several VirtualBox VM images in order to build the Kubernetes cluster,
      located in the "VM_images" sub-directory


This is it. Nothing else is needed... except the desire to learn :-)



###############################################################################
#
# Learn Kubernetes Basics - Part 1 - Kubernetes Basics
#
###############################################################################


=====================
1.1 Kubernetes Basics
=====================

This tutorial provides a walkthrough of the basics of the Kubernetes cluster
orchestration system. Each module contains some background information on
major Kubernetes features and concepts. You will actually deploy and manage 
a simple cluster and its containerized applications by yourself.

Following the tutorial steps, you can learn to:

  - Deploy a Kubernetes cluster
  - Deploy a containerized application on a cluster.
  - Scale the deployment.
  - Update the containerized application with a new software version.
  - Debug the containerized application.

I purposedly copied some reference documentation from the official Kubernetes
site on design and architecture (section 1.3 + APPENDIX 1) because it is 
examplary of the philosophy underlying the whole Kubernetes project: clarity of 
purpose translated in the ability of distributed teams to efficiently 
contributes into a larger goal, native and forceful openness of the design, 
resilience-by-design deeply embeded in the implementation of every layer. It 
is REMARKABLE and should be a source of inspiration in many Orange 
endeavours...



===================================
1.2 What can Kubernetes do for you?
===================================

With modern web services, users expect applications to be available 24/7, and
developers expect to deploy new versions of those applications several times
a day.

Kubernetes coordinates a highly available cluster of computers that are 
connected to work as a single unit. The abstractions in Kubernetes allow 
you to deploy containerized applications to a cluster without tying them 
specifically to individual machines. To make use of this new model of 
deployment, applications need to be packaged in a way that decouples them 
from individual hosts: they need to be containerized. 

    KUBERNETES CAN MANAGE CONTAINERIZED APPLICATIONS ONLY

Containerized applications are more flexible and available than in past 
deployment models, where applications were installed directly onto specific 
machines as packages deeply integrated into the host. Kubernetes automates the 
distribution and scheduling of application containers across a cluster in 
a more efficient way.


Kubernetes is a production-ready, open source platform designed
with Google's accumulated experience in container orchestration, combined 
with best-of-breed ideas from the community.


A Kubernetes cluster consists of two types of resources:

    - The Master coordinates the cluster
    - Nodes are the workers that run applications


(picture: cluster Diagram)


The Master is responsible for managing the cluster. The master coordinates 
all activities in your cluster, such as scheduling applications, 
maintaining applications' desired state, scaling applications, and rolling 
out new updates.

A node is a VM or a physical computer that serves as a worker machine in 
a Kubernetes cluster. Each node has a Kubelet, which is an agent for 
managing the node and communicating with the Kubernetes master. The node 
should also have tools for handling container operations, such as Docker or 
rkt. A Kubernetes cluster that handles production traffic should have a 
minimum of three nodes.

Masters manage the cluster and the nodes are used to host the running 
applications.

When you deploy applications on Kubernetes, you tell the master to start 
the application containers. The master schedules the containers to run on 
the cluster's nodes. The nodes communicate with the master using the 
Kubernetes API, which the master exposes. End users can also use the 
Kubernetes API directly to interact with the cluster.

A Kubernetes cluster can be deployed on either physical or virtual 
machines. In our case, I explained in Part I how to deploy a Kubernetes 
cluster in such a way.



======================================
1.3 Kubernetes Design and Architecture
======================================


1.3.1 Scope
===========

Kubernetes is a platform for deploying and managing containers. Kubernetes 
provides a container runtime, container orchestration, container-centric 
infrastructure orchestration, self-healing mechanisms such as health checking 
and re-scheduling, and service discovery and load balancing.

Kubernetes aspires to be an extensible, pluggable, building-block OSS 
platform and toolkit. Therefore, architecturally, we want Kubernetes to be 
built as a collection of pluggable components and layers, with the ability to 
use alternative schedulers, controllers, storage systems, and distribution 
mechanisms, and we're evolving its current code in that direction. 
Furthermore, we want others to be able to extend Kubernetes functionality, 
such as with higher-level PaaS functionality or multi-cluster layers, without 
modification of core Kubernetes source. Therefore, its API isn't just (or 
even necessarily mainly) targeted at end users, but at tool and extension 
developers. Its APIs are intended to serve as the foundation for an open 
ecosystem of tools, automation systems, and higher-level API layers. 
Consequently, there are no "internal" inter-component APIs. All APIs are 
visible and available, including the APIs used by the scheduler, the node 
controller, the replication-controller manager, Kubelet's API, etc. There's 
no glass to break -- in order to handle more complex use cases, one can just 
access the lower-level APIs in a fully transparent, composable manner.


1.3.2 Goals
===========

The project is committed to the following (aspirational) design ideals:

    - Portable. Kubernetes runs everywhere -- public cloud, private cloud, 
      bare metal, laptop -- with consistent behavior so that applications and 
      tools are portable throughout the ecosystem as well as between 
      development and production environments.
    - General-purpose. Kubernetes should run all major categories of 
      workloads to enable you to run all of your workloads on a single 
      infrastructure, stateless and stateful, microservices and monoliths, 
      services and batch, greenfield and legacy.
    - Meet users partway. Kubernetes doesn’t just cater to purely greenfield 
      cloud-native applications, nor does it meet all users where they are. 
      It focuses on deployment and management of microservices and 
      cloud-native applications, but provides some mechanisms to facilitate 
      migration of monolithic and legacy applications.
    - Flexible. Kubernetes functionality can be consumed a la carte and (in 
      most cases) Kubernetes does not prevent you from using your own 
      solutions in lieu of built-in functionality.
    - Extensible. Kubernetes enables you to integrate it into your 
      environment and to add the additional capabilities you need, by 
      exposing the same interfaces used by built-in functionality.
    - Automatable. Kubernetes aims to dramatically reduce the burden of 
      manual operations. It supports both declarative control by specifying 
      users’ desired intent via its API, as well as imperative control to 
      support higher-level orchestration and automation. The declarative 
      approach is key to the system’s self-healing and autonomic capabilities.
    - Advance the state of the art. While Kubernetes intends to support 
      non-cloud-native applications, it also aspires to advance the 
      cloud-native and DevOps state of the art, such as in the participation 
      of applications in their own management. However, in doing so, we 
      strive not to force applications to lock themselves into Kubernetes 
      APIs, which is, for example, why we prefer configuration over 
      convention in the downward API. Additionally, Kubernetes is not bound 
      by the lowest common denominator of systems upon which it depends, such 
      as container runtimes and cloud providers. An example where we pushed 
      the envelope of what was achievable was in its IP per Pod networking 
      model.


1.3.3 Architecture
==================

A running Kubernetes cluster contains node agents (kubelet) and a cluster 
control plane (AKA master), with cluster state backed by a distributed storage 
system (etcd).


a) Cluster control plane (AKA master)
=====================================

The Kubernetes control plane is split into a set of components, which can all 
run on a single master node, or can be replicated in order to support 
high-availability clusters, or can even be run on Kubernetes itself 
(AKA self-hosted).

Kubernetes provides a REST API supporting primarily CRUD operations on 
(mostly) persistent resources, which serve as the hub of its control plane. 
Kubernetes’s API provides IaaS-like container-centric primitives such as Pods, 
Services, and Ingress, and also lifecycle APIs to support orchestration 
(self-healing, scaling, updates, termination) of common types of workloads, 
such as ReplicaSet (simple fungible/stateless app manager), Deployment 
(orchestrates updates of stateless apps), Job (batch), CronJob (cron), 
DaemonSet (cluster services), and StatefulSet (stateful apps). We 
deliberately decoupled service naming/discovery and load balancing from 
application implementation, since the latter is diverse and open-ended.

Both user clients and components containing asynchronous controllers interact 
with the same API resources, which serve as coordination points, common 
intermediate representation, and shared state. Most resources contain 
metadata, including labels and annotations, fully elaborated desired state 
(spec), including default values, and observed state (status).

Controllers work continuously to drive the actual state towards the desired 
state, while reporting back the currently observed state for users and for 
other controllers.

While the controllers are level-based (as described here and here) to 
maximize fault tolerance, they typically watch for changes to relevant 
resources in order to minimize reaction latency and redundant work. This 
enables decentralized and decoupled choreography-like coordination without a 
message bus.


API Server
==========

The API server serves up the Kubernetes API. It is intended to be a relatively 
simple server, with most/all business logic implemented in separate components 
or in plug-ins. It mainly processes REST operations, validates them, and 
updates the corresponding objects in etcd (and perhaps eventually other 
stores). Note that, for a number of reasons, Kubernetes deliberately does not 
support atomic transactions across multiple resources.

Kubernetes cannot function without this basic API machinery, which includes:

    - REST semantics, watch, durability and consistency guarantees, API 
      versioning, defaulting, and validation
    - Built-in admission-control semantics, synchronous admission-control 
      hooks, and asynchronous resource initialization
    - API registration and discovery

Additionally, the API server acts as the gateway to the cluster. By 
definition, the API server must be accessible by clients from outside the 
cluster, whereas the nodes, and certainly containers, may not be. Clients 
authenticate the API server and also use it as a bastion and proxy/tunnel to 
nodes and pods (and services).


Cluster state store
===================

All persistent cluster state is stored in an instance of etcd. This provides 
a way to store configuration data reliably. With watch support, coordinating 
components can be notified very quickly of changes.


Controller-Manager Server
=========================

Most other cluster-level functions are currently performed by a separate 
process, called the Controller Manager. It performs both lifecycle functions 
(e.g., namespace creation and lifecycle, event garbage collection, 
terminated-pod garbage collection, cascading-deletion garbage collection, 
node garbage collection) and API business logic (e.g., scaling of pods 
controlled by a ReplicaSet).

The application management and composition layer, providing self-healing, 
scaling, application lifecycle management, service discovery, routing, and 
service binding and provisioning.

These functions may eventually be split into separate components to make them 
more easily extended or replaced.

Scheduler
=========

Kubernetes enables users to ask a cluster to run a set of containers. The 
scheduler component automatically chooses hosts to run those containers on.

The scheduler watches for unscheduled pods and binds them to nodes via the 
/binding pod subresource API, according to the availability of the requested 
resources, quality of service requirements, affinity and anti-affinity 
specifications, and other constraints.

Kubernetes supports user-provided schedulers and multiple concurrent cluster 
schedulers, using the shared-state approach pioneered by Omega. In addition to 
the disadvantages of pessimistic concurrency described by the Omega paper, 
two-level scheduling models that hide information from the upper-level 
schedulers need to implement all of the same features in the lower-level 
scheduler as required by all upper-layer schedulers in order to ensure that 
their scheduling requests can be satisfied by available desired resources.


b) The Kubernetes Node
======================

The Kubernetes node has the services necessary to run application containers 
and be managed from the master systems.

Kubelet
=======

The most important and most prominent controller in Kubernetes is the Kubelet,
which is the primary implementer of the Pod and Node APIs that drive the 
container execution layer. Without these APIs, Kubernetes would just be a 
CRUD-oriented REST application framework backed by a key-value store (and 
perhaps the API machinery will eventually be spun out as an independent 
project).

Kubernetes executes isolated application containers as its default, native 
mode of execution, as opposed to processes and traditional operating-system 
packages. Not only are application containers isolated from each other, but 
they are also isolated from the hosts on which they execute, which is critical 
to decoupling management of individual applications from each other and from 
management of the underlying cluster physical/virtual infrastructure.

Kubernetes provides Pods that can host multiple containers and storage volumes 
as its fundamental execution primitive in order to facilitate packaging a 
single application per container, decoupling deployment-time concerns from 
build-time concerns, and migration from physical/virtual machines. The Pod 
primitive is key to glean the primary benefits of deployment on modern cloud 
platforms, such as Kubernetes.

API admission control may reject pods or add additional scheduling constraints 
to them, but Kubelet is the final arbiter of what pods can and cannot run on 
a given node, not the schedulers or DaemonSets.

Kubelet also currently links in the cAdvisor resource monitoring agent.


Container runtime
=================

Each node runs a container runtime, which is responsible for downloading 
images and running containers.

Kubelet does not link in the base container runtime. Instead, we're defining 
a Container Runtime Interface to control the underlying runtime and facilitate 
pluggability of that layer. This decoupling is needed in order to maintain 
clear component boundaries, facilitate testing, and facilitate pluggability. 
Runtimes supported today, either upstream or by forks, include at least docker 
(for Linux and Windows), rkt, cri-o, and frakti.


Kube Proxy
==========

The service abstraction provides a way to group pods under a common access 
policy (e.g., load-balanced). The implementation of this creates a virtual 
IP which clients can access and which is transparently proxied to the pods in 
a Service. Each node runs a kube-proxy process which programs iptables rules 
to trap access to service IPs and redirect them to the correct backends. This 
provides a highly-available load-balancing solution with low performance 
overhead by balancing client traffic from a node on that same node.

Service endpoints are found primarily via DNS.


Add-ons and other dependencies
=================================

A number of components, called add-ons typically run on Kubernetes itself:
    - DNS
    - Ingress controller
    - Heapster (resource monitoring)
    - Dashboard (GUI)


c) ReplicaSet
=============






Controllers
===========

Un contrôleur est une boucle d'arbitrage qui pilote l'état courant d'un cluster vers son état désiré. Il effectue cette action en gérant un ensemble de pods. Un des types de contrôleur est appelé "contrôleur de réplication”, il gère la réplication et la mise à l'échelle en lançant un nombre spécifique de copies d'un pod sur un cluster. Il gère également la création de pods de remplacement si le nœud sous-jacent est en défaut[19]. Deux des contrôleurs qui font partie du cœur de système de Kubernetes sont : le “DaemonSet Controller” pour lancer un seul pod sur chaque machine (ou un sous-ensemble de machine), ainsi que le “Job Controller” pour lancer des pods qui ont une fin déterminée (par exemple des scripts)[20]. L'ensemble des pods qu'un contrôleur gère est déterminé par des labels selectors qui font partie de la définition du contrôleur.
ServicesModifier

Un service Kubernetes est un groupe de pods travaillant ensemble, par exemple, une couche dans une application multi-couches. L'ensemble des pods qui constituent un service sont définis par un label selector. Kubernetes fournit un service de découverte et de routage en assignant une adresse IP et un nom de domaine à un service, et équilibre la charge du trafic en utilisant le round-robin des connexions réseaux de cette adresse sur l'ensemble des pods correspondant au sélecteur (même lorsqu'en cas de défaut, les pods changent de machines)[15]. Par défaut, un service est exposé à l'intérieur d'un cluster (ex: les pods de backend peuvent être groupés dans un service, avec les requêtes des pods de frontend load balancées vers les backend), mais un service peut également être exposé à l'extérieur d'un cluster (par exemple pour que les clients puissent joindre les pods de frontend)[21].
ArchitectureModifier
Diagramme d'architecture de Kubernetes

Plan de contrôle Kubernetes

Le maître Kubernetes est l'unité de contrôle principale qui gère la charge de travail et dirige les communications dans le système. Le plan de contrôle de Kubernetes consiste en plusieurs composants, chacun ayant son propre processus, qui peuvent s'exécuter sur un seul node maître ou sur plusieurs maîtres permettant de créer des clusters haute disponibilité[22]. Les différents composants du plan de contrôle de Kubernetes sont décrits ci-dessous:
etcdModifier

etcd[23] est une unité de stockage distribuée persistante et légère de données clé-valeur développée par CoreOS, qui permet de stocker de manière fiable les données de configuration du cluster, représentant l'état du cluster à n'importe quel instant. D'autres composants scrutent les changements dans ce stockage pour aller eux-mêmes vers l'état désiré[22].
serveur d'APIModifier

Le serveur d'API est un élément clé et sert l'API Kubernetes grâce à JSON via HTTP. Il fournit l'interface interne et externe de Kubernetes[14],[24]. Le serveur d'API gère et valide des requêtes REST et met à jour l'état des objets de l'API dans etcd, permettant ainsi aux clients de configurer la charge de travail et les containers sur les nœuds de travail.
L'ordonnanceurModifier

L'ordonnanceur est un composant additionnel permettant de sélectionner quel node devrait faire tourner un pod non ordonnancé en se basant sur la disponibilité des ressources. L'ordonnanceur gère l'utilisation des ressources sur chaque node afin de s'assurer que la charge de travail n'est pas en excès par rapport aux ressources disponibles. Pour accomplir cet objectif, l'ordonnanceur doit connaître les ressources disponibles et celles actuellement assignées sur les serveurs.
Controller managerModifier

Le gestionnaire de contrôle (controller manager) est le processus dans lequel s'exécutent les contrôleurs principaux de Kubernetes tels que DaemonSet Controller et le Replication Controller. Les contrôleurs communiquent avec le serveur d'API pour créer, mettre à jour et effacer les ressources qu'ils gèrent (pods, service endpoints, etc.)[24].
Node KubernetesModifier

Le Node aussi appelé Worker ou Minion est une machine unique (ou une machine virtuelle) où des conteneurs (charges de travail) sont déployés. Chaque node du cluster doit exécuter le programme de conteneurisation (par exemple Docker), ainsi que les composants mentionnés ci-dessous, pour communiquer avec le maître afin de configurer la partie réseau de ces conteneurs.
KubeletModifier

Kubelet est responsable de l'état d'exécution de chaque nœud (c'est-à-dire, d'assurer que tous les conteneurs sur un nœud sont en bonne santé). Il prend en charge le démarrage, l'arrêt, et la maintenance des conteneurs d'applications (organisés en pods) dirigé par le plan de contrôle[14],[25].

Kubelet surveille l'état d'un pod et s'il n'est pas dans l'état voulu, le pod sera redéployé sur le même node. Le statut du node est relayé à intervalle de quelques secondes via messages d’état vers le maître. Dès que le maître détecte un défaut sur un node, le Replication Controller voit ce changement d'état et lance les pods sur d'autres hôtes en bonne santé[réf. nécessaire].
Kube-proxyModifier

Le kube-proxy est l’implémentation d'un proxy réseau et d'un répartiteur de charge, il gère le service d'abstraction ainsi que d'autres opérations réseaux[14]. Il est responsable d'effectuer le routage du trafic vers le conteneur approprié en se basant sur l'adresse IP et le numéro de port de la requête entrante. 
    
    
    
Services !!!!



###############################################################################
#
# Learn Kubernetes Basics - Part 2 - Create a cluster
#
###############################################################################


Let's now refine a bit few concepts on the node, and get these concepts in 
action in setting up and deploying a cluster!


======================================
2.1 - Naming conventions & unique data
======================================


In this section, we will build step by step the infrastructure on which we 
will deploy a Kubernetes cluster. We intentionaly make every step manual, so 
that you can appreciate the full process at leat once in your lifetime. In 
real production world, most of these steps are automated and you would simply 
"push a button".

The steps are:
    - building an initial  VM image ("K8s BARE") with all the prerequisites 
      installed (Ubuntu OS, docker, kubernetes)
    - renaming a VM to be the master node ("K8s master - cluster not deployed"), 
      derived from "K8S BARE"
    - renaming two 'slave' VMs ("K8s slave - cluster not deployed") derived from 
      "K8s BARE"
    - initialize and deploy a Kubernetes on the master, and join the two 
      slaves into the cluster (all VMs are then called "cluster deployed")
    - setup a dashboard and access it

You may skip this step and use directly the "cluster deployed" images in order to skip this step and go directly to part 3.


###
###      WARNING
###
### Few informations will be unique to each deployement and you need to 
### carefully copy and save these informations in order to adapt the commands 
### as shown in the tutorial (i.e. replace the tutorial's info with yours) and 
### run your own version of the commands:
###
### 1) the ip@ of the VMs
###
### In the tutorial, I used (copied from a real execution):
###    master:  192.168.0.108
###    slave 1: 192.168.0.109
###    slave 2: 192.168.0.110
###
### 2) the token required for a node to join into the cluster
###
### In the following examples, I used:
###
### $ kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
        --discovery-token-ca-cert-hash \
        sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729
###
###


===========================
2.3 - Building the first VM
===========================

(you can skip this phase and use directly the "K8s BARE" image)

Nothing fancy here: we've built from the Ubuntu Server 18.04 LTS, with:
  - two CPUs (it is mandatory to run a Kubernetes master)
  - 2048 MB memory
  - two network interfaces:
       * one NAT (which is default setting) for internet connectivity
       * one configured in a "Bridge Adapter" mode, pointing on the most 
         relevant network adapter (in my case: the Ethernet card on the 
         desktop, or the wifi card on the laptop)

(image "K8s BARE - 0 - Description")
(image "K8s BARE - 1 - Memory")
(image "K8s BARE - 2 - CPU")
(image "K8s BARE - 3 - 1st Network interface")
(image "K8s BARE - 4 - 2nd Network interface")

In order to make this VM a generic basis on which we can run any type of k8s
nodes, we then need to log as root and (in this order):
  - to install docker
  - to disable the swap
  - to install kubernetes

For convenience, in this tutorial, we will configure the VMs by connecting 
into them via ssh from terminals running on the host machine, using the 
default admin profile (login=thierry, password=thierry) (Secure, huh!!!). 
There are few advantages doing so, namely the fact that the prompt then tells 
on which machine you are actually logged, which is very convenient when you 
will have 3 VMs running at the same time.

To do so, you need to identify the ip@ of the VM: it will be displayed on 
your first loging into the VM from the VM window:

(image "K8s BARE - 5 - Login from the VM")

Open a terminal and - as a regular user on hte host machine, ssh into the VM:
(in my case, the hostname of my laptop is 'laptop' and my login is 'tso')

tso@laptop:~$ ssh thierry@192.168.0.111

The first time you connect on a remote machine, you're asked to confirm: 
confirm by typing 'yes' and then enter the password:

The authenticity of host '192.168.0.111 (192.168.0.111)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.111' (ECDSA) to the list of known hosts.
thierry@192.168.0.111's password: 

You are now connected on the VM and your prompt will show it:

(K8s BARE - 6 - Login from a terminal)

thierry@k8s_node:~$

You are logged as 'thierry' and not anymore 'tso' and you are on 'k8s_node'*
and not on 'laptop' anymore.

*: the initial name of the machine is whatever you have specified when 
building the fist image from the Ubuntu OS in VirtualBox. I used the generic 
name 'k8s_node' because the same VM will later be used to configure both 
master and slave nodes.


We must do the following steps as 'root':

$ sudo su

The prompt becomes # and we can continue the next steps.

We will install docker: collect the certificates, add the signing key to the 
local apt repository, add the docker repository... and finally install docker:

# apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common
# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg | apt-key add -
# add-apt-repository "deb https://download.docker.com/linux/debian stretch stable"
# apt-get update
# apt-get install -y docker-ce

Then we must disable the swap: we do so immediately with the "swappoff" 
command:

# swapoff -a

and we also prevent the swap to be re-activated at next boot, by editing the 
fstab file (/etc/fstab) and disable the line indicating the swap partition 
by commenting it (i.e. add a '#' at the beginning of the line). Thus, the 
original file changes from:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
/swap.img	none	swap	sw	0	0

to:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
#/swap.img	none	swap	sw	0	0


We will then install Kubernetes: collect the certificates, add the signing 
key to the local apt repository, add the google kubernetes repository... and 
finally install docker:

# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
# apt-get update
# apt-get install -y kubelet kubeadm kubectl


Here we are !!!
The generic VM is setup, and we can use this basis for crafting teh master 
and slave nodes. As a matter of precaution, you should log out, go in 
VirtualBox and shutdown the VM (softly), take a snapshot of the VM and export 
it as and appliance under the name of  "K8s BARE".


=========================================
2.4 - Building the master and slave nodes
=========================================

In this section, we will configure one master node and two slave nodes 
from "K8s BARE". To do so, we will clone K8s_BARE (which VirtualBox does very
easily).

In order to reduce the memory footprint of the tutorial, choose 'linked clone'
rather than 'full clone', since each VM virtual disk is more than 3.5GB (and
the laptop does not have infinite storage).

(K8s BARE - 7 - Cloning)

And you should get something looking like this:

(K8s BARE - 8 - Cloned machines)

Start the three VMs in VirtualBox, and as shown in the previous section, 
log in, identify the ip@, and connect on these VMs from terminals running on 
the host machine.

Since almost all prerequisites were already installed in the previous phase, 
the task here merely consists in renaming the machines so that a) you know on
which machine you are logged, and b) you prevent naming conflicts within the
kubernetes cluster later on (since every node must have a unique name in the 
cluster).

So we will edit two files: /etc/hosts and /etc/hostname

Log into the first VM (in our case 'ssh thierry@192.168.0.108) and edit the
file /etc/hostname: replace the previous host name (k8s-node) with the new
host name 'k8s-master'.

Then edit the file /etc/hosts and add the new name 'k8s-master after
'127.0.0.1   localhost'.

After edition, it should look like this:

thierry@k8s-master:~$ cat /etc/hostname 
k8s-master

thierry@k8s-master:~$ cat /etc/hosts
127.0.0.1 localhost k8s-master
127.0.1.1 k8s_node

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

Then do so on the two other machines (respectively at 192.168.0.109 and 
192.168.0.110) with the new host names 'k8s-slave1' and 'k8s-slave2'.

That's it! We now have 3 machines running on the laptop, with all the 
prerequisites installed and the right configuration to make them a master 
and two slaves.

We can test the connectivity between the machines on the laptop (ping from 
one machine to the other) to check that there is no problem, and then go to
the next section.


=====================================
2.5 - Configure and start the cluster
=====================================

To start with, we will initiate the cluster on the master node.

SSH onto the master node and log as root. To initiate the cluster, we will use 
'kubeadm' with the 'init' command. We specify two arguments:
    - specify the 'internal cluster network' which will enable the nodes to 
      communicate one with the other. This internal network uses the second 
      network interface declared in VirtualBox (the one in bridge mode, and
      use a dedicated internal IP range (10.244.0.0-255 in this example).
    - specifiy that the master node must advertise its own IP@ in order to 
      enable the slave nodes to first connect to the master node.

root@k8s-master:/# kubeadm init --pod-network-cidr=10.244.0.0/16 \
      --apiserver-advertise-address=192.168.0.108

The whole setup can take a bit of time, and it shoudl conclude with displaying 
the following message (truncated):

[init] Using Kubernetes version: v1.16.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[...]
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as 
root:

kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729


Here you are: your Kubernetes master has initialized successfully!
You can see that the message displayed during the initialisation process 
includes three very important directives:
    - you need to create a configuration file to be able to operate the 
      cluster as a regulaer user (i.e. not root)
    - you need to actually deploy the network which will connect all the pods 
      together, and you will have to choose one of the possible network 
      configurations.
    - you are given the information needed for a slave node to join the 
      cluster by giving the right token and the public key of the master.

So, let's follow these directives:

The next step is to configure the 'regular' user (i.e. not root) on the 
master node who will manage the cluster. First we logout as 'root' but we 
stay logged as 'thierry' on the master, and as a 'thierry' we enter the 
commands given in the initialization message:

$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Then we must deploy the network within the cluster. For the sake of 
simplicity, we will use the 'flannel' network configuration (not because it 
is better than others, but because I know it is simple to make it work).

thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

Once you have received this message, the master will need a bit of time to 
process the deployment as it will actually spawn noew pods in charge of 
operating the network. You can see the progress by asking several times to 
show the pods running on the master with the command:
$ kubectl get pods --all-namespaces -o wide

See the results at several seconds distance, and observe that the pods status
evolves as their get from 'Pending' to 'ContainerCreating' and finally to 
'Running'.

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             0/1     Pending   0          64s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     Pending   0          64s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          77s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          81s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          97s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          13s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          64s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          97s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE    IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             0/1     ContainerCreating   0          70s    <none>      k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     ContainerCreating   0          70s    <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          83s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          87s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             1          103s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running             0          19s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running             0          70s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             1          103s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   0          78s    10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     Running   0          78s    10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          91s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          95s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          111s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          27s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          78s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          111s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   0          83s    10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             1/1     Running   0          83s    10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          96s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          100s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          116s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          32s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          83s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          116s   10.0.2.15    k8s-master   <none>           <none>

This is actually interesting as it reveals one key nature of Kubernetes: it
will enable you to run containerized applications, but it relies itself on 
launching containers (i.e. containerized applications) to do its job. As your
can see above, it launches a DNS application wich will serve to connect pods
within the cluster: this DNS application is itself running in a container, 
managed as a pod.

Looking at the list of the pods, you see that Kubernetes launched several 
applications: an API server, a proxy, a scheduler, a datastore (ETCD), two
DNS... and a controller manager. We will see abit later all these functions.

To check that the cluster is alive, you can run the version command:

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:23:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:13:49Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}

and view the cluster details:

$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.108:6443
KubeDNS is running at https://192.168.0.108:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

Nevertheless, the master is still alone; you have not yet joined a slave into
the cluster. You can see all the nodes in the cluster with the command
'kubectl get nodes':

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   8m36s   v1.16.3

So it's time to join the two slaves into the cluster. To do so, we need to 
log into the slave nodes (with ssh), as 'root', and use the token and key 
which were displayed at the end of the cluster initialisation message:

root@k8s-slave1:/# kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729

and then:

root@k8s-slave2:/# kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729


Come back to the master node and check the status of the cluster. Check the 
number of nodes:

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE    VERSION
k8s-master   Ready      master   10m    v1.16.3
k8s-slave1   Ready      <none>   5m1s   v1.16.3
k8s-slave2   NotReady   <none>   5s     v1.16.3

Now, let's look at the pods running on the cluster: you can see that some 
pods are now running on the slave nodes as well:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   1          29h   10.244.0.4   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             1/1     Running   1          29h   10.244.0.5   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   3          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-mrm2b          1/1     Running   2          29h   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-s5dnw          1/1     Running   1          29h   10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-6pwws                     1/1     Running   1          29h   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-rfpq5                     1/1     Running   1          29h   10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   3          29h   10.0.2.15    k8s-master   <none>           <none>


YES!!! The cluster is up and running and the master is actually scheduling 
pods on the slave nods. At this moment, the pods are only Kubernetes 
applications, but this is already showing the principles by which Kubernetes
will schedule you application on the pods.


=========================
2.6 - Setup the dashboard   ---  NOT COMPLETED ---
=========================

The cluster is running but it is very difficult to see what is actually 
happening, so it is time to setup the dashboad which visualizes the status 
of the cluster.

Start with login into the master:




root@k8s-master:/# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/head.yaml
namespace/kubernetes-dashboard-head created
serviceaccount/kubernetes-dashboard-head created
service/kubernetes-dashboard-head created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard-head created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
deployment.apps/kubernetes-dashboard-head created
service/dashboard-metrics-scraper-head created
deployment.apps/dashboard-metrics-scraper-head created

As you can see, Kubernetes pulls the resources from GitHub and starts pods 
to run the dashboard application (which is a web server). Check where the 
pods are running:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide | grep dashb
kubernetes-dashboard-head   dashboard-metrics-scraper-head-7cc7d9bb4b-r9lr6   1/1     Running   0          99s   10.244.1.2   k8s-slave1   <none>           <none>
kubernetes-dashboard-head   kubernetes-dashboard-head-5c87564c95-gl8q6        1/1     Running   0          99s   10.244.2.2   k8s-slave2   <none>           <none>

The workload is distributed over the two slave nodes. In order to access this
dasboard, we now need to creae users with teh right roles and authorisations
(i.e. profiles with the rights in RBAC mode)

Créez un compte utilisateur
Créez un fichier admin-user.yaml avec ce qui suit dedans :

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Puis créez le rôle qui lui sera attaché : admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

Chargez ces configurations dans le cluster :

kubectl apply -f admin-user.yaml
kubectl apply -f admin-role.yaml




thierry@k8s-master:~$ touch admin-user.yaml
thierry@k8s-master:~$ vi admin-user.yaml 
thierry@k8s-master:~$ touch admin-role.yaml
thierry@k8s-master:~$ vi admin-role.yaml 
thierry@k8s-master:~$ kubectl apply -f admin-user.yaml
serviceaccount/admin-user created
thierry@k8s-master:~$ kubectl apply -f admin-role.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created


Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :

Name:         admin-user-token-g4h4m
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 2124843b-2815-47f5-a558-cb5956bd61d7

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkUyTVprdjJuMGJ1NkVyeFlweFVHd2RtWHR2ZHFDS1JFb21SMDNQNEY3UlUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWc0aDRtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMTI0ODQzYi0yODE1LTQ3ZjUtYTU1OC1jYjU5NTZiZDYxZDciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.JmnyHQCNRqT90GhMub82keXrhJGi5cOVrKdPoM1Psw6s7qDBRpOobthUVw1KJ5abnFgTnEhVmRW52HcF4dCg2aPcKa5PcFidXtXVdyFXHM2-VUVYDw-97yeVKm9degPES11rs5bcnGGX0Fp7TAN38IhwmDDVcmcR4S7EftnOk1_RkG77Fuauu4mDYDO6kzm9ySrdMP_eF2XsoZr_1JLGucGmMDKaAryRAHPTl11t-i45mE_6NtxGdYo1eukPqKpvKF7-fQz-0Bbg3Pzavoko-nrv1TPZECPAj_cXZk3wptoYzg-Ye_h_dua0ue6CUanE1SY60u0kDmPK8aXGJfZUaw


Copy the token (yeah, I know, its long...)


Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 thierry@192.168.0.108

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://192.168.0.108:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all 

=> ça ne marche pas: ce qui s'affiche est un record de failure.

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
=============================================================================
=============================================================================
NOTES FOR COMPLETING THE DASHBOARD SECTION
=============================================================================
=============================================================================

 
Installing the dashboard:


To install the Dashboard, run the following command from the master:

$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml

Kubernetes va aller chercher la configuration nécessaire la mise à en place du Dashboard directement depuis son dépôt Github et le faire apparaître dans la liste des pods de votre cluster.

$  kubectl get pods --all-namespaces -o wide | grep dashb
kube-system   kubernetes-dashboard-77fd78f978-f8p9l   1/1     Running   0          60s     10.244.1.230   k8snode1    <none>

Il est  "Running", ça veut dire qu'il est disponible, mais pas encore accessible.
Créez un compte utilisateur
Créez un fichier admin-user.yaml avec ce qui suit dedans :

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Puis créez le rôle qui lui sera attaché : admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

Chargez ces configurations dans le cluster :

kubectl apply -f admin-user.yaml
kubectl apply -f admin-role.yaml

Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :

dada@k8smaster:~$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-b8qmq
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: a8a600b1-e010-11e8-96ec-0800273c4560

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJl.........

J'ai tronqué l'affichage du token. Il est d'une longueur dingue. Copiez-le dans un coin maintenant que vous l'avez.
Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 dada@IP_DU_MASTER

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all

Et voir ceci : 


Vous voici avec une belle interface pour admirer le comportement de votre cluster k8s. Foncez cliquer absolument partout et chercher le pourquoi du comment de telles options à tel endroit !





###############################################################################
#
# Learn Kubernetes Basics - Part 3 - Deploy an app
#
###############################################################################



Once you have a running Kubernetes cluster, you can deploy your containerized 
applications on top of it. To do so, you create a Kubernetes Deployment 
configuration. The Deployment instructs Kubernetes how to create and update 
instances of your application. Once you've created a Deployment, the 
Kubernetes master schedules mentioned application instances onto individual 
Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment Controller 
continuously monitors those instances. If the Node hosting an instance goes 
down or is deleted, the Deployment controller replaces the instance with an 
instance on another Node in the cluster. This provides a self-healing 
mechanism to address machine failure or maintenance.

In a pre-orchestration world, installation scripts would often be used to 
start applications, but they did not allow recovery from machine failure. By 
both creating your application instances and keeping them running across 
Nodes, Kubernetes Deployments provide a fundamentally different approach to 
application management.


============================================
3.1 - Deploying your first app on Kubernetes
============================================


You can create and manage a Deployment by using the Kubernetes command line 
interface, Kubectl. Kubectl uses the Kubernetes API to interact with the 
cluster. In this module, you'll learn the most common Kubectl commands 
needed to create Deployments that run your applications on a Kubernetes 
cluster.

When you create a Deployment, you'll need to specify the container image for 
your application and the number of replicas that you want to run. You can 
change that information later by updating your Deployment; Modules 5 and 6 of 
the bootcamp discuss how you can scale and update your Deployments.

Applications need to be packaged into one of the supported container formats 
in order to be deployed on Kubernetes

For your first Deployment, you'll use a Node.js application packaged in a 
Docker container: we will build a container image to run this application.


Let’s deploy our first app on Kubernetes with the 'kubectl create deployment'
command. We need to provide the deployment name and app image location 
(include the full repository url for images hosted outside Docker hub).

$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1

Great! You just deployed your first application by creating a deployment. This 
performed a few things for you:

    - searched for a suitable node where an instance of the application could 
      be run (we have only 1 available node)
    - scheduled the application to run on that Node
    - configured the cluster to reschedule the instance on a new Node when 
      needed

To list your deployments use the get deployments command:

$ kubectl get deployments

We see that there is 1 deployment running a single instance of your app. The 
instance is running inside a Docker container on your node.







Using kubectl to Create a Deployment
Objectives

    Learn about application Deployments.
    Deploy your first app on Kubernetes with kubectl.

Kubernetes Deployments

Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes Deployment configuration. The Deployment instructs Kubernetes how to create and update instances of your application. Once you've created a Deployment, the Kubernetes master schedules mentioned application instances onto individual Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment Controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. This provides a self-healing mechanism to address machine failure or maintenance.

In a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure. By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management.
Summary:

    Deployments
    Kubectl

A Deployment is responsible for creating and updating instances of your application

Deploying your first app on Kubernetes


You can create and manage a Deployment by using the Kubernetes command line interface, Kubectl. Kubectl uses the Kubernetes API to interact with the cluster. In this module, you'll learn the most common Kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.

When you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Modules 5 and 6 of the bootcamp discuss how you can scale and update your Deployments.

Applications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes

For your first Deployment, you'll use a Node.js application packaged in a Docker container. (If you didn't already try creating a Node.js application and deploying it using a container, you can do that first by following the instructions from the Hello Minikube tutorial).

Now that you know what Deployments are, let's go to the online tutorial and deploy our first app!









==============================================
3.2 - ReplicaSet - how to implement resilience
==============================================




=========================================
3.3 - Load balance the app on the cluster
=========================================

(https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/)

We will now illustrate how Kubernetes can deploy the app in a resilient way, by
distributing the app on several nodes/pods at the same time, following a 
deployment policy that we will indicate to the Master.

Here, we will create a service for an application running on three pods:

    Run a Hello World application in your cluster:

service/load-balancer-example.yaml [Copy service/load-balancer-example.yaml to clipboard]

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: load-balancer-example
  name: hello-world
spec:
  replicas: 5
  selector:
    matchLabels:
      app.kubernetes.io/name: load-balancer-example
  template:
    metadata:
      labels:
        app.kubernetes.io/name: load-balancer-example
    spec:
      containers:
      - image: gcr.io/google-samples/node-hello:1.0
        name: hello-world
        ports:
        - containerPort: 8080

kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml

The preceding command creates a Deployment object and an associated ReplicaSet 
object. The ReplicaSet has five Pods, each of which runs the Hello World 
application.

    Display information about the Deployment:

    kubectl get deployments hello-world
    kubectl describe deployments hello-world

    Display information about your ReplicaSet objects:

    kubectl get replicasets
    kubectl describe replicasets

    Create a Service object that exposes the deployment:

    kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

    Display information about the Service:

    kubectl get services my-service

    The output is similar to this:

    NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)    AGE
    my-service   LoadBalancer   10.3.245.137   104.198.205.71   8080/TCP   54s

    Note: If the external IP address is shown as <pending>, wait for a minute and enter the same command again.

    Display detailed information about the Service:

    kubectl describe services my-service

    The output is similar to this:

    Name:           my-service
    Namespace:      default
    Labels:         app.kubernetes.io/name=load-balancer-example
    Annotations:    <none>
    Selector:       app.kubernetes.io/name=load-balancer-example
    Type:           LoadBalancer
    IP:             10.3.245.137
    LoadBalancer Ingress:   104.198.205.71
    Port:           <unset> 8080/TCP
    NodePort:       <unset> 32377/TCP
    Endpoints:      10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more...
    Session Affinity:   None
    Events:         <none>

    Make a note of the external IP address (LoadBalancer Ingress) exposed by your service. In this example, the external IP address is 104.198.205.71. Also note the value of Port and NodePort. In this example, the Port is 8080 and the NodePort is 32377.

    In the preceding output, you can see that the service has several endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal addresses of the pods that are running the Hello World application. To verify these are pod addresses, enter this command:

    kubectl get pods --output=wide

    The output is similar to this:

    NAME                         ...  IP         NODE
    hello-world-2895499144-1jaz9 ...  10.0.1.6   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-2e5uh ...  10.0.1.8   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-9m4h1 ...  10.0.0.6   gke-cluster-1-default-pool-e0b8d269-5v7a
    hello-world-2895499144-o4z13 ...  10.0.1.7   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-segjf ...  10.0.2.5   gke-cluster-1-default-pool-e0b8d269-cpuc

    Use the external IP address (LoadBalancer Ingress) to access the Hello World application:

    curl http://<external-ip>:<port>

    where <external-ip> is the external IP address (LoadBalancer Ingress) of your Service, and <port> is the value of Port in your Service description. If you are using minikube, typing minikube service my-service will automatically open the Hello World application in a browser.

    The response to a successful request is a hello message:

    Hello Kubernetes!

Cleaning up

To delete the Service, enter this command:

kubectl delete services my-service

To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:

kubectl delete deployment hello-world





STATEFULSET



StatefulSets are intended to be used with stateful applications and distributed systems. However, the administration of stateful applications and distributed systems on Kubernetes is a broad, complex topic. In order to demonstrate the basic features of a StatefulSet, and not to conflate the former topic with the latter, you will deploy a simple web application using a StatefulSet.

After this tutorial, you will be familiar with the following.

    How to create a StatefulSet
    How a StatefulSet manages its Pods
    How to delete a StatefulSet
    How to scale a StatefulSet
    How to update a StatefulSet’s Pods

Before you begin

Before you begin this tutorial, you should familiarize yourself with the following Kubernetes concepts.

    Pods
    Cluster DNS
    Headless Services
    PersistentVolumes
    PersistentVolume Provisioning
    StatefulSets
    kubectl CLI

This tutorial assumes that your cluster is configured to dynamically provision PersistentVolumes. If your cluster is not configured to do so, you will have to manually provision two 1 GiB volumes prior to starting this tutorial.
Creating a StatefulSet

Begin by creating a StatefulSet using the example below. It is similar to the example presented in the StatefulSets concept. It creates a Headless Service, nginx, to publish the IP addresses of Pods in the StatefulSet, web.
application/web/web.yaml [Copy application/web/web.yaml to clipboard]

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

Download the example above, and save it to a file named web.yaml

You will need to use two terminal windows. In the first terminal, use kubectl get to watch the creation of the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In the second terminal, use kubectl apply to create the Headless Service and StatefulSet defined in web.yaml.

kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

The command above creates two Pods, each running an NGINX webserver. Get the nginx Service and the web StatefulSet to verify that they were created successfully.

kubectl get service nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         <none>        80/TCP    12s

kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         1         20s

Ordered Pod Creation

For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}. Examine the output of the kubectl get command in the first terminal. Eventually, the output will look like the example below.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s

Notice that the web-1 Pod is not launched until the web-0 Pod is Running and Ready.
Pods in a StatefulSet

Pods in a StatefulSet have a unique ordinal index and a stable network identity.
Examining the Pod’s Ordinal Index

Get the StatefulSet’s Pods.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          1m
web-1     1/1       Running   0          1m

As mentioned in the StatefulSets concept, the Pods in a StatefulSet have a sticky, unique identity. This identity is based on a unique ordinal index that is assigned to each Pod by the StatefulSet controller. The Pods’ names take the form <statefulset name>-<ordinal index>. Since the web StatefulSet has two replicas, it creates two Pods, web-0 and web-1.
Using Stable Network Identities

Each Pod has a stable hostname based on its ordinal index. Use kubectl exec to execute the hostname command in each Pod.

for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

Use kubectl run to execute a container that provides the nslookup command from the dnsutils package. Using nslookup on the Pods’ hostnames, you can examine their in-cluster DNS addresses.

kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm  
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.6

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.6

The CNAME of the headless service points to SRV records (one for each Pod that is Running and Ready). The SRV records point to A record entries that contain the Pods’ IP addresses.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pod -w -l app=nginx

In a second terminal, use kubectl delete to delete all the Pods in the StatefulSet.

kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Wait for the StatefulSet to restart them, and for both Pods to transition to Running and Ready.

kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s

Use kubectl exec and kubectl run to view the Pods hostnames and in-cluster DNS entries.

for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh 
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8

The Pods’ ordinals, hostnames, SRV records, and A record names have not changed, but the IP addresses associated with the Pods may have changed. In the cluster used for this tutorial, they have. This is why it is important not to configure other applications to connect to Pods in a StatefulSet by IP address.

If you need to find and connect to the active members of a StatefulSet, you should query the CNAME of the Headless Service (nginx.default.svc.cluster.local). The SRV records associated with the CNAME will contain only the Pods in the StatefulSet that are Running and Ready.

If your application already implements connection logic that tests for liveness and readiness, you can use the SRV records of the Pods ( web-0.nginx.default.svc.cluster.local, web-1.nginx.default.svc.cluster.local), as they are stable, and your application will be able to discover the Pods’ addresses when they transition to Running and Ready.
Writing to Stable Storage

Get the PersistentVolumeClaims for web-0 and web-1.

kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s

The StatefulSet controller created two PersistentVolumeClaims that are bound to two PersistentVolumes. As the cluster used in this tutorial is configured to dynamically provision PersistentVolumes, the PersistentVolumes were created and bound automatically.

The NGINX webservers, by default, will serve an index file at /usr/share/nginx/html/index.html. The volumeMounts field in the StatefulSets spec ensures that the /usr/share/nginx/html directory is backed by a PersistentVolume.

Write the Pods’ hostnames to their index.html files and verify that the NGINX webservers serve the hostnames.

for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

    Note:

    If you instead see 403 Forbidden responses for the above curl command, you will need to fix the permissions of the directory mounted by the volumeMounts (due to a bug when using hostPath volumes) with:

    for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done

    before retrying the curl command above.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pod -w -l app=nginx

In a second terminal, delete all of the StatefulSet’s Pods.

kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Examine the output of the kubectl get command in the first terminal, and wait for all of the Pods to transition to Running and Ready.

kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s

Verify the web servers continue to serve their hostnames.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though web-0 and web-1 were rescheduled, they continue to serve their hostnames because the PersistentVolumes associated with their PersistentVolumeClaims are remounted to their volumeMounts. No matter what node web-0and web-1 are scheduled on, their PersistentVolumes will be mounted to the appropriate mount points.
Scaling a StatefulSet

Scaling a StatefulSet refers to increasing or decreasing the number of replicas. This is accomplished by updating the replicas field. You can use either kubectl scale or kubectl patch to scale a StatefulSet.
Scaling Up

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

In another terminal window, use kubectl scale to scale the number of replicas to 5.

kubectl scale sts web --replicas=5
statefulset.apps/web scaled

Examine the output of the kubectl get command in the first terminal, and wait for the three additional Pods to transition to Running and Ready.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2h
web-1     1/1       Running   0          2h
NAME      READY     STATUS    RESTARTS   AGE
web-2     0/1       Pending   0          0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       ContainerCreating   0         0s
web-3     1/1       Running   0         18s
web-4     0/1       Pending   0         0s
web-4     0/1       Pending   0         0s
web-4     0/1       ContainerCreating   0         0s
web-4     1/1       Running   0         19s

The StatefulSet controller scaled the number of replicas. As with StatefulSet creation, the StatefulSet controller created each Pod sequentially with respect to its ordinal index, and it waited for each Pod’s predecessor to be Running and Ready before launching the subsequent Pod.
Scaling Down

In one terminal, watch the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In another terminal, use kubectl patch to scale the StatefulSet back down to three replicas.

kubectl patch sts web -p '{"spec":{"replicas":3}}'
statefulset.apps/web patched

Wait for web-4 and web-3 to transition to Terminating.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3h
web-1     1/1       Running             0          3h
web-2     1/1       Running             0          55s
web-3     1/1       Running             0          36s
web-4     0/1       ContainerCreating   0          18s
NAME      READY     STATUS    RESTARTS   AGE
web-4     1/1       Running   0          19s
web-4     1/1       Terminating   0         24s
web-4     1/1       Terminating   0         24s
web-3     1/1       Terminating   0         42s
web-3     1/1       Terminating   0         42s

Ordered Pod Termination

The controller deleted one Pod at a time, in reverse order with respect to its ordinal index, and it waited for each to be completely shutdown before deleting the next.

Get the StatefulSet’s PersistentVolumeClaims.

kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-2   Bound     pvc-e1125b27-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-3   Bound     pvc-e1176df6-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-4   Bound     pvc-e11bb5f8-b508-11e6-932f-42010a800002   1Gi        RWO           13h

There are still five PersistentVolumeClaims and five PersistentVolumes. When exploring a Pod’s stable storage, we saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet’s Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.
Updating StatefulSets

In Kubernetes 1.7 and later, the StatefulSet controller supports automated updates. The strategy used is determined by the spec.updateStrategy field of the StatefulSet API Object. This feature can be used to upgrade the container images, resource requests and/or limits, labels, and annotations of the Pods in a StatefulSet. There are two valid update strategies, RollingUpdate and OnDelete.

RollingUpdate update strategy is the default for StatefulSets.
Rolling Update

The RollingUpdate update strategy will update all Pods in a StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.

Patch the web StatefulSet to apply the RollingUpdate update strategy.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate"}}}'
statefulset.apps/web patched

In one terminal window, patch the web StatefulSet to change the container image again.

kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"gcr.io/google_containers/nginx-slim:0.8"}]'
statefulset.apps/web patched

In another terminal, watch the Pods in the StatefulSet.

kubectl get po -l app=nginx -w
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          7m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          8m
web-2     1/1       Terminating   0         8m
web-2     1/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Pending   0         0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-1     1/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         6s
web-0     1/1       Terminating   0         7m
web-0     1/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s

The Pods in the StatefulSet are updated in reverse ordinal order. The StatefulSet controller terminates each Pod, and waits for it to transition to Running and Ready prior to updating the next Pod. Note that, even though the StatefulSet controller will not proceed to update the next Pod until its ordinal successor is Running and Ready, it will restore any Pod that fails during the update to its current version. Pods that have already received the update will be restored to the updated version, and Pods that have not yet received the update will be restored to the previous version. In this way, the controller attempts to continue to keep the application healthy and the update consistent in the presence of intermittent failures.

Get the Pods to view their container images.

for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8

All the Pods in the StatefulSet are now running the previous container image.

Tip You can also use kubectl rollout status sts/<name> to view the status of a rolling update.
Staging an Update

You can stage an update to a StatefulSet by using the partition parameter of the RollingUpdate update strategy. A staged update will keep all of the Pods in the StatefulSet at the current version while allowing mutations to the StatefulSet’s .spec.template.

Patch the web StatefulSet to add a partition to the updateStrategy field.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'
statefulset.apps/web patched

Patch the StatefulSet again to change the container’s image.

kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"k8s.gcr.io/nginx-slim:0.7"}]'
statefulset.apps/web patched

Delete a Pod in the StatefulSet.

kubectl delete po web-2
pod "web-2" deleted

Wait for the Pod to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

Notice that, even though the update strategy is RollingUpdate the StatefulSet controller restored the Pod with its original container. This is because the ordinal of the Pod is less than the partition specified by the updateStrategy.
Rolling Out a Canary

You can roll out a canary to test a modification by decrementing the partition you specified above.

Patch the StatefulSet to decrement the partition.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'
statefulset.apps/web patched

Wait for web-2 to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.7

When you changed the partition, the StatefulSet controller automatically updated the web-2 Pod because the Pod’s ordinal was greater than or equal to the partition.

Delete the web-1 Pod.

kubectl delete po web-1
pod "web-1" deleted

Wait for the web-1 Pod to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Running       0          6m
web-1     0/1       Terminating   0          6m
web-2     1/1       Running       0          2m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s

Get the web-1 Pods container.

kubectl get po web-1 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

web-1 was restored to its original configuration because the Pod’s ordinal was less than the partition. When a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet’s .spec.template is updated. If a Pod that has an ordinal less than the partition is deleted or otherwise terminated, it will be restored to its original configuration.
Phased Roll Outs

You can perform a phased roll out (e.g. a linear, geometric, or exponential roll out) using a partitioned rolling update in a similar manner to how you rolled out a canary. To perform a phased roll out, set the partition to the ordinal at which you want the controller to pause the update.

The partition is currently set to 2. Set the partition to 0.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":0}}}}'
statefulset.apps/web patched

Wait for all of the Pods in the StatefulSet to become Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3m
web-1     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          2m
web-1     1/1       Running   0         18s
web-0     1/1       Terminating   0         3m
web-0     1/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         3s

Get the Pod’s containers.

for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7

By moving the partition to 0, you allowed the StatefulSet controller to continue the update process.
On Delete

The OnDelete update strategy implements the legacy (1.6 and prior) behavior, When you select this update strategy, the StatefulSet controller will not automatically update Pods when a modification is made to the StatefulSet’s .spec.template field. This strategy can be selected by setting the .spec.template.updateStrategy.type to OnDelete.
Deleting StatefulSets

StatefulSet supports both Non-Cascading and Cascading deletion. In a Non-Cascading Delete, the StatefulSet’s Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are deleted.
Non-Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

Use kubectl delete to delete the StatefulSet. Make sure to supply the --cascade=false parameter to the command. This parameter tells Kubernetes to only delete the StatefulSet, and to not delete any of its Pods.

kubectl delete statefulset web --cascade=false
statefulset.apps "web" deleted

Get the Pods to examine their status.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          6m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          5m

Even though web has been deleted, all of the Pods are still Running and Ready. Delete web-0.

kubectl delete pod web-0
pod "web-0" deleted

Get the StatefulSet’s Pods.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          10m
web-2     1/1       Running   0          7m

As the web StatefulSet has been deleted, web-0 has not been relaunched.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In a second terminal, recreate the StatefulSet. Note that, unless you deleted the nginx Service ( which you should not have ), you will see an error indicating that the Service already exists.

kubectl apply -f web.yaml
statefulset.apps/web created
service/nginx unchanged

Ignore the error. It only indicates that an attempt was made to create the nginx Headless Service even though that Service already exists.

Examine the output of the kubectl get command running in the first terminal.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          16m
web-2     1/1       Running   0          2m
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         18s
web-2     1/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m

When the web StatefulSet was recreated, it first relaunched web-0. Since web-1 was already Running and Ready, when web-0 transitioned to Running and Ready, it simply adopted this Pod. Since you recreated the StatefulSet with replicas equal to 2, once web-0 had been recreated, and once web-1 had been determined to already be Running and Ready, web-2 was terminated.

Let’s take another look at the contents of the index.html file served by the Pods’ webservers.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you deleted both the StatefulSet and the web-0 Pod, it still serves the hostname originally entered into its index.html file. This is because the StatefulSet never deletes the PersistentVolumes associated with a Pod. When you recreated the StatefulSet and it relaunched web-0, its original PersistentVolume was remounted.
Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

In another terminal, delete the StatefulSet again. This time, omit the --cascade=false parameter.

kubectl delete statefulset web
statefulset.apps "web" deleted

Examine the output of the kubectl get command running in the first terminal, and wait for all of the Pods to transition to Terminating.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          11m
web-1     1/1       Running   0          27m
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Terminating   0          12m
web-1     1/1       Terminating   0         29m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m

As you saw in the Scaling Down section, the Pods are terminated one at a time, with respect to the reverse order of their ordinal indices. Before terminating a Pod, the StatefulSet controller waits for the Pod’s successor to be completely terminated.

Note that, while a cascading delete will delete the StatefulSet and its Pods, it will not delete the Headless Service associated with the StatefulSet. You must delete the nginx Service manually.

kubectl delete service nginx
service "nginx" deleted

Recreate the StatefulSet and Headless Service one more time.

kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

When all of the StatefulSet’s Pods transition to Running and Ready, retrieve the contents of their index.html files.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you completely deleted the StatefulSet, and all of its Pods, the Pods are recreated with their PersistentVolumes mounted, and web-0 and web-1 will still serve their hostnames.

Finally delete the web StatefulSet and the nginx service.

kubectl delete service nginx
service "nginx" deleted

kubectl delete statefulset web
statefulset "web" deleted

Pod Management Policy

For some distributed systems, the StatefulSet ordering guarantees are unnecessary and/or undesirable. These systems require only uniqueness and identity. To address this, in Kubernetes 1.7, we introduced .spec.podManagementPolicy to the StatefulSet API Object.
OrderedReady Pod Management

OrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above.
Parallel Pod Management

Parallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.
application/web/web-parallel.yaml [Copy application/web/web-parallel.yaml to clipboard]

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  podManagementPolicy: "Parallel"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

Download the example above, and save it to a file named web-parallel.yaml

This manifest is identical to the one you downloaded above except that the .spec.podManagementPolicy of the web StatefulSet is set to Parallel.

In one terminal, watch the Pods in the StatefulSet.

kubectl get po -l app=nginx -w

In another terminal, create the StatefulSet and Service in the manifest.

kubectl apply -f web-parallel.yaml
service/nginx created
statefulset.apps/web created

Examine the output of the kubectl get command that you executed in the first terminal.

kubectl get po -l app=nginx -w
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-1     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
web-1     1/1       Running   0         10s

The StatefulSet controller launched both web-0 and web-1 at the same time.

Keep the second terminal open, and, in another terminal window scale the StatefulSet.

kubectl scale statefulset/web --replicas=4
statefulset.apps/web scaled

Examine the output of the terminal where the kubectl get command is running.

web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         7s
web-3     0/1       ContainerCreating   0         7s
web-2     1/1       Running   0         10s
web-3     1/1       Running   0         26s

The StatefulSet controller launched two new Pods, and it did not wait for the first to become Running and Ready prior to launching the second.

Keep this terminal open, and in another terminal delete the web StatefulSet.

kubectl delete sts web

Again, examine the output of the kubectl get command running in the other terminal.

web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-1     1/1       Terminating   0         44m
web-0     1/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m

The StatefulSet controller deletes all Pods concurrently, it does not wait for a Pod’s ordinal successor to terminate prior to deleting that Pod.

Close the terminal where the kubectl get command is running and delete the nginx Service.

kubectl delete svc nginx

Cleaning up

You will need to delete the persistent storage media for the PersistentVolumes used in this tutorial. Follow the necessary steps, based on your environment, storage configuration, and provisioning method, to ensure that all storage is reclaimed.


###############################################################################
#
# Learn Kubernetes Basics - Part 4 - Explore your app
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 5 - Expose Your App Publicly
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 6 - Scale Your App
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 7 - Update Your App
#
###############################################################################






==============================================================================

LOGS

==============================================================================


thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          82s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          82s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          28s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          30s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          82s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          93s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          93s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          39s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          52s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          93s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          52s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          99s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          99s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          45s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          58s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          47s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          99s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          58s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          101s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          101s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          47s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          60s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          49s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          101s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          60s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending    0          2m23s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending    0          2m23s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          89s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          102s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          91s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          0/1     Init:0/1   0          3s      10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          2m23s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          102s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged configured
clusterrole.rbac.authorization.k8s.io/flannel unchanged
clusterrolebinding.rbac.authorization.k8s.io/flannel unchanged
serviceaccount/flannel unchanged
configmap/kube-flannel-cfg unchanged
daemonset.apps/kube-flannel-ds-amd64 unchanged
daemonset.apps/kube-flannel-ds-arm64 unchanged
daemonset.apps/kube-flannel-ds-arm unchanged
daemonset.apps/kube-flannel-ds-ppc64le unchanged
daemonset.apps/kube-flannel-ds-s390x unchanged
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m35s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m35s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          101s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          114s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          103s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          15s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m35s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          114s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m39s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m39s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          105s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          118s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          107s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          19s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m39s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          118s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m42s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m42s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          108s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m1s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          110s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          22s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m42s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m1s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending             0          2m46s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     ContainerCreating   0          2m46s   <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          112s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          2m5s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          114s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          26s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          2m46s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          2m5s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     ContainerCreating   0          2m50s   <none>       k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Running             0          2m50s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          116s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          2m9s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          118s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          30s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          2m50s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          2m9s    10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m53s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m53s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          119s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          33s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m53s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m12s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m55s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m55s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          35s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m55s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m57s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m57s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          37s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m57s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m16s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          3m      10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          3m      10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m19s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          40s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          3m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m19s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          3m3s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          3m3s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m9s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m11s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          43s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          3m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m22s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          4m30s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          4m30s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          3m36s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          3m49s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          3m38s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             0          11s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          2m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          4m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-c78r7                     0/1     ContainerCreating   0          11s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          3m49s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          4m34s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          4m34s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m40s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m53s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m42s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running   0          15s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          4m34s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running   0          15s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m53s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE    VERSION
k8s-master   Ready    master   5m3s   v1.16.3
k8s-slave1   Ready    <none>   24s    v1.16.3
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m52s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m52s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          5m58s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m11s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m33s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m32s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          2s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m52s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          2s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m33s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m11s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m54s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m54s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m13s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m35s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m34s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          4s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m54s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          4s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m35s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m13s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m56s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m56s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m15s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m4s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m37s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m36s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          6s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m56s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          6s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m37s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m15s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m57s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m57s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m38s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m37s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          7s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m57s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          7s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m38s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m16s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          6m59s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          6m59s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m18s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m7s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m40s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m39s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          9s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          6m59s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          9s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m40s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m18s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          7m      10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          7m      10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m19s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m41s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m40s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          10s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          7m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          10s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m41s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m19s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          7m2s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          7m2s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m43s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m42s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          12s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          7m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          12s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m43s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m21s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS            RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running           0          7m4s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running           0          7m4s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running           0          6m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running           0          6m23s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running           0          6m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running           1          2m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running           0          4m44s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     PodInitializing   0          14s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running           0          7m4s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running           0          14s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running           0          2m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running           0          6m23s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          7m6s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          7m6s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          6m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          6m25s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          6m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running   1          2m47s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          4m46s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          1/1     Running   0          16s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          7m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running   0          16s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running   0          2m47s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          6m25s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   7m33s   v1.16.3
k8s-slave1   Ready    <none>   2m54s   v1.16.3
k8s-slave2   Ready    <none>   23s     v1.16.3
thierry@k8s-master:~$ 







===============================================================================
===============================================================================

APPENDIX 1 - Kubernetes Node & Controllers architecture

===============================================================================
===============================================================================


========
1. Nodes
========

As seen above, a node is a worker machine in Kubernetes. A node may be a VM or 
physical machine (we then speak of a 'bare metal deployment'), depending on the 
cluster. Each node contains the services necessary to run pods and is managed 
by the master components. The services on a node include:
    - the container runtime,
    - kubelet
    - kube-proxy.


Node Status
===========

A node’s status contains the following information:
    - Addresses
    - Conditions
    - Capacity and Allocatable
    - Info

Node status and other details about a node can be displayed using below 
command:

$ kubectl describe node <insert-node-name-here>

Addresses
=========

The usage of these fields varies depending on your cloud provider or bare metal 
configuration.

    HostName:   The hostname as reported by the node’s kernel. Can be 
                overridden via the kubelet --hostname-override parameter.
    ExternalIP: Typically the IP address of the node that is externally 
                routable (available from outside the cluster).
    InternalIP: Typically the IP address of the node that is routable only 
                within the cluster.

Conditions
==========

The conditions field describes the status of all Running nodes. Examples of 
conditions include:

Node                Condition
Ready	            True if the node is healthy and ready to accept pods,
                    False if the node is not healthy and is not accepting pods, 
                    Unknown if the node controller has not heard from the node 
                      in the last node-monitor-grace-period (default is 40 
                      seconds)
MemoryPressure	    True if pressure exists on the node memory – that is, if 
                      the node memory is low; 
                    False otherwise
PIDPressure	        True if pressure exists on the processes – that is, if 
                      there are too many processes on the node; 
                    False otherwise
DiskPressure	    True if pressure exists on the disk size – that is, if 
                      the disk capacity is low; 
                    False otherwise
NetworkUnavailable	True if the network for the node is not correctly 
                      configured, 
                    False otherwise

The node condition is represented as a JSON object. For example, the following 
response describes a healthy node.

"conditions": [
  {
    "type": "Ready",
    "status": "True",
    "reason": "KubeletReady",
    "message": "kubelet is posting ready status",
    "lastHeartbeatTime": "2019-06-05T18:38:35Z",
    "lastTransitionTime": "2019-06-05T11:41:27Z"
  }
]


If the Status of the Ready condition remains Unknown or False for longer than 
the pod-eviction-timeout, an argument is passed to the kube-controller-manager 
and all the Pods on the node are scheduled for deletion by the Node Controller. 
The default eviction timeout duration is five minutes. In some cases when the 
node is unreachable, the apiserver is unable to communicate with the kubelet 
on the node. The decision to delete the pods cannot be communicated to the 
kubelet until communication with the apiserver is re-established. In the 
meantime, the pods that are scheduled for deletion may continue to run on the 
partitioned node.

In versions of Kubernetes prior to 1.5, the node controller would force delete 
these unreachable pods from the apiserver. However, in 1.5 and higher, the 
node controller does not force delete pods until it is confirmed that they 
have stopped running in the cluster. You can see the pods that might be 
running on an unreachable node as being in the Terminating or Unknown state. 
In cases where Kubernetes cannot deduce from the underlying infrastructure if 
a node has permanently left a cluster, the cluster administrator may need to 
delete the node object by hand. Deleting the node object from Kubernetes 
causes all the Pod objects running on the node to be deleted from the 
apiserver, and frees up their names.

In version 1.12, TaintNodesByCondition feature is promoted to beta, so node 
lifecycle controller automatically creates taints that represent conditions. 
Similarly the scheduler ignores conditions when considering a Node; instead it 
looks at the Node’s taints and a Pod’s tolerations.

Now users can choose between the old scheduling model and a new, more flexible 
scheduling model. A Pod that does not have any tolerations gets scheduled 
according to the old model. But a Pod that tolerates the taints of a particular 
Node can be scheduled on that Node.

    Caution: Enabling this feature creates a small delay between the time when 
      a condition is observed and when a taint is created. This delay is 
      usually less than one second, but it can increase the number of Pods 
      that are successfully scheduled but rejected by the kubelet.

Capacity and Allocatable
========================

Describes the resources available on the node: CPU, memory and the maximum 
number of pods that can be scheduled onto the node.

The fields in the capacity block indicate the total amount of resources that 
a Node has. The allocatable block indicates the amount of resources on a Node 
that is available to be consumed by normal Pods.

You may read more about capacity and allocatable resources while learning how 
to reserve compute resources on a Node.

Info
====

Describes general information about the node, such as kernel version, 
Kubernetes version (kubelet and kube-proxy version), Docker version (if used), 
and OS name. This information is gathered by Kubelet from the node.

=============
2. Management
=============

Unlike pods and services, a node is not inherently created by Kubernetes: it 
is created externally by cloud providers like Google Compute Engine, or it 
exists in your pool of physical or virtual machines. So when Kubernetes 
creates a node, it creates an object that represents the node. After creation, 
Kubernetes checks whether the node is valid or not. For example, if you try to 
create a node from the following content:

{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}

Kubernetes creates a node object internally (the representation), and 
validates the node by health checking based on the metadata.name field. If 
the node is valid – that is, if all necessary services are running – it is 
eligible to run a pod. Otherwise, it is ignored for any cluster activity until 
it becomes valid.

    Note: Kubernetes keeps the object for the invalid node and keeps checking 
      to see whether it becomes valid. You must explicitly delete the Node 
      object to stop this process.

Currently, there are three components that interact with the Kubernetes node 
interface: node controller, kubelet, and kubectl.

Node Controller
===============

The node controller is a Kubernetes master component which manages various 
aspects of nodes.

The node controller has multiple roles in a node’s life.

    - The first is assigning a CIDR block to the node when it is registered (if 
      CIDR assignment is turned on).

    - The second is keeping the node controller’s internal list of nodes up to 
      date with the cloud provider’s list of available machines. When running 
      in a cloud environment, whenever a node is unhealthy, the node controller 
      asks the cloud provider if the VM for that node is still available. If 
      not, the node controller deletes the node from its list of nodes.

    - The third is monitoring the nodes’ health. The node controller is 
      responsible for updating the NodeReady condition of NodeStatus to 
      ConditionUnknown when a node becomes unreachable (i.e. the node 
      controller stops receiving heartbeats for some reason, e.g. due to the 
      node being down), and then later evicting all the pods from the node 
      (using graceful termination) if the node continues to be unreachable. 
      (The default timeouts are 40s to start reporting ConditionUnknown and 
      5m after that to start evicting pods.) The node controller checks the 
      state of each node every --node-monitor-period seconds.

In versions of Kubernetes prior to 1.13, NodeStatus is the heartbeat from the 
node. Node lease feature is enabled by default since 1.14 as a beta feature 
(feature gate NodeLease, KEP-0009). When node lease feature is enabled, each 
node has an associated Lease object in kube-node-lease namespace that is 
renewed by the node periodically, and both NodeStatus and node lease are 
treated as heartbeats from the node. Node leases are renewed frequently while 
NodeStatus is reported from node to master only when there is some change or 
enough time has passed (default is 1 minute, which is longer than the default 
timeout of 40 seconds for unreachable nodes). Since node lease is much more 
lightweight than NodeStatus, this feature makes node heartbeat significantly 
cheaper from both scalability and performance perspectives.

In Kubernetes 1.4, we updated the logic of the node controller to better 
handle cases when a large number of nodes have problems with reaching the 
master (e.g. because the master has networking problem). Starting with 1.4, 
the node controller looks at the state of all nodes in the cluster when making 
a decision about pod eviction.

In most cases, node controller limits the eviction rate to --node-eviction-rate 
(default 0.1) per second, meaning it won’t evict pods from more than 1 node 
per 10 seconds.

The node eviction behavior changes when a node in a given availability zone 
becomes unhealthy. The node controller checks what percentage of nodes in the 
zone are unhealthy (NodeReady condition is ConditionUnknown or ConditionFalse) 
at the same time. If the fraction of unhealthy nodes is at least 
--unhealthy-zone-threshold (default 0.55) then the eviction rate is reduced: 
if the cluster is small (i.e. has less than or equal to 
--large-cluster-size-threshold nodes - default 50) then evictions are stopped, 
otherwise the eviction rate is reduced to --secondary-node-eviction-rate 
(default 0.01) per second. The reason these policies are implemented per 
availability zone is because one availability zone might become partitioned 
from the master while the others remain connected. If your cluster does not 
span multiple cloud provider availability zones, then there is only one 
availability zone (the whole cluster).

A key reason for spreading your nodes across availability zones is so that the 
workload can be shifted to healthy zones when one entire zone goes down. 
Therefore, if all nodes in a zone are unhealthy then node controller evicts at 
the normal rate --node-eviction-rate. The corner case is when all zones are 
completely unhealthy (i.e. there are no healthy nodes in the cluster). In such 
case, the node controller assumes that there’s some problem with master 
connectivity and stops all evictions until some connectivity is restored.

Starting in Kubernetes 1.6, the NodeController is also responsible for evicting 
pods that are running on nodes with NoExecute taints, when the pods do not 
tolerate the taints. Additionally, as an alpha feature that is disabled by 
default, the NodeController is responsible for adding taints corresponding 
to node problems like node unreachable or not ready. See this documentation 
for details about NoExecute taints and the alpha feature.

Starting in version 1.8, the node controller can be made responsible for 
creating taints that represent Node conditions. This is an alpha feature of 
version 1.8.

Self-Registration of Nodes
==========================

When the kubelet flag --register-node is true (the default), the kubelet will 
attempt to register itself with the API server. This is the preferred pattern, 
used by most distros.

For self-registration, the kubelet is started with the following options:

    --kubeconfig - Path to credentials to authenticate itself to the apiserver.
    --cloud-provider - How to talk to a cloud provider to read metadata about 
        itself.
    --register-node - Automatically register with the API server.
    --register-with-taints - Register the node with the given list of taints 
        (comma separated <key>=<value>:<effect>). No-op if register-node is 
        false.
    --node-ip - IP address of the node.
    --node-labels - Labels to add when registering the node in the cluster 
        (see label restrictions enforced by the NodeRestriction admission 
        plugin in 1.13+).
    --node-status-update-frequency - Specifies how often kubelet posts node 
        status to master.

When the Node authorization mode and NodeRestriction admission plugin are 
enabled, kubelets are only authorized to create/modify their own Node resource.


Manual Node Administration
==========================

A cluster administrator can create and modify node objects.

If the administrator wishes to create node objects manually, set the kubelet 
flag --register-node=false.

The administrator can modify node resources (regardless of the setting of 
--register-node). Modifications include setting labels on the node and marking 
it unschedulable.

Labels on nodes can be used in conjunction with node selectors on pods to 
control scheduling, e.g. to constrain a pod to only be eligible to run on a 
subset of the nodes.

Marking a node as unschedulable prevents new pods from being scheduled to that 
node, but does not affect any existing pods on the node. This is useful as a 
preparatory step before a node reboot, etc. For example, to mark a node 
unschedulable, run this command:

$ kubectl cordon $NODENAME

    Note: Pods created by a DaemonSet controller bypass the Kubernetes 
        scheduler and do not respect the unschedulable attribute on a node. 
        This assumes that daemons belong on the machine even if it is being 
        drained of applications while it prepares for a reboot.


Node capacity
=============

The capacity of the node (number of cpus and amount of memory) is part of the 
node object. Normally, nodes register themselves and report their capacity 
when creating the node object. If you are doing manual node administration, 
then you need to set node capacity when adding a node.

The Kubernetes scheduler ensures that there are enough resources for all the 
pods on a node. It checks that the sum of the requests of containers on the 
node is no greater than the node capacity. It includes all containers started 
by the kubelet, but not containers started directly by the container runtime 
nor any process running outside of the containers.

If you want to explicitly reserve resources for non-Pod processes, follow this 
tutorial to reserve resources for system daemons.




===============================================================================
===============================================================================

APPENDIX 2 - ReplicaSet, Deployment, StatefulSet, DaemonSet

===============================================================================
===============================================================================



==============
1 - ReplicaSet
==============


A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at 
any given time. As such, it is often used to guarantee the availability of a 
specified number of identical Pods.


1.1 - How a ReplicaSet works
============================

A ReplicaSet is defined with fields, including a selector that specifies how 
to identify Pods it can acquire, a number of replicas indicating how many Pods 
it should be maintaining, and a pod template specifying the data of new Pods 
it should create to meet the number of replicas criteria. A ReplicaSet then 
fulfills its purpose by creating and deleting Pods as needed to reach the 
desired number. When a ReplicaSet needs to create new Pods, it uses its Pod 
template.

The link a ReplicaSet has to its Pods is via the Pods’ metadata.ownerReferences 
field, which specifies what resource the current object is owned by. All Pods 
acquired by a ReplicaSet have their owning ReplicaSet’s identifying 
information within their ownerReferences field. It’s through this link that 
the ReplicaSet knows of the state of the Pods it is maintaining and plans 
accordingly.

A ReplicaSet identifies new Pods to acquire by using its selector. If there is 
a Pod that has no OwnerReference or the OwnerReference is not a Controller and 
it matches a ReplicaSet’s selector, it will be immediately acquired by said 
ReplicaSet.


1.2 - When to use a ReplicaSet
==============================

A ReplicaSet ensures that a specified number of pod replicas are running at 
any given time. However, a Deployment is a higher-level concept that manages 
ReplicaSets and provides declarative updates to Pods along with a lot of other 
useful features. Therefore, we recommend using Deployments instead of directly 
using ReplicaSets, unless you require custom update orchestration or don’t 
require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects: 
use a Deployment instead, and define your application in the spec section.

File: controllers/frontend.yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3


Saving this manifest into frontend.yaml and submitting it to a Kubernetes 
cluster will create the defined ReplicaSet and the Pods that it manages.

$ kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml

You can then get the current ReplicaSets deployed:

$ kubectl get rs

And see the frontend one you created:

NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s

You can also check on the state of the replicaset:

$ kubectl describe rs/frontend

And you will see output similar to:

Name:		      frontend
Namespace:	      default
Selector:	      tier=frontend,tier in (frontend)
Labels:		      app=guestbook
                  tier=frontend
Annotations:	  <none>
Replicas:	      3 current / 3 desired
Pods Status:	  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:         app=guestbook
                  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         80/TCP
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:
      GET_HOSTS_FROM:   dns
    Mounts:             <none>
  Volumes:              <none>
Events:
  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----                -------------    --------    ------            -------
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-qhloh
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-9si5l

  
And lastly you can check for the Pods brought up:

$vkubectl get Pods

You should see Pod information similar to:

NAME             READY     STATUS    RESTARTS   AGE
frontend-9si5l   1/1       Running   0          1m
frontend-dnjpy   1/1       Running   0          1m
frontend-qhloh   1/1       Running   0          1m


You can also verify that the owner reference of these pods is set to the 
frontend ReplicaSet. To do this, get the yaml of one of the Pods running:

$ kubectl get pods frontend-9si5l -o yaml

The output will look similar to this, with the frontend ReplicaSet’s info set 
in the metadata’s ownerReferences field:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: 2019-01-31T17:20:41Z
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-9si5l
  namespace: default
  ownerReferences:
  - apiVersion: extensions/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: 892a2330-257c-11e9-aecd-025000000001
...

Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to 
make sure that the bare Pods do not have labels which match the selector of 
one of your ReplicaSets. The reason for this is because a ReplicaSet is not 
limited to owning Pods specified by its template– it can acquire other Pods 
in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the 
following manifest:

File: pods/pod-rs.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: hello1
    image: gcr.io/google-samples/hello-app:2.0

---

apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    tier: frontend
spec:
  containers:
  - name: hello2
    image: gcr.io/google-samples/hello-app:1.0


As those Pods do not have a Controller (or any object) as their owner reference 
and match the selector of the frontend ReplicaSet, they will immediately be 
acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and 
has set up its initial Pod replicas to fulfill its replica count requirement:

$ kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml

The new Pods will be acquired by the ReplicaSet, and then immediately 
terminated as the ReplicaSet would be over its desired count.

Fetching the Pods:

$ kubectl get Pods

The output shows that the new Pods are either already terminated, or in the 
process of being terminated:

NAME             READY   STATUS        RESTARTS   AGE
frontend-9si5l   1/1     Running       0          1m
frontend-dnjpy   1/1     Running       0          1m
frontend-qhloh   1/1     Running       0          1m
pod2             0/1     Terminating   0          4s

If you create the Pods first:

$ kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml

And then create the ReplicaSet however:

$ kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml

You shall see that the ReplicaSet has acquired the Pods and has only created 
new ones according to its spec until the number of its new Pods and the 
original matches its desired count. As fetching the Pods:

$ kubectl get Pods

Will reveal in its output:

NAME             READY   STATUS    RESTARTS   AGE
frontend-pxj4r   1/1     Running   0          5s
pod1             1/1     Running   0          13s
pod2             1/1     Running   0          13s

In this manner, a ReplicaSet can own a non-homogenous set of Pods.


1.3 - Writing a ReplicaSet manifest
===================================

As with all other Kubernetes API objects, a ReplicaSet needs the apiVersion, 
kind, and metadata fields. For ReplicaSets, the kind is always just ReplicaSet. 
In Kubernetes 1.9 the API version apps/v1 on the ReplicaSet kind is the current 
version and is enabled by default. The API version apps/v1beta2 is deprecated. 
Refer to the first lines of the frontend.yaml example for guidance.

A ReplicaSet also needs a .spec section.

a) Pod Template
===============

The .spec.template is a pod template which is also required to have labels in 
place. In our frontend.yaml example we had one label: tier: frontend. Be 
careful not to overlap with the selectors of other controllers, lest they try 
to adopt this Pod.

For the template’s restart policy field, .spec.template.spec.restartPolicy, 
the only allowed value is Always, which is the default.

b) Pod Selector
===============

The .spec.selector field is a label selector. As discussed earlier these are 
the labels used to identify potential Pods to acquire. In our frontend.yaml 
example, the selector was:

matchLabels:
	tier: frontend

In the ReplicaSet, .spec.template.metadata.labels must match spec.selector, or 
it will be rejected by the API.

    Note: For 2 ReplicaSets specifying the same .spec.selector but different 
      .spec.template.metadata.labels and .spec.template.spec fields, each 
      ReplicaSet ignores the Pods created by the other ReplicaSet.

c) Replicas
===========

You can specify how many Pods should run concurrently by setting 
.spec.replicas. The ReplicaSet will create/delete its Pods to match this number.

If you do not specify .spec.replicas, then it defaults to 1.


1.4 - Working with ReplicaSets
==============================


a) Deleting a ReplicaSet and its Pods
=====================================

To delete a ReplicaSet and all of its Pods, use kubectl delete. The Garbage 
collector automatically deletes all of the dependent Pods by default.

When using the REST API or the client-go library, you must set 
propagationPolicy to Background or Foreground in the -d option. For example:

$ kubectl proxy --port=8080
$ curl -X DELETE  'localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
> -H "Content-Type: application/json"


b) Deleting just a ReplicaSet
=============================

You can delete a ReplicaSet without affecting any of its Pods using kubectl 
delete with the --cascade=false option. When using the REST API or the 
client-go library, you must set propagationPolicy to Orphan. For example:

$ kubectl proxy --port=8080
$ curl -X DELETE  'localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \
> -H "Content-Type: application/json"

Once the original is deleted, you can create a new ReplicaSet to replace it. 
As long as the old and new .spec.selector are the same, then the new one will 
adopt the old Pods. However, it will not make any effort to make existing Pods 
match a new, different pod template. To update Pods to a new spec in a 
controlled way, use a Deployment, as ReplicaSets do not support a rolling 
update directly.


c) Isolating Pods from a ReplicaSet
===================================

You can remove Pods from a ReplicaSet by changing their labels. This technique 
may be used to remove Pods from service for debugging, data recovery, etc. Pods 
that are removed in this way will be replaced automatically (assuming that 
the number of replicas is not also changed).


d) Scaling a ReplicaSet
=======================

A ReplicaSet can be easily scaled up or down by simply updating the 
.spec.replicas field. The ReplicaSet controller ensures that a desired number 
of Pods with a matching label selector are available and operational.


e) ReplicaSet as a Horizontal Pod Autoscaler Target
===================================================

A ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA). That 
is, a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA 
targeting the ReplicaSet we created in the previous example

File: controllers/hpa-rs.yaml

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50

Saving this manifest into hpa-rs.yaml and submitting it to a Kubernetes 
cluster should create the defined HPA that autoscales the target ReplicaSet 
depending on the CPU usage of the replicated Pods.

$ kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml

Alternatively, you can use the kubectl autoscale command to accomplish the 
same (and it’s easier!)

$ kubectl autoscale rs frontend --max=10


1.5 - Alternatives to ReplicaSet
================================


a) Deployment (recommended)
===========================
 
Deployment is an object which can own ReplicaSets and update them and their 
Pods via declarative, server-side rolling updates. While ReplicaSets can be 
used independently, today they’re mainly used by Deployments as a mechanism to
orchestrate Pod creation, deletion and updates. When you use Deployments you 
don’t have to worry about managing the ReplicaSets that they create. 
Deployments own and manage their ReplicaSets. As such, it is recommended to 
use Deployments when you want ReplicaSets.


b) Bare Pods
============

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods 
that are deleted or terminated for any reason, such as in the case of node 
failure or disruptive node maintenance, such as a kernel upgrade. For this 
reason, we recommend that you use a ReplicaSet even if your application 
requires only a single Pod. Think of it similarly to a process supervisor, 
only it supervises multiple Pods across multiple nodes instead of individual
processes on a single node. A ReplicaSet delegates local container restarts to 
some agent on the node (for example, Kubelet or Docker).


c) Job
======

Use a Job instead of a ReplicaSet for Pods that are expected to terminate on 
their own (that is, batch jobs).


d) DaemonSet
============

Use a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level
function, such as machine monitoring or machine logging. These Pods have a 
lifetime that is tied to a machine lifetime: the Pod needs to be running on 
the machine before other Pods start, and are safe to terminate when the 
machine is otherwise ready to be rebooted/shutdown.


e) ReplicationController
========================

ReplicaSets are the successors to ReplicationControllers. The two serve the 
same purpose, and behave similarly, except that a ReplicationController does 
not support set-based selector requirements as described in the labels user 
guide. As such, ReplicaSets are preferred over ReplicationControllers.




===============
2 - Deployments
===============


A Deployment provides declarative updates for Pods and ReplicaSets.

You describe a desired state in a Deployment, and the Deployment Controller 
changes the actual state to the desired state at a controlled rate. You can 
define Deployments to create new ReplicaSets, or to remove existing 
Deployments and adopt all their resources with new Deployments.

    Note: Do not manage ReplicaSets owned by a Deployment. Consider opening an 
      issue in the main Kubernetes repository if your use case is not covered 
      below.


2.1 - Use Case
==============

The following are typical use cases for Deployments:

    - Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods 
      in the background. Check the status of the rollout to see if it succeeds 
      or not.
    - Declare the new state of the Pods by updating the PodTemplateSpec of the
      Deployment. A new ReplicaSet is created and the Deployment manages 
      moving the Pods from the old ReplicaSet to the new one at a controlled 
      rate. Each new ReplicaSet updates the revision of the Deployment.
    - Rollback to an earlier Deployment revision if the current state of the
      Deployment is not stable. Each rollback updates the revision of the 
      Deployment.
    - Scale up the Deployment to facilitate more load.
    - Pause the Deployment to apply multiple fixes to its PodTemplateSpec and 
      then resume it to start a new rollout.
    - Use the status of the Deployment as an indicator that a rollout has stuck.
    - Clean up older ReplicaSets that you don’t need anymore.


2.2 - Creating a Deployment
===========================

The following is an example of a Deployment. It creates a ReplicaSet to bring 
up three nginx Pods:

File: controllers/nginx-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

In this example:

    - A Deployment named nginx-deployment is created, indicated by the 
      .metadata.name field.
    - The Deployment creates three replicated Pods, indicated by the replicas 
      field.

    - The selector field defines how the Deployment finds which Pods to manage. 
      In this case, you simply select a label that is defined in the Pod 
      template (app: nginx). However, more sophisticated selection rules are 
      possible, as long as the Pod template itself satisfies the rule.

        Note: The matchLabels field is a map of {key,value} pairs. A single 
          {key,value} in the matchLabels map is equivalent to an element of 
          matchExpressions, whose key field is “key” the operator is “In”, and 
          the values array contains only “value”. All of the requirements, 
          from both matchLabels and matchExpressions, must be satisfied in 
          order to match.

    - The template field contains the following sub-fields:
        * The Pods are labeled app: nginx using the labels field.
        * The Pod template’s specification, or .template.spec field, indicates 
          that the Pods run one container, nginx, which runs the nginx Docker 
          Hub image at version 1.7.9.
        * Create one container and name it nginx using the name field.


Follow the steps given below to create the above Deployment:

Before you begin, make sure your Kubernetes cluster is up and running.

a) Create the Deployment
========================

Create the Deployment by running the following command:

    Note: You may specify the –record flag to write the command executed in 
        the resource annotation kubernetes.io/change-cause. It is useful for 
        future introspection. For example, to see the commands executed in 
        each Deployment revision.

$ kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml

Run kubectl get deployments to check if the Deployment was created. If the 
Deployment is still being created, the output is similar to the following:

NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s

When you inspect the Deployments in your cluster, the following fields are 
displayed:

    - NAME lists the names of the Deployments in the cluster.
    - DESIRED displays the desired number of replicas of the application, 
      which you define when you create the Deployment. This is the desired 
      state.
    - CURRENT displays how many replicas are currently running.
    - UP-TO-DATE displays the number of replicas that have been updated to 
      achieve the desired state.
    - AVAILABLE displays how many replicas of the application are available 
      to your users.
    - AGE displays the amount of time that the application has been running.

Notice how the number of desired replicas is 3 according to .spec.replicas 
field.

To see the Deployment rollout status, run the followig command:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment.apps/nginx-deployment successfully rolled out

Run the following command again a few seconds later:

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s

Notice that the Deployment has created all three replicas, and all replicas 
are up-to-date (they contain the latest Pod template) and available.

To see the ReplicaSet (rs) created by the Deployment, run the follwing command:

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s

Notice that the name of the ReplicaSet is always formatted as 
[DEPLOYMENT-NAME]-[RANDOM-STRING]. The random string is randomly generated and 
uses the pod-template-hash as a seed.

To see the labels automatically generated for each Pod, run the following command:

$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453

The created ReplicaSet ensures that there are three nginx Pods.

    Note: You must specify an appropriate selector and Pod template labels 
      in a Deployment (in this case, app: nginx). Do not overlap labels or 
      selectors with other controllers (including other Deployments and 
      StatefulSets). Kubernetes doesn’t stop you from overlapping, and if 
      multiple controllers have overlapping selectors those controllers might 
      conflict and behave unexpectedly.

b) Pod-template-hash label
==========================

    Note: Do not change this label.

The pod-template-hash label is added by the Deployment controller to every 
ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It 
is generated by hashing the PodTemplate of the ReplicaSet and using the 
resulting hash as the label value that is added to the ReplicaSet selector, 
Pod template labels, and in any existing Pods that the ReplicaSet might have.


2.3 - Updating a Deployment
===========================


a) Updating the deployment
==========================

Note: A Deployment’s rollout is triggered if and only if the Deployment’s 
      Pod template (that is, .spec.template) is changed, for example if the 
      labels or container images of the template are updated. Other updates, 
      such as scaling the Deployment, do not trigger a rollout.

Follow the steps given below to update your Deployment:

Let’s update the nginx Pods to use the nginx:1.9.1 image instead of the 
nginx:1.7.9 image.

$ kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1

or simply use the following command:

$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 --record
deployment.apps/nginx-deployment image updated

Alternatively, you can edit the Deployment and change 
.spec.template.spec.containers[0].image from nginx:1.7.9 to nginx:1.9.1:

$ kubectl edit deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment edited

To see the rollout status, run:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
... and finally it shows:
deployment.apps/nginx-deployment successfully rolled out


b) Get more details on your updated Deployment:
===============================================

After the rollout succeeds, you can view the Deployment by running:
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s

Run the following command to see that the Deployment updated the Pods by 
creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling 
down the old ReplicaSet to 0 replicas.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s

Running 'get pods' should now show only the new Pods:

$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s

Next time you want to update these Pods, you only need to update the 
Deployment’s Pod template again.

Deployment ensures that only a certain number of Pods are down while they are 
being updated. By default, it ensures that at least 75% of the desired number 
of Pods are up (25% max unavailable).

Deployment also ensures that only a certain number of Pods are created above 
the desired number of Pods. By default, it ensures that at most 125% of the 
desired number of Pods are up (25% max surge).

For example, if you look at the above Deployment closely, you will see that 
it first created a new Pod, then deleted some old Pods, and created new ones. 
It does not kill old Pods until a sufficient number of new Pods have come up, 
and does not create new Pods until a sufficient number of old Pods have been 
killed. It makes sure that at least 2 Pods are available and that at max 4 
Pods in total are available.

c) Get details of your Deployment:
==================================

$ kubectl describe deployments
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
Labels:  app=nginx
 Containers:
  nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0

Here you see that when you first created the Deployment, it created a 
ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas 
directly. When you updated the Deployment, it created a new ReplicaSet 
(nginx-deployment-1564180365) and scaled it up to 1 and then scaled down the 
old ReplicaSet to 2, so that at least 2 Pods were available and at most 4 Pods 
were created at all times. It then continued scaling up and down the new and 
the old ReplicaSet, with the same rolling update strategy. Finally, you’ll 
have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is 
scaled down to 0.


d) Rollover (aka multiple updates in-flight)
============================================

Each time a new Deployment is observed by the Deployment controller, a 
ReplicaSet is created to bring up the desired Pods. If the Deployment is 
updated, the existing ReplicaSet that controls Pods whose labels match 
.spec.selector but whose template does not match .spec.template are scaled 
down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old 
ReplicaSets is scaled to 0.

If you update a Deployment while an existing rollout is in progress, the 
Deployment creates a new ReplicaSet as per the update and start scaling that 
up, and rolls over the ReplicaSet that it was scaling up previously – it will 
add it to its list of old ReplicaSets and start scaling it down.

For example, suppose you create a Deployment to create 5 replicas of 
nginx:1.7.9, but then update the Deployment to create 5 replicas of 
nginx:1.9.1, when only 3 replicas of nginx:1.7.9 had been created. In that 
case, the Deployment immediately starts killing the 3 nginx:1.7.9 Pods that it 
had created, and starts creating nginx:1.9.1 Pods. It does not wait for the 5 
replicas of nginx:1.7.9 to be created before changing course.

e) Label selector updates
=========================

It is generally discouraged to make label selector updates and it is suggested 
to plan your selectors up front. In any case, if you need to perform a label 
selector update, exercise great caution and make sure you have grasped all of 
the implications.

    Note: In API version apps/v1, a Deployment’s label selector is immutable 
      after it gets created.

    - Selector additions require the Pod template labels in the Deployment 
      spec to be updated with the new label too, otherwise a validation error 
      is returned. This change is a non-overlapping one, meaning that the new 
      selector does not select ReplicaSets and Pods created with the old 
      selector, resulting in orphaning all old ReplicaSets and creating a new 
      ReplicaSet.
    - Selector updates changes the existing value in a selector key – result 
      in the same behavior as additions.
    - Selector removals removes an existing key from the Deployment selector – 
      do not require any changes in the Pod template labels. Existing 
      ReplicaSets are not orphaned, and a new ReplicaSet is not created, but 
      note that the removed label still exists in any existing Pods and 
      ReplicaSets.


2.4 - Rolling Back a Deployment
===============================

Sometimes, you may want to rollback a Deployment; for example, when the 
Deployment is not stable, such as crash looping. By default, all of the 
Deployment’s rollout history is kept in the system so that you can rollback 
anytime you want (you can change that by modifying revision history limit).

    Note: A Deployment’s revision is created when a Deployment’s rollout is 
      triggered. This means that the new revision is created if and only if 
      the Deployment’s Pod template (.spec.template) is changed, for example 
      if you update the labels or container images of the template. Other 
      updates, such as scaling the Deployment, do not create a Deployment 
      revision, so that you can facilitate simultaneous manual- or 
      auto-scaling. This means that when you roll back to an earlier revision, 
      only the Deployment’s Pod template part is rolled back.

Suppose that you made a typo while updating the Deployment, by putting the 
image name as nginx:1.91 instead of nginx:1.9.1:

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true
deployment.apps/nginx-deployment image updated

The rollout gets stuck. You can verify it by checking the rollout status:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...

Press Ctrl-C to stop the above rollout status watch. For more information on 
stuck rollouts, read more here.

You see that the number of old replicas (nginx-deployment-1564180365 and 
nginx-deployment-2035384211) is 2, and new replicas 
(nginx-deployment-3066724191) is 1.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s

Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is 
stuck in an image pull loop.

$ kubectl get pods
NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s

    Note: The Deployment controller stops the bad rollout automatically, and 
      stops scaling up the new ReplicaSet. This depends on the rollingUpdate 
      parameters (maxUnavailable specifically) that you have specified. 
      Kubernetes by default sets the value to 25%.

a) Get the description of the Deployment:
=========================================

$ kubectl describe deployment
Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1

To fix this, you need to rollback to a previous revision of Deployment that is 
stable.

b) Checking Rollout History of a Deployment
===========================================

Follow the steps given below to check the rollout history:

First, check the revisions of this Deployment:

$ kubectl rollout history deployment.v1.apps/nginx-deployment
deployments "nginx-deployment"
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true

CHANGE-CAUSE is copied from the Deployment annotation 
kubernetes.io/change-cause to its revisions upon creation. You can specify the
CHANGE-CAUSE message by:
    - Annotating the Deployment with kubectl annotate 
      deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"
    - Append the --record flag to save the kubectl command that is making 
      changes to the resource.
    - Manually editing the manifest of the resource.

To see the details of each revision, run:

$ kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
deployments "nginx-deployment" revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
  Containers:
    nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      <none>
  No volumes.


c) Rolling Back to a Previous Revision
======================================

Follow the steps given below to rollback the Deployment from the current 
version to the previous version, which is version 2.

Now you’ve decided to undo the current rollout and rollback to the previous 
revision:

$ kubectl rollout undo deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment

Alternatively, you can rollback to a specific revision by specifying it with 
--to-revision:

$ kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2
deployment.apps/nginx-deployment

For more details about rollout related commands, read kubectl rollout.

The Deployment is now rolled back to a previous stable revision. As you can 
see, a DeploymentRollback event for rolling back to revision 2 is generated 
from Deployment controller.


To check if the rollback was successful and the Deployment is running as expected, run:

$ kubectl get deployment nginx-deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m

Get the description of the Deployment:

$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment "nginx-deployment" to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0


2.5 - Scaling a Deployment
==========================

You can scale a Deployment by using the following command:

$ kubectl scale deployment.v1.apps/nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

Assuming horizontal Pod autoscaling is enabled in your cluster, you can setup 
an autoscaler for your Deployment and choose the minimum and maximum number of 
Pods you want to run based on the CPU utilization of your existing Pods.

$ kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80
deployment.apps/nginx-deployment scaled

a) Proportional scaling

RollingUpdate Deployments support running multiple versions of an application 
at the same time. When you or an autoscaler scales a RollingUpdate Deployment 
that is in the middle of a rollout (either in progress or paused), the 
Deployment controller balances the additional replicas in the existing active 
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called 
proportional scaling.

For example, you are running a Deployment with 10 replicas, maxSurge=3, and 
maxUnavailable=2.
Ensure that the 10 replicas in your Deployment are running.

$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s

You update to a new image which happens to be unresolvable from inside the 
cluster.

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag
deployment.apps/nginx-deployment image updated

The image update starts a new rollout with ReplicaSet 
nginx-deployment-1989198191, but it’s blocked due to the maxUnavailable 
requirement that you mentioned above. Check out the rollout status:

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m

Then a new scaling request for the Deployment comes along. The autoscaler 
increments the Deployment replicas to 15. The Deployment controller needs to 
decide where to add these new 5 replicas. If you weren’t using proportional 
scaling, all 5 of them would be added in the new ReplicaSet. With proportional 
scaling, you spread the additional replicas across all ReplicaSets. Bigger 
proportions go to the ReplicaSets with the most replicas and lower proportions 
go to ReplicaSets with less replicas. Any leftovers are added to the 
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not 
scaled up.

In our example above, 3 replicas are added to the old ReplicaSet and 2 
replicas are added to the new ReplicaSet. The rollout process should 
eventually move all replicas to the new ReplicaSet, assuming the new replicas 
become healthy. To confirm this, run:

$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m

The rollout status confirms how the replicas were added to each ReplicaSet.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m


2.6 - Pausing and Resuming a Deployment
=======================================

You can pause a Deployment before triggering one or more updates and then 
resume it. This allows you to apply multiple fixes in between pausing and 
resuming without triggering unnecessary rollouts.

For example, with a Deployment that was just created: Get the Deployment details:

$ kubectl get deploy
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m

Get the rollout status:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m

Pause by running the following command:

$ kubectl rollout pause deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment paused

Then update the image of the Deployment:

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1
deployment.apps/nginx-deployment image updated

Notice that no new rollout started:

$ kubectl rollout history deployment.v1.apps/nginx-deployment
deployments "nginx"
REVISION  CHANGE-CAUSE
1   <none>

Get the rollout status to ensure that the Deployment is updated successfully:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m

You can make as many updates as you wish, for example, update the resources 
that will be used:

$ kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
deployment.apps/nginx-deployment resource requirements updated

The initial state of the Deployment prior to pausing it will continue its 
function, but new updates to the Deployment will not have any effect as long 
as the Deployment is paused.

Eventually, resume the Deployment and observe a new ReplicaSet coming up with 
all the new updates:

$ kubectl rollout resume deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment resumed

Watch the status of the rollout until it’s done.

$ kubectl get rs -w
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s

Get the status of the latest rollout:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s

    Note: You cannot rollback a paused Deployment until you resume it.


2.7 - Deployment status
=======================

A Deployment enters various states during its lifecycle. It can be progressing 
while rolling out a new ReplicaSet, it can be complete, or it can fail to 
progress.


a) Progressing Deployment
=========================

Kubernetes marks a Deployment as progressing when one of the following tasks 
is performed:

    - The Deployment creates a new ReplicaSet.
    - The Deployment is scaling up its newest ReplicaSet.
    - The Deployment is scaling down its older ReplicaSet(s).
    - New Pods become ready or available (ready for at least MinReadySeconds).

You can monitor the progress for a Deployment by using kubectl rollout status.


b) Complete Deployment
======================

Kubernetes marks a Deployment as complete when it has the following 
characteristics:

    - All of the replicas associated with the Deployment have been updated to 
      the latest version you’ve specified, meaning any updates you’ve 
      requested have been completed.
    - All of the replicas associated with the Deployment are available.
    - No old replicas for the Deployment are running.

You can check if a Deployment has completed by using kubectl rollout status. 
If the rollout completed successfully, kubectl rollout status returns a zero 
exit code.

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
$ echo $?
0

c) Failed Deployment
====================

Your Deployment may get stuck trying to deploy its newest ReplicaSet without 
ever completing. This can occur due to some of the following factors:

    - Insufficient quota
    - Readiness probe failures
    - Image pull errors
    - Insufficient permissions
    - Limit ranges
    - Application runtime misconfiguration

One way you can detect this condition is to specify a deadline parameter in 
your Deployment spec: (.spec.progressDeadlineSeconds).
.spec.progressDeadlineSeconds denotes the number of seconds the Deployment 
controller waits before indicating (in the Deployment status) that the 
Deployment progress has stalled.

The following kubectl command sets the spec with progressDeadlineSeconds to 
make the controller report lack of progress for a Deployment after 10 minutes:

$ kubectl patch deployment.v1.apps/nginx-deployment -p \
    '{"spec":{"progressDeadlineSeconds":600}}'
deployment.apps/nginx-deployment patched

Once the deadline has been exceeded, the Deployment controller adds a 
DeploymentCondition with the following attributes to the Deployment’s 
.status.conditions:

    Type=Progressing
    Status=False
    Reason=ProgressDeadlineExceeded

See the Kubernetes API conventions for more information on status conditions.

    Note: Kubernetes takes no action on a stalled Deployment other than to 
      report a status condition with Reason=ProgressDeadlineExceeded. Higher 
      level orchestrators can take advantage of it and act accordingly, for 
      example, rollback the Deployment to its previous version.

    Note: If you pause a Deployment, Kubernetes does not check progress 
      against your specified deadline. You can safely pause a Deployment in 
      the middle of a rollout and resume without triggering the condition for 
      exceeding the deadline.

You may experience transient errors with your Deployments, either due to a 
low timeout that you have set or due to any other kind of error that can be 
treated as transient. For example, let’s suppose you have insufficient quota. 
If you describe the Deployment you will notice the following section:

$ kubectl describe deployment nginx-deployment
<...>
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
<...>

Run the following command:

$ kubectl get deployment nginx-deployment -o yaml
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set "nginx-deployment-4262182780" is progressing.
    reason: ReplicaSetUpdated
    status: "True"
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2

Eventually, once the Deployment progress deadline is exceeded, Kubernetes 
updates the status and the reason for the Progressing condition:

Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate

You can address an issue of insufficient quota by scaling down your Deployment, 
by scaling down other controllers you may be running, or by increasing quota 
in your namespace. If you satisfy the quota conditions and the Deployment 
controller then completes the Deployment rollout, you’ll see the Deployment’s 
status update with a successful condition 
(Status=True and Reason=NewReplicaSetAvailable).

Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable

Type=Available with Status=True means that your Deployment has minimum 
availability. Minimum availability is dictated by the parameters specified in 
the deployment strategy. Type=Progressing with Status=True means that your 
Deployment is either in the middle of a rollout and it is progressing or that 
it has successfully completed its progress and the minimum required new 
replicas are available (see the Reason of the condition for the particulars - 
in our case Reason=NewReplicaSetAvailable means that the Deployment is 
complete).

You can check if a Deployment has failed to progress by using the command 
'kubectl rollout status': it returns a non-zero exit code if the Deployment 
has exceeded the progression deadline.

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment "nginx" exceeded its progress deadline
$ echo $?
1

d) Operating on a failed deployment
===================================

All actions that apply to a complete Deployment also apply to a failed 
Deployment. You can scale it up/down, roll back to a previous revision, or 
even pause it if you need to apply multiple tweaks in the Deployment Pod 
template.


2.8 - Clean up Policy
=====================

You can set .spec.revisionHistoryLimit field in a Deployment to specify how 
many old ReplicaSets for this Deployment you want to retain. The rest will be 
garbage-collected in the background. By default, it is 10.

    Note: Explicitly setting this field to 0, will result in cleaning up all 
      the history of your Deployment thus that Deployment will not be able to 
      roll back.


2.9 - Canary Deployment
=======================

If you want to roll out releases to a subset of users or servers using the 
Deployment, you can create multiple Deployments, one for each release, 
following the canary pattern described in managing resources.



================
3 - StatefulSets
================


StatefulSet is the workload API object used to manage stateful applications.

Manages the deployment and scaling of a set of Pods , and provides guarantees 
about the ordering and uniqueness of these Pods.

Like a Deployment, a StatefulSet manages Pods that are based on an identical 
container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity 
for each of their Pods. These pods are created from the same spec, but are not 
interchangeable: each has a persistent identifier that it maintains across any 
rescheduling.


3.1 - Using StatefulSets
========================

StatefulSets are valuable for applications that require one or more of the 
following.

    - Stable, unique network identifiers.
    - Stable, persistent storage.
    - Ordered, graceful deployment and scaling.
    - Ordered, automated rolling updates.

In the above, stable is synonymous with persistence across Pod (re)scheduling. 
If an application doesn’t require any stable identifiers or ordered deployment, 
deletion, or scaling, you should deploy your application using a workload 
object that provides a set of stateless replicas. Deployment or ReplicaSet may 
be better suited to your stateless needs.


3.2 - Limitations
=================

    The storage for a given Pod must either be provisioned by a PersistentVolume Provisioner based on the requested storage class, or pre-provisioned by an admin.
    Deleting and/or scaling a StatefulSet down will not delete the volumes associated with the StatefulSet. This is done to ensure data safety, which is generally more valuable than an automatic purge of all related StatefulSet resources.
    StatefulSets currently require a Headless Service to be responsible for the network identity of the Pods. You are responsible for creating this Service.
    StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is possible to scale the StatefulSet down to 0 prior to deletion.
    When using Rolling Updates with the default Pod Management Policy (OrderedReady), it’s possible to get into a broken state that requires manual intervention to repair.

3.3 - Components
================

The example below demonstrates the components of a StatefulSet.

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi

In the above example:

    A Headless Service, named nginx, is used to control the network domain.
    The StatefulSet, named web, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.
    The volumeClaimTemplates will provide stable storage using PersistentVolumes provisioned by a PersistentVolume Provisioner.

3.4 - Pod Selector
==================

You must set the .spec.selector field of a StatefulSet to match the labels of its .spec.template.metadata.labels. Prior to Kubernetes 1.8, the .spec.selector field was defaulted when omitted. In 1.8 and later versions, failing to specify a matching Pod Selector will result in a validation error during StatefulSet creation.

3.5 - Pod Identity
==================

StatefulSet Pods have a unique identity that is comprised of an ordinal, a stable network identity, and stable storage. The identity sticks to the Pod, regardless of which node it’s (re)scheduled on.

a) Ordinal Index
================

For a StatefulSet with N replicas, each Pod in the StatefulSet will be assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.

b) Stable Network ID
====================

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet and the ordinal of the Pod. The pattern for the constructed hostname is $(statefulset name)-$(ordinal). The example above will create three Pods named web-0,web-1,web-2. A StatefulSet can use a Headless Service to control the domain of its Pods. The domain managed by this Service takes the form: $(service name).$(namespace).svc.cluster.local, where “cluster.local” is the cluster domain. As each Pod is created, it gets a matching DNS subdomain, taking the form: $(podname).$(governing service domain), where the governing service is defined by the serviceName field on the StatefulSet.

As mentioned in the limitations section, you are responsible for creating the Headless Service responsible for the network identity of the pods.

Here are some examples of choices for Cluster Domain, Service name, StatefulSet name, and how that affects the DNS names for the StatefulSet’s Pods.
Cluster Domain	Service (ns/name)	StatefulSet (ns/name)	StatefulSet Domain	Pod DNS	Pod Hostname
cluster.local	default/nginx	default/web	nginx.default.svc.cluster.local	web-{0..N-1}.nginx.default.svc.cluster.local	web-{0..N-1}
cluster.local	foo/nginx	foo/web	nginx.foo.svc.cluster.local	web-{0..N-1}.nginx.foo.svc.cluster.local	web-{0..N-1}
kube.local	foo/nginx	foo/web	nginx.foo.svc.kube.local	web-{0..N-1}.nginx.foo.svc.kube.local	web-{0..N-1}

    Note: Cluster Domain will be set to cluster.local unless otherwise configured.

c) Stable Storage
=================

Kubernetes creates one PersistentVolume for each VolumeClaimTemplate. In the nginx example above, each Pod will receive a single PersistentVolume with a StorageClass of my-storage-class and 1 Gib of provisioned storage. If no StorageClass is specified, then the default StorageClass will be used. When a Pod is (re)scheduled onto a node, its volumeMounts mount the PersistentVolumes associated with its PersistentVolume Claims. Note that, the PersistentVolumes associated with the Pods’ PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted. This must be done manually.

d) Pod Name Label
=================

When the StatefulSet Controller creates a Pod, it adds a label, statefulset.kubernetes.io/pod-name, that is set to the name of the Pod. This label allows you to attach a Service to a specific Pod in the StatefulSet.

3.6 - Deployment and Scaling Guarantees
=======================================

    - For a StatefulSet with N replicas, when Pods are being deployed, they 
      are created sequentially, in order from {0..N-1}.
    - When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.
    - Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.
    - Before a Pod is terminated, all of its successors must be completely shutdown.

The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 0. This practice is unsafe and strongly discouraged. For further explanation, please refer to force deleting StatefulSet Pods.

When the nginx example above is created, three Pods will be deployed in the order web-0, web-1, web-2. web-1 will not be deployed before web-0 is Running and Ready, and web-2 will not be deployed until web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such that replicas=1, web-2 would be terminated first. web-1 would not be terminated until web-2 is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and is completely shutdown, but prior to web-1’s termination, web-1 would not be terminated until web-0 is Running and Ready.


3.7 - Update Strategies
=======================

In Kubernetes 1.7 and later, StatefulSet’s .spec.updateStrategy field allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.

On Delete

The OnDelete update strategy implements the legacy (1.6 and prior) behavior. When a StatefulSet’s .spec.updateStrategy.type is set to OnDelete, the StatefulSet controller will not automatically update the Pods in a StatefulSet. Users must manually delete Pods to cause the controller to create new Pods that reflect modifications made to a StatefulSet’s .spec.template.

Rolling Updates

The RollingUpdate update strategy implements automated, rolling update for the Pods in a StatefulSet. It is the default strategy when .spec.updateStrategy is left unspecified. When a StatefulSet’s .spec.updateStrategy.type is set to RollingUpdate, the StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed in the same order as Pod termination (from the largest ordinal to the smallest), updating each Pod one at a time. It will wait until an updated Pod is Running and Ready prior to updating its predecessor.

Partitions

The RollingUpdate update strategy can be partitioned, by specifying a .spec.updateStrategy.rollingUpdate.partition. If a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet’s .spec.template is updated. All Pods with an ordinal that is less than the partition will not be updated, and, even if they are deleted, they will be recreated at the previous version. If a StatefulSet’s .spec.updateStrategy.rollingUpdate.partition is greater than its .spec.replicas, updates to its .spec.template will not be propagated to its Pods. In most cases you will not need to use a partition, but they are useful if you want to stage an update, roll out a canary, or perform a phased roll out.

Forced Rollback

When using Rolling Updates with the default Pod Management Policy (OrderedReady), it’s possible to get into a broken state that requires manual intervention to repair.

If you update the Pod template to a configuration that never becomes Running and Ready (for example, due to a bad binary or application-level configuration error), StatefulSet will stop the rollout and wait.

In this state, it’s not enough to revert the Pod template to a good configuration. Due to a known issue, StatefulSet will continue to wait for the broken Pod to become Ready (which never happens) before it will attempt to revert it back to the working configuration.

After reverting the template, you must also delete any Pods that StatefulSet had already attempted to run with the bad configuration. StatefulSet will then begin to recreate the Pods using the reverted template.


============
4- DaemonSet
============


A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

    running a cluster storage daemon, such as glusterd, ceph, on each node.
    running a logs collection daemon on every node, such as fluentd or logstash.
    running a node monitoring daemon on every node, such as Prometheus Node Exporter, Flowmill, Sysdig Agent, collectd, Dynatrace OneAgent, AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or Instana Agent.

In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.

    Writing a DaemonSet Spec
    How Daemon Pods are Scheduled
    Communicating with Daemon Pods
    Updating a DaemonSet
    Alternatives to DaemonSet

Writing a DaemonSet Spec
Create a DaemonSet

You can describe a DaemonSet in a YAML file. For example, the daemonset.yaml file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:
controllers/daemonset.yaml [Copy controllers/daemonset.yaml to clipboard]

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

    Create a DaemonSet based on the YAML file:

    kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml

Required Fields

As with all other Kubernetes config, a DaemonSet needs apiVersion, kind, and metadata fields. For general information about working with config files, see deploying applications, configuring containers, and object management using kubectl documents.

A DaemonSet also needs a .spec section.
Pod Template

The .spec.template is one of the required fields in .spec.

The .spec.template is a pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate labels (see pod selector).

A Pod Template in a DaemonSet must have a RestartPolicy equal to Always, or be unspecified, which defaults to Always.
Pod Selector

The .spec.selector field is a pod selector. It works the same as the .spec.selector of a Job.

As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template. The pod selector will no longer be defaulted when left empty. Selector defaulting was not compatible with kubectl apply. Also, once a DaemonSet is created, its .spec.selector can not be mutated. Mutating the pod selector can lead to the unintentional orphaning of Pods, and it was found to be confusing to users.

The .spec.selector is an object consisting of two fields:

    matchLabels - works the same as the .spec.selector of a ReplicationController.
    matchExpressions - allows to build more sophisticated selectors by specifying key, list of values and an operator that relates the key and values.

When the two are specified the result is ANDed.

If the .spec.selector is specified, it must match the .spec.template.metadata.labels. Config with these not matching will be rejected by the API.

Also you should not normally create any Pods whose labels match this selector, either directly, via another DaemonSet, or via another workload resource such as ReplicaSet. Otherwise, the DaemonSet Controller will think that those Pods were created by it. Kubernetes will not stop you from doing this. One case where you might want to do this is manually create a Pod with a different value on a node for testing.
Running Pods on Only Some Nodes

If you specify a .spec.template.spec.nodeSelector, then the DaemonSet controller will create Pods on nodes which match that node selector. Likewise if you specify a .spec.template.spec.affinity, then DaemonSet controller will create Pods on nodes which match that node affinity. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
How Daemon Pods are Scheduled
Scheduled by default scheduler
FEATURE STATE: Kubernetes v1.17 stable

A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the node that a Pod runs on is selected by the Kubernetes scheduler. However, DaemonSet pods are created and scheduled by the DaemonSet controller instead. That introduces the following issues:

    Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created and in Pending state, but DaemonSet pods are not created in Pending state. This is confusing to the user.
    Pod preemption is handled by default scheduler. When preemption is enabled, the DaemonSet controller will make scheduling decisions without considering pod priority and preemption.

ScheduleDaemonSetPods allows you to schedule DaemonSets using the default scheduler instead of the DaemonSet controller, by adding the NodeAffinity term to the DaemonSet pods, instead of the .spec.nodeName term. The default scheduler is then used to bind the pod to the target host. If node affinity of the DaemonSet pod already exists, it is replaced. The DaemonSet controller only performs these operations when creating or modifying DaemonSet pods, and no changes are made to the spec.template of the DaemonSet.

nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchFields:
      - key: metadata.name
        operator: In
        values:
        - target-host-name

In addition, node.kubernetes.io/unschedulable:NoSchedule toleration is added automatically to DaemonSet Pods. The default scheduler ignores unschedulable Nodes when scheduling DaemonSet Pods.
Taints and Tolerations

Although Daemon Pods respect taints and tolerations, the following tolerations are added to DaemonSet Pods automatically according to the related features.
Toleration Key	Effect	Version	Description
node.kubernetes.io/not-ready	NoExecute	1.13+	DaemonSet pods will not be evicted when there are node problems such as a network partition.
node.kubernetes.io/unreachable	NoExecute	1.13+	DaemonSet pods will not be evicted when there are node problems such as a network partition.
node.kubernetes.io/disk-pressure	NoSchedule	1.8+	
node.kubernetes.io/memory-pressure	NoSchedule	1.8+	
node.kubernetes.io/unschedulable	NoSchedule	1.12+	DaemonSet pods tolerate unschedulable attributes by default scheduler.
node.kubernetes.io/network-unavailable	NoSchedule	1.12+	DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.
Communicating with Daemon Pods

Some possible patterns for communicating with Pods in a DaemonSet are:

    Push: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have clients.
    NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that the pods are reachable via the node IPs. Clients know the list of node IPs somehow, and know the port by convention.
    DNS: Create a headless service with the same pod selector, and then discover DaemonSets using the endpoints resource or retrieve multiple A records from DNS.
    Service: Create a service with the same Pod selector, and use the service to reach a daemon on a random node. (No way to reach specific node.)

Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be updated. Also, the DaemonSet controller will use the original template the next time a node (even with the same name) is created.

You can delete a DaemonSet. If you specify --cascade=false with kubectl, then the Pods will be left on the nodes. If you subsequently create a new DaemonSet with the same selector, the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces them according to its updateStrategy.

You can perform a rolling update on a DaemonSet.
Alternatives to DaemonSet
Init Scripts

It is certainly possible to run daemon processes by directly starting them on a node (e.g. using init, upstartd, or systemd). This is perfectly fine. However, there are several advantages to running such processes via a DaemonSet:

    Ability to monitor and manage logs for daemons in the same way as applications.
    Same config language and tools (e.g. Pod templates, kubectl) for daemons and applications.
    Running daemons in containers with resource limits increases isolation between daemons from app containers. However, this can also be accomplished by running the daemons in a container but not in a Pod (e.g. start directly via Docker).

Bare Pods

It is possible to create Pods directly which specify a particular node to run on. However, a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should use a DaemonSet rather than creating individual Pods.
Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.
Deployments

DaemonSets are similar to Deployments in that they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers, storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates are more important than controlling exactly which host the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or certain hosts, and when it needs to start before other Pods.
