#!/usr/bin/env bash


###############################################################################
#
# Learn Kubernetes Basics - Part 0 - pre-requisites
#
###############################################################################


This tutorial mixes elements from the official Kubernetes site 'get started' 
sections (many of these), and also from Dada's blog (specifically for setting 
up the Kubernetes clusters on VMs running on a laptop):
    https://kubernetes.io/docs/tutorials/kubernetes-basics/
    https://www.dadall.info/article658/preparer-virtualbox-pour-kubernetes

Here are the identified pre-requisites to run this tutorial and actually
learn something from this experience:

    - have a linux laptop, with a 'admin' account (i.e. need to have
      the sudo privilege). Ubuntu will be perfect for beginners.
    - have curl, git and virtualbox installed

Also, you will find several resources in the directory:

    - flannel.yml - a kubernetes network model
    - server.js   - a node.js example application used as of part 3
    - Dockerfile  - to build the container for the application
    - several VirtualBox VM images in order to build the Kubernetes cluster,
      located in the "VM_images" sub-directory


This is it. Nothing else is needed... except the desire to learn :-)



###############################################################################
#
# Learn Kubernetes Basics - Part 1 - Kubernetes Basics
#
###############################################################################


=====================
1.1 Kubernetes Basics
=====================

This tutorial provides a walkthrough of the basics of the Kubernetes cluster
orchestration system. Each module contains some background information on
major Kubernetes features and concepts. You will actually deploy and manage 
a simple cluster and its containerized applications by yourself.

Following the tutorial steps, you can learn to:

  - Deploy a Kubernetes cluster
  - Deploy a containerized application on a cluster.
  - Scale the deployment.
  - Update the containerized application with a new software version.
  - Debug the containerized application.


===================================
1.2 What can Kubernetes do for you?
===================================

With modern web services, users expect applications to be available 24/7, and
developers expect to deploy new versions of those applications several times
a day. Containerization helps package software to serve these goals, enabling
applications to be released and updated in an easy and fast way without
downtime. Kubernetes helps you make sure those containerized applications run
where and when you want, and helps them find the resources and tools they
need to work. Kubernetes is a production-ready, open source platform designed
with Google's accumulated experience in container orchestration, combined 
with best-of-breed ideas from the community.


======================================
1.3 Kubernetes Design and Architecture
======================================


1.3.1 Scope
===========

Kubernetes is a platform for deploying and managing containers. Kubernetes 
provides a container runtime, container orchestration, container-centric 
infrastructure orchestration, self-healing mechanisms such as health checking 
and re-scheduling, and service discovery and load balancing.

Kubernetes aspires to be an extensible, pluggable, building-block OSS 
platform and toolkit. Therefore, architecturally, we want Kubernetes to be 
built as a collection of pluggable components and layers, with the ability to 
use alternative schedulers, controllers, storage systems, and distribution 
mechanisms, and we're evolving its current code in that direction. 
Furthermore, we want others to be able to extend Kubernetes functionality, 
such as with higher-level PaaS functionality or multi-cluster layers, without 
modification of core Kubernetes source. Therefore, its API isn't just (or 
even necessarily mainly) targeted at end users, but at tool and extension 
developers. Its APIs are intended to serve as the foundation for an open 
ecosystem of tools, automation systems, and higher-level API layers. 
Consequently, there are no "internal" inter-component APIs. All APIs are 
visible and available, including the APIs used by the scheduler, the node 
controller, the replication-controller manager, Kubelet's API, etc. There's 
no glass to break -- in order to handle more complex use cases, one can just 
access the lower-level APIs in a fully transparent, composable manner.


1.3.2 Goals
===========

The project is committed to the following (aspirational) design ideals:

    - Portable. Kubernetes runs everywhere -- public cloud, private cloud, 
      bare metal, laptop -- with consistent behavior so that applications and 
      tools are portable throughout the ecosystem as well as between 
      development and production environments.
    - General-purpose. Kubernetes should run all major categories of 
      workloads to enable you to run all of your workloads on a single 
      infrastructure, stateless and stateful, microservices and monoliths, 
      services and batch, greenfield and legacy.
    - Meet users partway. Kubernetes doesn’t just cater to purely greenfield 
      cloud-native applications, nor does it meet all users where they are. 
      It focuses on deployment and management of microservices and 
      cloud-native applications, but provides some mechanisms to facilitate 
      migration of monolithic and legacy applications.
    - Flexible. Kubernetes functionality can be consumed a la carte and (in 
      most cases) Kubernetes does not prevent you from using your own 
      solutions in lieu of built-in functionality.
    - Extensible. Kubernetes enables you to integrate it into your 
      environment and to add the additional capabilities you need, by 
      exposing the same interfaces used by built-in functionality.
    - Automatable. Kubernetes aims to dramatically reduce the burden of 
      manual operations. It supports both declarative control by specifying 
      users’ desired intent via its API, as well as imperative control to 
      support higher-level orchestration and automation. The declarative 
      approach is key to the system’s self-healing and autonomic capabilities.
    - Advance the state of the art. While Kubernetes intends to support 
      non-cloud-native applications, it also aspires to advance the 
      cloud-native and DevOps state of the art, such as in the participation 
      of applications in their own management. However, in doing so, we 
      strive not to force applications to lock themselves into Kubernetes 
      APIs, which is, for example, why we prefer configuration over 
      convention in the downward API. Additionally, Kubernetes is not bound 
      by the lowest common denominator of systems upon which it depends, such 
      as container runtimes and cloud providers. An example where we pushed 
      the envelope of what was achievable was in its IP per Pod networking 
      model.


1.3.3 Architecture
==================

A running Kubernetes cluster contains node agents (kubelet) and a cluster 
control plane (AKA master), with cluster state backed by a distributed storage 
system (etcd).


a) Cluster control plane (AKA master)
=====================================

The Kubernetes control plane is split into a set of components, which can all 
run on a single master node, or can be replicated in order to support 
high-availability clusters, or can even be run on Kubernetes itself 
(AKA self-hosted).

Kubernetes provides a REST API supporting primarily CRUD operations on 
(mostly) persistent resources, which serve as the hub of its control plane. 
Kubernetes’s API provides IaaS-like container-centric primitives such as Pods, 
Services, and Ingress, and also lifecycle APIs to support orchestration 
(self-healing, scaling, updates, termination) of common types of workloads, 
such as ReplicaSet (simple fungible/stateless app manager), Deployment 
(orchestrates updates of stateless apps), Job (batch), CronJob (cron), 
DaemonSet (cluster services), and StatefulSet (stateful apps). We 
deliberately decoupled service naming/discovery and load balancing from 
application implementation, since the latter is diverse and open-ended.

Both user clients and components containing asynchronous controllers interact 
with the same API resources, which serve as coordination points, common 
intermediate representation, and shared state. Most resources contain 
metadata, including labels and annotations, fully elaborated desired state 
(spec), including default values, and observed state (status).

Controllers work continuously to drive the actual state towards the desired 
state, while reporting back the currently observed state for users and for 
other controllers.

While the controllers are level-based (as described here and here) to 
maximize fault tolerance, they typically watch for changes to relevant 
resources in order to minimize reaction latency and redundant work. This 
enables decentralized and decoupled choreography-like coordination without a 
message bus.

API Server
==========

The API server serves up the Kubernetes API. It is intended to be a relatively 
simple server, with most/all business logic implemented in separate components 
or in plug-ins. It mainly processes REST operations, validates them, and 
updates the corresponding objects in etcd (and perhaps eventually other 
stores). Note that, for a number of reasons, Kubernetes deliberately does not 
support atomic transactions across multiple resources.

Kubernetes cannot function without this basic API machinery, which includes:

    - REST semantics, watch, durability and consistency guarantees, API 
      versioning, defaulting, and validation
    - Built-in admission-control semantics, synchronous admission-control 
      hooks, and asynchronous resource initialization
    - API registration and discovery

Additionally, the API server acts as the gateway to the cluster. By 
definition, the API server must be accessible by clients from outside the 
cluster, whereas the nodes, and certainly containers, may not be. Clients 
authenticate the API server and also use it as a bastion and proxy/tunnel to 
nodes and pods (and services).

Cluster state store
===================

All persistent cluster state is stored in an instance of etcd. This provides 
a way to store configuration data reliably. With watch support, coordinating 
components can be notified very quickly of changes.

Controller-Manager Server
=========================

Most other cluster-level functions are currently performed by a separate 
process, called the Controller Manager. It performs both lifecycle functions 
(e.g., namespace creation and lifecycle, event garbage collection, 
terminated-pod garbage collection, cascading-deletion garbage collection, 
node garbage collection) and API business logic (e.g., scaling of pods 
controlled by a ReplicaSet).

The application management and composition layer, providing self-healing, 
scaling, application lifecycle management, service discovery, routing, and 
service binding and provisioning.

These functions may eventually be split into separate components to make them 
more easily extended or replaced.

Scheduler
=========

Kubernetes enables users to ask a cluster to run a set of containers. The 
scheduler component automatically chooses hosts to run those containers on.

The scheduler watches for unscheduled pods and binds them to nodes via the 
/binding pod subresource API, according to the availability of the requested 
resources, quality of service requirements, affinity and anti-affinity 
specifications, and other constraints.

Kubernetes supports user-provided schedulers and multiple concurrent cluster 
schedulers, using the shared-state approach pioneered by Omega. In addition to 
the disadvantages of pessimistic concurrency described by the Omega paper, 
two-level scheduling models that hide information from the upper-level 
schedulers need to implement all of the same features in the lower-level 
scheduler as required by all upper-layer schedulers in order to ensure that 
their scheduling requests can be satisfied by available desired resources.

b) The Kubernetes Node
======================

The Kubernetes node has the services necessary to run application containers 
and be managed from the master systems.

Kubelet
=======

The most important and most prominent controller in Kubernetes is the Kubelet,
which is the primary implementer of the Pod and Node APIs that drive the 
container execution layer. Without these APIs, Kubernetes would just be a 
CRUD-oriented REST application framework backed by a key-value store (and 
perhaps the API machinery will eventually be spun out as an independent 
project).

Kubernetes executes isolated application containers as its default, native 
mode of execution, as opposed to processes and traditional operating-system 
packages. Not only are application containers isolated from each other, but 
they are also isolated from the hosts on which they execute, which is critical 
to decoupling management of individual applications from each other and from 
management of the underlying cluster physical/virtual infrastructure.

Kubernetes provides Pods that can host multiple containers and storage volumes 
as its fundamental execution primitive in order to facilitate packaging a 
single application per container, decoupling deployment-time concerns from 
build-time concerns, and migration from physical/virtual machines. The Pod 
primitive is key to glean the primary benefits of deployment on modern cloud 
platforms, such as Kubernetes.

API admission control may reject pods or add additional scheduling constraints 
to them, but Kubelet is the final arbiter of what pods can and cannot run on 
a given node, not the schedulers or DaemonSets.

Kubelet also currently links in the cAdvisor resource monitoring agent.

Container runtime
=================

Each node runs a container runtime, which is responsible for downloading 
images and running containers.

Kubelet does not link in the base container runtime. Instead, we're defining 
a Container Runtime Interface to control the underlying runtime and facilitate 
pluggability of that layer. This decoupling is needed in order to maintain 
clear component boundaries, facilitate testing, and facilitate pluggability. 
Runtimes supported today, either upstream or by forks, include at least docker 
(for Linux and Windows), rkt, cri-o, and frakti.

Kube Proxy
==========

The service abstraction provides a way to group pods under a common access 
policy (e.g., load-balanced). The implementation of this creates a virtual 
IP which clients can access and which is transparently proxied to the pods in 
a Service. Each node runs a kube-proxy process which programs iptables rules 
to trap access to service IPs and redirect them to the correct backends. This 
provides a highly-available load-balancing solution with low performance 
overhead by balancing client traffic from a node on that same node.

Service endpoints are found primarily via DNS.

Add-ons and other dependencies
==============================

A number of components, called add-ons typically run on Kubernetes itself:
    - DNS
    - Ingress controller
    - Heapster (resource monitoring)
    - Dashboard (GUI)




###############################################################################
#
# Learn Kubernetes Basics - Part 2 - Create a cluster
#
###############################################################################


Let's now refine a bit few concepts on the node, and get these concepts in 
action in setting up and deploying a cluster!

======================================
2.1 - Naming conventions & unique data
======================================


In this section, we will build step by step the infrastructure on which we 
will deploy a Kubernetes cluster. We intentionaly make every step manual, so 
that you can appreciate the full process at leat once in your lifetime. In 
real production world, most of these steps are automated and you would simply 
"push a button".

The steps are:
    - building an initial  VM image ("K8s BARE") with all the prerequisites 
      installed (Ubuntu OS, docker, kubernetes)
    - renaming a VM to be the master node ("K8s master - cluster not deployed"), 
      derived from "K8S BARE"
    - renaming two 'slave' VMs ("K8s slave - cluster not deployed") derived from 
      "K8s BARE"
    - initialize and deploy a Kubernetes on the master, and join the two 
      slaves into the cluster (all VMs are then called "cluster deployed")
    - setup a dashboard and access it

You may skip this step and use directly the "cluster deployed" images in order to skip this step and go directly to part 3.


#
#      WARNING
#
# Few informations will be unique to each deployement and you need to 
# carefully copy and save these informations in order to adapt the commands 
# as shown in the tutorial (i.e. replace the tutorial's info with yours) and 
# run your own version of the commands:
#
# 1) the ip@ of the VMs
#
# In the tutorial, I used (copied from a real execution):
#    master:  192.168.0.108
#    slave 1: 192.168.0.109
#    slave 2: 192.168.0.110
#
# 2) the token required for a node to join into the cluster
#
# In the following examples, I used:
#
# kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash \
    sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729
# 
# 


===========================
2.3 - Building the first VM
===========================

(you can skip this phase and use directly the "K8s BARE" image)

Nothing fancy here: we've built from the Ubuntu Server 18.04 LTS, with:
  - two CPUs (it is mandatory to run a Kubernetes master)
  - 2048 MB memory
  - two network interfaces:
       * one NAT (which is default setting) for internet connectivity
       * one configured in a "Bridge Adapter" mode, pointing on the most 
         relevant network adapter (in my case: the Ethernet card on the 
         desktop, or the wifi card on the laptop)

(image "K8s BARE - 0 - Description")
(image "K8s BARE - 1 - Memory")
(image "K8s BARE - 2 - CPU")
(image "K8s BARE - 3 - 1st Network interface")
(image "K8s BARE - 4 - 2nd Network interface")

In order to make this VM a generic basis on which we can run any type of k8s
nodes, we then need to log as root and (in this order):
  - to install docker
  - to disable the swap
  - to install kubernetes

For convenience, in this tutorial, we will configure the VMs by connecting 
into them via ssh from terminals running on the host machine, using the 
default admin profile (login=thierry, password=thierry) (Secure, huh!!!). 
There are few advantages doing so, namely the fact that the prompt then tells 
on which machine you are actually logged, which is very convenient when you 
will have 3 VMs running at the same time.

To do so, you need to identify the ip@ of the VM: it will be displayed on 
your first loging into the VM from the VM window:

(image "K8s BARE - 5 - Login from the VM")

Open a terminal and - as a regular user on hte host machine, ssh into the VM:
(in my case, the hostname of my laptop is 'laptop' and my login is 'tso')

tso@laptop:~$ ssh thierry@192.168.0.111

The first time you connect on a remote machine, you're asked to confirm: 
confirm by typing 'yes' and then enter the password:

The authenticity of host '192.168.0.111 (192.168.0.111)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.111' (ECDSA) to the list of known hosts.
thierry@192.168.0.111's password: 

You are now connected on the VM and your prompt will show it:

(K8s BARE - 6 - Login from a terminal)

thierry@k8s_node:~$

You are logged as 'thierry' and not anymore 'tso' and you are on 'k8s_node'*
and not on 'laptop' anymore.

*: the initial name of the machine is whatever you have specified when 
building the fist image from the Ubuntu OS in VirtualBox. I used the generic 
name 'k8s_node' because the same VM will later be used to configure both 
master and slave nodes.


We must do the following steps as 'root':

$ sudo su

The prompt becomes # and we can continue the next steps.

We will install docker: collect the certificates, add the signing key to the 
local apt repository, add the docker repository... and finally install docker:

# apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common
# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg | apt-key add -
# add-apt-repository "deb https://download.docker.com/linux/debian stretch stable"
# apt-get update
# apt-get install -y docker-ce

Then we must disable the swap: we do so immediately with the "swappoff" 
command:

# swapoff -a

and we also prevent the swap to be re-activated at next boot, by editing the 
fstab file (/etc/fstab) and disable the line indicating the swap partition 
by commenting it (i.e. add a '#' at the beginning of the line). Thus, the 
original file changes from:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
/swap.img	none	swap	sw	0	0

to:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
#/swap.img	none	swap	sw	0	0


We will then install Kubernetes: collect the certificates, add the signing 
key to the local apt repository, add the google kubernetes repository... and 
finally install docker:

# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
# add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
# apt-get update
# apt-get install -y kubelet kubeadm kubectl


Here we are !!!
The generic VM is setup, and we can use this basis for crafting teh master 
and slave nodes. As a matter of precaution, you should log out, go in 
VirtualBox and shutdown the VM (softly), take a snapshot of the VM and export 
it as and appliance under the name of  "K8s BARE".


=========================================
2.4 - Building the master and slave nodes
=========================================

In this section, we will configure one master node and two slave nodes 
from "K8s BARE". To do so, we will clone K8s_BARE (which VirtualBox does very
easily).

In order to reduce the memory footprint of the tutorial, choose 'linked clone'
rather than 'full clone', since each VM virtual disk is more than 3.5GB (and
the laptop does not have infinite storage).

(K8s BARE - 7 - Cloning)

And you should get something looking like this:

(K8s BARE - 8 - Cloned machines)

Start the three VMs in VirtualBox, and as shown in the previous section, 
log in, identify the ip@, and connect on these VMs from terminals running on 
the host machine.

Since almost all prerequisites were already installed in the previous phase, 
the task here merely consists in renaming the machines so that a) you know on
which machine you are logged, and b) you prevent naming conflicts within the
kubernetes cluster later on (since every node must have a unique name in the 
cluster).

So we will edit two files: /etc/hosts and /etc/hostname

Log into the first VM (in our case 'ssh thierry@192.168.0.108) and edit the
file /etc/hostname: replace the previous host name (k8s-node) with the new
host name 'k8s-master'.

Then edit the file /etc/hosts and add the new name 'k8s-master after
'127.0.0.1   localhost'.

After edition, it should look like this:

thierry@k8s-master:~$ cat /etc/hostname 
k8s-master

thierry@k8s-master:~$ cat /etc/hosts
127.0.0.1 localhost k8s-master
127.0.1.1 k8s_node

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

Then do so on the two other machines (respectively at 192.168.0.109 and 
192.168.0.110) with the new host names 'k8s-slave1' and 'k8s-slave2'.

That's it! We now have 3 machines running on the laptop, with all the 
prerequisites installed and the right configuration to make them a master 
and two slaves.

We can test the connectivity between the machines on the laptop (ping from 
one machine to the other) to check that there is no problem, and then go to
the next section.


=====================================
2.5 - Configure and start the cluster
=====================================

To start with, we will initiate the cluster on the master node.

SSH onto the master node and log as root. To initiate the cluster, we will use 
'kubeadm' with the 'init' command. We specify two arguments:
    - specify the 'internal cluster network' which will enable the nodes to 
      communicate one with the other. This internal network uses the second 
      network interface declared in VirtualBox (the one in bridge mode, and
      use a dedicated internal IP range (10.244.0.0-255 in this example).
    - specifiy that the master node must advertise its own IP@ in order to 
      enable the slave nodes to first connect to the master node.

root@k8s-master:/# kubeadm init --pod-network-cidr=10.244.0.0/16 \
      --apiserver-advertise-address=192.168.0.108

The whole setup can take a bit of time, and it shoudl conclude with displaying 
the following message (truncated):

[init] Using Kubernetes version: v1.16.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[...]
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as 
root:

kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729


Here you are: your Kubernetes master has initialized successfully!
You can see that the message displayed during the initialisation process 
includes three very important directives:
    - you need to create a configuration file to be able to operate the 
      cluster as a regulaer user (i.e. not root)
    - you need to actually deploy the network which will connect all the pods 
      together, and you will have to choose one of the possible network 
      configurations.
    - you are given the information needed for a slave node to join the 
      cluster by giving the right token and the public key of the master.

So, let's follow these directives:

The next step is to configure the 'regular' user (i.e. not root) on the 
master node who will manage the cluster. First we logout as 'root' but we 
stay logged as 'thierry' on the master, and as a 'thierry' we enter the 
commands given in the initialization message:

$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Then we must deploy the network within the cluster. For the sake of 
simplicity, we will use the 'flannel' network configuration (not because it 
is better than others, but because I know it is simple to make it work).

thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

Once you have received this message, the master will need a bit of time to 
process the deployment as it will actually spawn noew pods in charge of 
operating the network. You can see the progress by asking several times to 
show the pods running on the master with the command:
$ kubectl get pods --all-namespaces -o wide

See the results at several seconds distance, and observe that the pods status
evolves as their get from 'Pending' to 'ContainerCreating' and finally to 
'Running'.

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             0/1     Pending   0          64s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     Pending   0          64s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          77s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          81s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          97s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          13s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          64s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          97s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE    IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             0/1     ContainerCreating   0          70s    <none>      k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     ContainerCreating   0          70s    <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          83s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          87s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             1          103s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running             0          19s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running             0          70s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             1          103s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   0          78s    10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             0/1     Running   0          78s    10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          91s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          95s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          111s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          27s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          78s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          111s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   0          83s    10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             1/1     Running   0          83s    10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          96s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          100s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   1          116s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   0          32s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   0          83s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   1          116s   10.0.2.15    k8s-master   <none>           <none>

This is actually interesting as it reveals one key nature of Kubernetes: it
will enable you to run containerized applications, but it relies itself on 
launching containers (i.e. containerized applications) to do its job. As your
can see above, it launches a DNS application wich will serve to connect pods
within the cluster: this DNS application is itself running in a container, 
managed as a pod.

Looking at the list of the pods, you see that Kubernetes launched several 
applications: an API server, a proxy, a scheduler, a datastore (ETCD), two
DNS... and a controller manager. We will see abit later all these functions.

Nevertheless, the master is still alone; you have not yet joined a slave into
the cluster. You can see all the nodes in the cluster with the command
'kubectl get nodes':

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   8m36s   v1.16.3


So it's time to join the two slaves into the cluster. To do so, we need to 
log into the slave nodes (with ssh), as 'root', and use the token and key 
which were displayed at the end of the cluster initialisation message:

root@k8s-slave1:/# kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729

et ensuite:

root@k8s-slave2:/# kubeadm join 192.168.0.108:6443 --token q3nz0g.n2l9ow787j7enj8j \
    --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729


You then come back to the master node and check the status of the cluster. 
Check the number of nodes:

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   5m53s   v1.16.3
k8s-slave1   Ready    <none>   21s     v1.16.3
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE    VERSION
k8s-master   Ready      master   10m    v1.16.3
k8s-slave1   Ready      <none>   5m1s   v1.16.3
k8s-slave2   NotReady   <none>   5s     v1.16.3
thierry@k8s-master:~$ kubectl get nodes

Now, let's look at the pods running on the cluster: you can see that some 
pods are now running on the slave nodes as well:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-nxhfs             1/1     Running   1          29h   10.244.0.4   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-t8zsz             1/1     Running   1          29h   10.244.0.5   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   3          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-mrm2b          1/1     Running   2          29h   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-nxx7k          1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-s5dnw          1/1     Running   1          29h   10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-6pwws                     1/1     Running   1          29h   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-rfpq5                     1/1     Running   1          29h   10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-znwt2                     1/1     Running   1          29h   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   3          29h   10.0.2.15    k8s-master   <none>           <none>


YES!!! The cluster is up and running and the master is actually scheduling 
pods on the slave nods. At this moment, the pods are only Kubernetes 
applications, but this is already showing the principles by which Kubernetes
will schedule you application on the pods.


=========================
2.6 - Setup the dashboard
=========================

The cluster is running but it is very difficult to see what is actually 
happening, so it is time to setup the dashboad which visualizes the status 
of the cluster.

Start with login into the master:




root@k8s-master:/# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/head.yaml
namespace/kubernetes-dashboard-head created
serviceaccount/kubernetes-dashboard-head created
service/kubernetes-dashboard-head created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard-head created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
deployment.apps/kubernetes-dashboard-head created
service/dashboard-metrics-scraper-head created
deployment.apps/dashboard-metrics-scraper-head created

As you can see, Kubernetes pulls the resources from GitHub and starts pods 
to run the dashboard application (which is a web server). Check where the 
pods are running:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide | grep dashb
kubernetes-dashboard-head   dashboard-metrics-scraper-head-7cc7d9bb4b-r9lr6   1/1     Running   0          99s   10.244.1.2   k8s-slave1   <none>           <none>
kubernetes-dashboard-head   kubernetes-dashboard-head-5c87564c95-gl8q6        1/1     Running   0          99s   10.244.2.2   k8s-slave2   <none>           <none>

The workload is distributed over the two slave nodes. In order to access this
dasboard, we now need to creae users with teh right roles and authorisations
(i.e. profiles with the rights in RBAC mode)

Créez un compte utilisateur
Créez un fichier admin-user.yaml avec ce qui suit dedans :

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Puis créez le rôle qui lui sera attaché : admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

Chargez ces configurations dans le cluster :

kubectl apply -f admin-user.yaml
kubectl apply -f admin-role.yaml




thierry@k8s-master:~$ touch admin-user.yaml
thierry@k8s-master:~$ vi admin-user.yaml 
thierry@k8s-master:~$ touch admin-role.yaml
thierry@k8s-master:~$ vi admin-role.yaml 
thierry@k8s-master:~$ kubectl apply -f admin-user.yaml
serviceaccount/admin-user created
thierry@k8s-master:~$ kubectl apply -f admin-role.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created


Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :

Name:         admin-user-token-g4h4m
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 2124843b-2815-47f5-a558-cb5956bd61d7

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkUyTVprdjJuMGJ1NkVyeFlweFVHd2RtWHR2ZHFDS1JFb21SMDNQNEY3UlUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWc0aDRtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMTI0ODQzYi0yODE1LTQ3ZjUtYTU1OC1jYjU5NTZiZDYxZDciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.JmnyHQCNRqT90GhMub82keXrhJGi5cOVrKdPoM1Psw6s7qDBRpOobthUVw1KJ5abnFgTnEhVmRW52HcF4dCg2aPcKa5PcFidXtXVdyFXHM2-VUVYDw-97yeVKm9degPES11rs5bcnGGX0Fp7TAN38IhwmDDVcmcR4S7EftnOk1_RkG77Fuauu4mDYDO6kzm9ySrdMP_eF2XsoZr_1JLGucGmMDKaAryRAHPTl11t-i45mE_6NtxGdYo1eukPqKpvKF7-fQz-0Bbg3Pzavoko-nrv1TPZECPAj_cXZk3wptoYzg-Ye_h_dua0ue6CUanE1SY60u0kDmPK8aXGJfZUaw


Copy the token (yeah, I know, its long...)


Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 thierry@192.168.0.108

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://192.168.0.108:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all 

=> ça ne marche pas: ce qui s'affiche est un record de failure.

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    

Kubernetes coordinates a highly available cluster of computers that are 
connected to work as a single unit. The abstractions in Kubernetes allow 
you to deploy containerized applications to a cluster without tying them 
specifically to individual machines. To make use of this new model of 
deployment, applications need to be packaged in a way that decouples them 
from individual hosts: they need to be containerized. Containerized 
applications are more flexible and available than in past deployment 
models, where applications were installed directly onto specific machines 
as packages deeply integrated into the host. Kubernetes automates the 
distribution and scheduling of application containers across a cluster in 
a more efficient way.
Kubernetes is an open-source platform and is production-ready.

A Kubernetes cluster consists of two types of resources:

    The Master coordinates the cluster
    Nodes are the workers that run applications


Cluster Diagram


The Master is responsible for managing the cluster. The master coordinates 
all activities in your cluster, such as scheduling applications, 
maintaining applications' desired state, scaling applications, and rolling 
out new updates.

A node is a VM or a physical computer that serves as a worker machine in 
a Kubernetes cluster. Each node has a Kubelet, which is an agent for 
managing the node and communicating with the Kubernetes master. The node 
should also have tools for handling container operations, such as Docker or 
rkt. A Kubernetes cluster that handles production traffic should have a 
minimum of three nodes.

Masters manage the cluster and the nodes are used to host the running 
applications.

When you deploy applications on Kubernetes, you tell the master to start 
the application containers. The master schedules the containers to run on 
the cluster's nodes. The nodes communicate with the master using the 
Kubernetes API, which the master exposes. End users can also use the 
Kubernetes API directly to interact with the cluster.

A Kubernetes cluster can be deployed on either physical or virtual 
machines. In our case, I explained in Part I how to deploy a Kubernetes 
cluster in such a way.

Various Kubernetes commands enable you to interact with the cluster:

To check that the cluster is alive, you can run the version command:

$ kubectl version
Client Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:23:11Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"16", GitVersion:"v1.16.3", GitCommit:"b3cbbae08ec52a7fc73d334838e18d17e8512749", GitTreeState:"clean", BuildDate:"2019-11-13T11:13:49Z", GoVersion:"go1.12.12", Compiler:"gc", Platform:"linux/amd64"}

And view the cluster details:

$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.108:6443
KubeDNS is running at https://192.168.0.108:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

We have a running master and a dashboard. The Kubernetes dashboard allows 
you to view your applications in a UI. During this tutorial, we’ll be 
focusing on the command line for deploying and exploring our application. 
To view the nodes in the cluster, run the kubectl get nodes command:

$ kubectl get nodes
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   64m   v1.16.3
k8s-slave1   Ready    <none>   58m   v1.16.3
k8s-slave2   Ready    <none>   53m   v1.16.3

This command shows all nodes that can be used to host our applications. 
Now we have three nodes, and we can see that their status is ready (it is 
ready to accept applications for deployment).



Installing the dashboard:


To install the Dashboard, run the following command from the master:

$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml

Kubernetes va aller chercher la configuration nécessaire la mise à en place du Dashboard directement depuis son dépôt Github et le faire apparaître dans la liste des pods de votre cluster.

$  kubectl get pods --all-namespaces -o wide | grep dashb
kube-system   kubernetes-dashboard-77fd78f978-f8p9l   1/1     Running   0          60s     10.244.1.230   k8snode1    <none>

Il est  "Running", ça veut dire qu'il est disponible, mais pas encore accessible.
Créez un compte utilisateur
Créez un fichier admin-user.yaml avec ce qui suit dedans :

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Puis créez le rôle qui lui sera attaché : admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

Chargez ces configurations dans le cluster :

kubectl apply -f admin-user.yaml
kubectl apply -f admin-role.yaml

Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :

dada@k8smaster:~$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-b8qmq
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: a8a600b1-e010-11e8-96ec-0800273c4560

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJl.........

J'ai tronqué l'affichage du token. Il est d'une longueur dingue. Copiez-le dans un coin maintenant que vous l'avez.
Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 dada@IP_DU_MASTER

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all

Et voir ceci : 


Vous voici avec une belle interface pour admirer le comportement de votre cluster k8s. Foncez cliquer absolument partout et chercher le pourquoi du comment de telles options à tel endroit !



Deploying your first app on Kubernetes


###############################################################################
#
# Learn Kubernetes Basics - Part 3 - Deploy an app
#
###############################################################################



Once you have a running Kubernetes cluster, you can deploy your containerized 
applications on top of it. To do so, you create a Kubernetes Deployment 
configuration. The Deployment instructs Kubernetes how to create and update 
instances of your application. Once you've created a Deployment, the 
Kubernetes master schedules mentioned application instances onto individual 
Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment Controller 
continuously monitors those instances. If the Node hosting an instance goes 
down or is deleted, the Deployment controller replaces the instance with an 
instance on another Node in the cluster. This provides a self-healing 
mechanism to address machine failure or maintenance.

In a pre-orchestration world, installation scripts would often be used to 
start applications, but they did not allow recovery from machine failure. By 
both creating your application instances and keeping them running across 
Nodes, Kubernetes Deployments provide a fundamentally different approach to 
application management.


====================================
3.1 - Basic components of Kubernetes
====================================



    Pods
    Cluster DNS
    Headless Services
    PersistentVolumes
    PersistentVolume Provisioning
    StatefulSets
    kubectl CLI


============================================
3.2 - Deploying your first app on Kubernetes
============================================

You can create and manage a Deployment by using the Kubernetes command line 
interface, Kubectl. Kubectl uses the Kubernetes API to interact with the 
cluster. In this module, you'll learn the most common Kubectl commands 
needed to create Deployments that run your applications on a Kubernetes 
cluster.

When you create a Deployment, you'll need to specify the container image for 
your application and the number of replicas that you want to run. You can 
change that information later by updating your Deployment; Modules 5 and 6 of 
the bootcamp discuss how you can scale and update your Deployments.

Applications need to be packaged into one of the supported container formats 
in order to be deployed on Kubernetes

For your first Deployment, you'll use a Node.js application packaged in a 
Docker container: we will build a container image to run this application.







Let’s deploy our first app on Kubernetes with the kubectl create deployment command. We need to provide the deployment name and app image location (include the full repository url for images hosted outside Docker hub).

kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1

Great! You just deployed your first application by creating a deployment. This performed a few things for you:

    searched for a suitable node where an instance of the application could be run (we have only 1 available node)
    scheduled the application to run on that Node
    configured the cluster to reschedule the instance on a new Node when needed

To list your deployments use the get deployments command:

kubectl get deployments

We see that there is 1 deployment running a single instance of your app. The instance is running inside a Docker container on your node.





LOAD BALANCING
https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/


Creating a service for an application running in five pods

    Run a Hello World application in your cluster:

service/load-balancer-example.yaml [Copy service/load-balancer-example.yaml to clipboard]

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: load-balancer-example
  name: hello-world
spec:
  replicas: 5
  selector:
    matchLabels:
      app.kubernetes.io/name: load-balancer-example
  template:
    metadata:
      labels:
        app.kubernetes.io/name: load-balancer-example
    spec:
      containers:
      - image: gcr.io/google-samples/node-hello:1.0
        name: hello-world
        ports:
        - containerPort: 8080

kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml

The preceding command creates a Deployment object and an associated ReplicaSet object. The ReplicaSet has five Pods, each of which runs the Hello World application.

    Display information about the Deployment:

    kubectl get deployments hello-world
    kubectl describe deployments hello-world

    Display information about your ReplicaSet objects:

    kubectl get replicasets
    kubectl describe replicasets

    Create a Service object that exposes the deployment:

    kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

    Display information about the Service:

    kubectl get services my-service

    The output is similar to this:

    NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)    AGE
    my-service   LoadBalancer   10.3.245.137   104.198.205.71   8080/TCP   54s

    Note: If the external IP address is shown as <pending>, wait for a minute and enter the same command again.

    Display detailed information about the Service:

    kubectl describe services my-service

    The output is similar to this:

    Name:           my-service
    Namespace:      default
    Labels:         app.kubernetes.io/name=load-balancer-example
    Annotations:    <none>
    Selector:       app.kubernetes.io/name=load-balancer-example
    Type:           LoadBalancer
    IP:             10.3.245.137
    LoadBalancer Ingress:   104.198.205.71
    Port:           <unset> 8080/TCP
    NodePort:       <unset> 32377/TCP
    Endpoints:      10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more...
    Session Affinity:   None
    Events:         <none>

    Make a note of the external IP address (LoadBalancer Ingress) exposed by your service. In this example, the external IP address is 104.198.205.71. Also note the value of Port and NodePort. In this example, the Port is 8080 and the NodePort is 32377.

    In the preceding output, you can see that the service has several endpoints: 10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal addresses of the pods that are running the Hello World application. To verify these are pod addresses, enter this command:

    kubectl get pods --output=wide

    The output is similar to this:

    NAME                         ...  IP         NODE
    hello-world-2895499144-1jaz9 ...  10.0.1.6   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-2e5uh ...  10.0.1.8   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-9m4h1 ...  10.0.0.6   gke-cluster-1-default-pool-e0b8d269-5v7a
    hello-world-2895499144-o4z13 ...  10.0.1.7   gke-cluster-1-default-pool-e0b8d269-1afc
    hello-world-2895499144-segjf ...  10.0.2.5   gke-cluster-1-default-pool-e0b8d269-cpuc

    Use the external IP address (LoadBalancer Ingress) to access the Hello World application:

    curl http://<external-ip>:<port>

    where <external-ip> is the external IP address (LoadBalancer Ingress) of your Service, and <port> is the value of Port in your Service description. If you are using minikube, typing minikube service my-service will automatically open the Hello World application in a browser.

    The response to a successful request is a hello message:

    Hello Kubernetes!

Cleaning up

To delete the Service, enter this command:

kubectl delete services my-service

To delete the Deployment, the ReplicaSet, and the Pods that are running the Hello World application, enter this command:

kubectl delete deployment hello-world





STATEFULSET



StatefulSets are intended to be used with stateful applications and distributed systems. However, the administration of stateful applications and distributed systems on Kubernetes is a broad, complex topic. In order to demonstrate the basic features of a StatefulSet, and not to conflate the former topic with the latter, you will deploy a simple web application using a StatefulSet.

After this tutorial, you will be familiar with the following.

    How to create a StatefulSet
    How a StatefulSet manages its Pods
    How to delete a StatefulSet
    How to scale a StatefulSet
    How to update a StatefulSet’s Pods

Before you begin

Before you begin this tutorial, you should familiarize yourself with the following Kubernetes concepts.

    Pods
    Cluster DNS
    Headless Services
    PersistentVolumes
    PersistentVolume Provisioning
    StatefulSets
    kubectl CLI

This tutorial assumes that your cluster is configured to dynamically provision PersistentVolumes. If your cluster is not configured to do so, you will have to manually provision two 1 GiB volumes prior to starting this tutorial.
Creating a StatefulSet

Begin by creating a StatefulSet using the example below. It is similar to the example presented in the StatefulSets concept. It creates a Headless Service, nginx, to publish the IP addresses of Pods in the StatefulSet, web.
application/web/web.yaml [Copy application/web/web.yaml to clipboard]

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

Download the example above, and save it to a file named web.yaml

You will need to use two terminal windows. In the first terminal, use kubectl get to watch the creation of the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In the second terminal, use kubectl apply to create the Headless Service and StatefulSet defined in web.yaml.

kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

The command above creates two Pods, each running an NGINX webserver. Get the nginx Service and the web StatefulSet to verify that they were created successfully.

kubectl get service nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         <none>        80/TCP    12s

kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         1         20s

Ordered Pod Creation

For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}. Examine the output of the kubectl get command in the first terminal. Eventually, the output will look like the example below.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         19s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s

Notice that the web-1 Pod is not launched until the web-0 Pod is Running and Ready.
Pods in a StatefulSet

Pods in a StatefulSet have a unique ordinal index and a stable network identity.
Examining the Pod’s Ordinal Index

Get the StatefulSet’s Pods.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          1m
web-1     1/1       Running   0          1m

As mentioned in the StatefulSets concept, the Pods in a StatefulSet have a sticky, unique identity. This identity is based on a unique ordinal index that is assigned to each Pod by the StatefulSet controller. The Pods’ names take the form <statefulset name>-<ordinal index>. Since the web StatefulSet has two replicas, it creates two Pods, web-0 and web-1.
Using Stable Network Identities

Each Pod has a stable hostname based on its ordinal index. Use kubectl exec to execute the hostname command in each Pod.

for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

Use kubectl run to execute a container that provides the nslookup command from the dnsutils package. Using nslookup on the Pods’ hostnames, you can examine their in-cluster DNS addresses.

kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm  
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.6

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.6

The CNAME of the headless service points to SRV records (one for each Pod that is Running and Ready). The SRV records point to A record entries that contain the Pods’ IP addresses.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pod -w -l app=nginx

In a second terminal, use kubectl delete to delete all the Pods in the StatefulSet.

kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Wait for the StatefulSet to restart them, and for both Pods to transition to Running and Ready.

kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s

Use kubectl exec and kubectl run to view the Pods hostnames and in-cluster DNS entries.

for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh 
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8

The Pods’ ordinals, hostnames, SRV records, and A record names have not changed, but the IP addresses associated with the Pods may have changed. In the cluster used for this tutorial, they have. This is why it is important not to configure other applications to connect to Pods in a StatefulSet by IP address.

If you need to find and connect to the active members of a StatefulSet, you should query the CNAME of the Headless Service (nginx.default.svc.cluster.local). The SRV records associated with the CNAME will contain only the Pods in the StatefulSet that are Running and Ready.

If your application already implements connection logic that tests for liveness and readiness, you can use the SRV records of the Pods ( web-0.nginx.default.svc.cluster.local, web-1.nginx.default.svc.cluster.local), as they are stable, and your application will be able to discover the Pods’ addresses when they transition to Running and Ready.
Writing to Stable Storage

Get the PersistentVolumeClaims for web-0 and web-1.

kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s

The StatefulSet controller created two PersistentVolumeClaims that are bound to two PersistentVolumes. As the cluster used in this tutorial is configured to dynamically provision PersistentVolumes, the PersistentVolumes were created and bound automatically.

The NGINX webservers, by default, will serve an index file at /usr/share/nginx/html/index.html. The volumeMounts field in the StatefulSets spec ensures that the /usr/share/nginx/html directory is backed by a PersistentVolume.

Write the Pods’ hostnames to their index.html files and verify that the NGINX webservers serve the hostnames.

for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

    Note:

    If you instead see 403 Forbidden responses for the above curl command, you will need to fix the permissions of the directory mounted by the volumeMounts (due to a bug when using hostPath volumes) with:

    for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done

    before retrying the curl command above.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pod -w -l app=nginx

In a second terminal, delete all of the StatefulSet’s Pods.

kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Examine the output of the kubectl get command in the first terminal, and wait for all of the Pods to transition to Running and Ready.

kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         34s

Verify the web servers continue to serve their hostnames.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though web-0 and web-1 were rescheduled, they continue to serve their hostnames because the PersistentVolumes associated with their PersistentVolumeClaims are remounted to their volumeMounts. No matter what node web-0and web-1 are scheduled on, their PersistentVolumes will be mounted to the appropriate mount points.
Scaling a StatefulSet

Scaling a StatefulSet refers to increasing or decreasing the number of replicas. This is accomplished by updating the replicas field. You can use either kubectl scale or kubectl patch to scale a StatefulSet.
Scaling Up

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

In another terminal window, use kubectl scale to scale the number of replicas to 5.

kubectl scale sts web --replicas=5
statefulset.apps/web scaled

Examine the output of the kubectl get command in the first terminal, and wait for the three additional Pods to transition to Running and Ready.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2h
web-1     1/1       Running   0          2h
NAME      READY     STATUS    RESTARTS   AGE
web-2     0/1       Pending   0          0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       ContainerCreating   0         0s
web-3     1/1       Running   0         18s
web-4     0/1       Pending   0         0s
web-4     0/1       Pending   0         0s
web-4     0/1       ContainerCreating   0         0s
web-4     1/1       Running   0         19s

The StatefulSet controller scaled the number of replicas. As with StatefulSet creation, the StatefulSet controller created each Pod sequentially with respect to its ordinal index, and it waited for each Pod’s predecessor to be Running and Ready before launching the subsequent Pod.
Scaling Down

In one terminal, watch the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In another terminal, use kubectl patch to scale the StatefulSet back down to three replicas.

kubectl patch sts web -p '{"spec":{"replicas":3}}'
statefulset.apps/web patched

Wait for web-4 and web-3 to transition to Terminating.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3h
web-1     1/1       Running             0          3h
web-2     1/1       Running             0          55s
web-3     1/1       Running             0          36s
web-4     0/1       ContainerCreating   0          18s
NAME      READY     STATUS    RESTARTS   AGE
web-4     1/1       Running   0          19s
web-4     1/1       Terminating   0         24s
web-4     1/1       Terminating   0         24s
web-3     1/1       Terminating   0         42s
web-3     1/1       Terminating   0         42s

Ordered Pod Termination

The controller deleted one Pod at a time, in reverse order with respect to its ordinal index, and it waited for each to be completely shutdown before deleting the next.

Get the StatefulSet’s PersistentVolumeClaims.

kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-2   Bound     pvc-e1125b27-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-3   Bound     pvc-e1176df6-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-4   Bound     pvc-e11bb5f8-b508-11e6-932f-42010a800002   1Gi        RWO           13h

There are still five PersistentVolumeClaims and five PersistentVolumes. When exploring a Pod’s stable storage, we saw that the PersistentVolumes mounted to the Pods of a StatefulSet are not deleted when the StatefulSet’s Pods are deleted. This is still true when Pod deletion is caused by scaling the StatefulSet down.
Updating StatefulSets

In Kubernetes 1.7 and later, the StatefulSet controller supports automated updates. The strategy used is determined by the spec.updateStrategy field of the StatefulSet API Object. This feature can be used to upgrade the container images, resource requests and/or limits, labels, and annotations of the Pods in a StatefulSet. There are two valid update strategies, RollingUpdate and OnDelete.

RollingUpdate update strategy is the default for StatefulSets.
Rolling Update

The RollingUpdate update strategy will update all Pods in a StatefulSet, in reverse ordinal order, while respecting the StatefulSet guarantees.

Patch the web StatefulSet to apply the RollingUpdate update strategy.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate"}}}'
statefulset.apps/web patched

In one terminal window, patch the web StatefulSet to change the container image again.

kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"gcr.io/google_containers/nginx-slim:0.8"}]'
statefulset.apps/web patched

In another terminal, watch the Pods in the StatefulSet.

kubectl get po -l app=nginx -w
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          7m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          8m
web-2     1/1       Terminating   0         8m
web-2     1/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Terminating   0         8m
web-2     0/1       Pending   0         0s
web-2     0/1       Pending   0         0s
web-2     0/1       ContainerCreating   0         0s
web-2     1/1       Running   0         19s
web-1     1/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Terminating   0         8m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         6s
web-0     1/1       Terminating   0         7m
web-0     1/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Terminating   0         7m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s

The Pods in the StatefulSet are updated in reverse ordinal order. The StatefulSet controller terminates each Pod, and waits for it to transition to Running and Ready prior to updating the next Pod. Note that, even though the StatefulSet controller will not proceed to update the next Pod until its ordinal successor is Running and Ready, it will restore any Pod that fails during the update to its current version. Pods that have already received the update will be restored to the updated version, and Pods that have not yet received the update will be restored to the previous version. In this way, the controller attempts to continue to keep the application healthy and the update consistent in the presence of intermittent failures.

Get the Pods to view their container images.

for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8

All the Pods in the StatefulSet are now running the previous container image.

Tip You can also use kubectl rollout status sts/<name> to view the status of a rolling update.
Staging an Update

You can stage an update to a StatefulSet by using the partition parameter of the RollingUpdate update strategy. A staged update will keep all of the Pods in the StatefulSet at the current version while allowing mutations to the StatefulSet’s .spec.template.

Patch the web StatefulSet to add a partition to the updateStrategy field.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'
statefulset.apps/web patched

Patch the StatefulSet again to change the container’s image.

kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"k8s.gcr.io/nginx-slim:0.7"}]'
statefulset.apps/web patched

Delete a Pod in the StatefulSet.

kubectl delete po web-2
pod "web-2" deleted

Wait for the Pod to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

Notice that, even though the update strategy is RollingUpdate the StatefulSet controller restored the Pod with its original container. This is because the ordinal of the Pod is less than the partition specified by the updateStrategy.
Rolling Out a Canary

You can roll out a canary to test a modification by decrementing the partition you specified above.

Patch the StatefulSet to decrement the partition.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'
statefulset.apps/web patched

Wait for web-2 to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.7

When you changed the partition, the StatefulSet controller automatically updated the web-2 Pod because the Pod’s ordinal was greater than or equal to the partition.

Delete the web-1 Pod.

kubectl delete po web-1
pod "web-1" deleted

Wait for the web-1 Pod to be Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Running       0          6m
web-1     0/1       Terminating   0          6m
web-2     1/1       Running       0          2m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Terminating   0         6m
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         18s

Get the web-1 Pods container.

kubectl get po web-1 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

web-1 was restored to its original configuration because the Pod’s ordinal was less than the partition. When a partition is specified, all Pods with an ordinal that is greater than or equal to the partition will be updated when the StatefulSet’s .spec.template is updated. If a Pod that has an ordinal less than the partition is deleted or otherwise terminated, it will be restored to its original configuration.
Phased Roll Outs

You can perform a phased roll out (e.g. a linear, geometric, or exponential roll out) using a partitioned rolling update in a similar manner to how you rolled out a canary. To perform a phased roll out, set the partition to the ordinal at which you want the controller to pause the update.

The partition is currently set to 2. Set the partition to 0.

kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":0}}}}'
statefulset.apps/web patched

Wait for all of the Pods in the StatefulSet to become Running and Ready.

kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3m
web-1     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          2m
web-1     1/1       Running   0         18s
web-0     1/1       Terminating   0         3m
web-0     1/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Terminating   0         3m
web-0     0/1       Pending   0         0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         3s

Get the Pod’s containers.

for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7

By moving the partition to 0, you allowed the StatefulSet controller to continue the update process.
On Delete

The OnDelete update strategy implements the legacy (1.6 and prior) behavior, When you select this update strategy, the StatefulSet controller will not automatically update Pods when a modification is made to the StatefulSet’s .spec.template field. This strategy can be selected by setting the .spec.template.updateStrategy.type to OnDelete.
Deleting StatefulSets

StatefulSet supports both Non-Cascading and Cascading deletion. In a Non-Cascading Delete, the StatefulSet’s Pods are not deleted when the StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its Pods are deleted.
Non-Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

Use kubectl delete to delete the StatefulSet. Make sure to supply the --cascade=false parameter to the command. This parameter tells Kubernetes to only delete the StatefulSet, and to not delete any of its Pods.

kubectl delete statefulset web --cascade=false
statefulset.apps "web" deleted

Get the Pods to examine their status.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          6m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          5m

Even though web has been deleted, all of the Pods are still Running and Ready. Delete web-0.

kubectl delete pod web-0
pod "web-0" deleted

Get the StatefulSet’s Pods.

kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          10m
web-2     1/1       Running   0          7m

As the web StatefulSet has been deleted, web-0 has not been relaunched.

In one terminal, watch the StatefulSet’s Pods.

kubectl get pods -w -l app=nginx

In a second terminal, recreate the StatefulSet. Note that, unless you deleted the nginx Service ( which you should not have ), you will see an error indicating that the Service already exists.

kubectl apply -f web.yaml
statefulset.apps/web created
service/nginx unchanged

Ignore the error. It only indicates that an attempt was made to create the nginx Headless Service even though that Service already exists.

Examine the output of the kubectl get command running in the first terminal.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          16m
web-2     1/1       Running   0          2m
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         18s
web-2     1/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m
web-2     0/1       Terminating   0         3m

When the web StatefulSet was recreated, it first relaunched web-0. Since web-1 was already Running and Ready, when web-0 transitioned to Running and Ready, it simply adopted this Pod. Since you recreated the StatefulSet with replicas equal to 2, once web-0 had been recreated, and once web-1 had been determined to already be Running and Ready, web-2 was terminated.

Let’s take another look at the contents of the index.html file served by the Pods’ webservers.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you deleted both the StatefulSet and the web-0 Pod, it still serves the hostname originally entered into its index.html file. This is because the StatefulSet never deletes the PersistentVolumes associated with a Pod. When you recreated the StatefulSet and it relaunched web-0, its original PersistentVolume was remounted.
Cascading Delete

In one terminal window, watch the Pods in the StatefulSet.

kubectl get pods -w -l app=nginx

In another terminal, delete the StatefulSet again. This time, omit the --cascade=false parameter.

kubectl delete statefulset web
statefulset.apps "web" deleted

Examine the output of the kubectl get command running in the first terminal, and wait for all of the Pods to transition to Terminating.

kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          11m
web-1     1/1       Running   0          27m
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Terminating   0          12m
web-1     1/1       Terminating   0         29m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-0     0/1       Terminating   0         12m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m
web-1     0/1       Terminating   0         29m

As you saw in the Scaling Down section, the Pods are terminated one at a time, with respect to the reverse order of their ordinal indices. Before terminating a Pod, the StatefulSet controller waits for the Pod’s successor to be completely terminated.

Note that, while a cascading delete will delete the StatefulSet and its Pods, it will not delete the Headless Service associated with the StatefulSet. You must delete the nginx Service manually.

kubectl delete service nginx
service "nginx" deleted

Recreate the StatefulSet and Headless Service one more time.

kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

When all of the StatefulSet’s Pods transition to Running and Ready, retrieve the contents of their index.html files.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you completely deleted the StatefulSet, and all of its Pods, the Pods are recreated with their PersistentVolumes mounted, and web-0 and web-1 will still serve their hostnames.

Finally delete the web StatefulSet and the nginx service.

kubectl delete service nginx
service "nginx" deleted

kubectl delete statefulset web
statefulset "web" deleted

Pod Management Policy

For some distributed systems, the StatefulSet ordering guarantees are unnecessary and/or undesirable. These systems require only uniqueness and identity. To address this, in Kubernetes 1.7, we introduced .spec.podManagementPolicy to the StatefulSet API Object.
OrderedReady Pod Management

OrderedReady pod management is the default for StatefulSets. It tells the StatefulSet controller to respect the ordering guarantees demonstrated above.
Parallel Pod Management

Parallel pod management tells the StatefulSet controller to launch or terminate all Pods in parallel, and not to wait for Pods to become Running and Ready or completely terminated prior to launching or terminating another Pod.
application/web/web-parallel.yaml [Copy application/web/web-parallel.yaml to clipboard]

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  podManagementPolicy: "Parallel"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

Download the example above, and save it to a file named web-parallel.yaml

This manifest is identical to the one you downloaded above except that the .spec.podManagementPolicy of the web StatefulSet is set to Parallel.

In one terminal, watch the Pods in the StatefulSet.

kubectl get po -l app=nginx -w

In another terminal, create the StatefulSet and Service in the manifest.

kubectl apply -f web-parallel.yaml
service/nginx created
statefulset.apps/web created

Examine the output of the kubectl get command that you executed in the first terminal.

kubectl get po -l app=nginx -w
NAME      READY     STATUS    RESTARTS   AGE
web-0     0/1       Pending   0          0s
web-0     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-1     0/1       Pending   0         0s
web-0     0/1       ContainerCreating   0         0s
web-1     0/1       ContainerCreating   0         0s
web-0     1/1       Running   0         10s
web-1     1/1       Running   0         10s

The StatefulSet controller launched both web-0 and web-1 at the same time.

Keep the second terminal open, and, in another terminal window scale the StatefulSet.

kubectl scale statefulset/web --replicas=4
statefulset.apps/web scaled

Examine the output of the terminal where the kubectl get command is running.

web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         7s
web-3     0/1       ContainerCreating   0         7s
web-2     1/1       Running   0         10s
web-3     1/1       Running   0         26s

The StatefulSet controller launched two new Pods, and it did not wait for the first to become Running and Ready prior to launching the second.

Keep this terminal open, and in another terminal delete the web StatefulSet.

kubectl delete sts web

Again, examine the output of the kubectl get command running in the other terminal.

web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-1     1/1       Terminating   0         44m
web-0     1/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m

The StatefulSet controller deletes all Pods concurrently, it does not wait for a Pod’s ordinal successor to terminate prior to deleting that Pod.

Close the terminal where the kubectl get command is running and delete the nginx Service.

kubectl delete svc nginx

Cleaning up

You will need to delete the persistent storage media for the PersistentVolumes used in this tutorial. Follow the necessary steps, based on your environment, storage configuration, and provisioning method, to ensure that all storage is reclaimed.


###############################################################################
#
# Learn Kubernetes Basics - Part 4 - Explore your app
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 5 - Expose Your App Publicly
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 6 - Scale Your App
#
###############################################################################


###############################################################################
#
# Learn Kubernetes Basics - Part 7 - Update Your App
#
###############################################################################





PART III : create a deployment

Using kubectl to Create a Deployment
Objectives

    Learn about application Deployments.
    Deploy your first app on Kubernetes with kubectl.

Kubernetes Deployments

Once you have a running Kubernetes cluster, you can deploy your containerized applications on top of it. To do so, you create a Kubernetes Deployment configuration. The Deployment instructs Kubernetes how to create and update instances of your application. Once you've created a Deployment, the Kubernetes master schedules mentioned application instances onto individual Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment Controller continuously monitors those instances. If the Node hosting an instance goes down or is deleted, the Deployment controller replaces the instance with an instance on another Node in the cluster. This provides a self-healing mechanism to address machine failure or maintenance.

In a pre-orchestration world, installation scripts would often be used to start applications, but they did not allow recovery from machine failure. By both creating your application instances and keeping them running across Nodes, Kubernetes Deployments provide a fundamentally different approach to application management.
Summary:

    Deployments
    Kubectl

A Deployment is responsible for creating and updating instances of your application

Deploying your first app on Kubernetes


You can create and manage a Deployment by using the Kubernetes command line interface, Kubectl. Kubectl uses the Kubernetes API to interact with the cluster. In this module, you'll learn the most common Kubectl commands needed to create Deployments that run your applications on a Kubernetes cluster.

When you create a Deployment, you'll need to specify the container image for your application and the number of replicas that you want to run. You can change that information later by updating your Deployment; Modules 5 and 6 of the bootcamp discuss how you can scale and update your Deployments.

Applications need to be packaged into one of the supported container formats in order to be deployed on Kubernetes

For your first Deployment, you'll use a Node.js application packaged in a Docker container. (If you didn't already try creating a Node.js application and deploying it using a container, you can do that first by following the instructions from the Hello Minikube tutorial).

Now that you know what Deployments are, let's go to the online tutorial and deploy our first app!



==============================================================================

LOGS

==============================================================================


thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          82s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          82s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          28s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          30s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          82s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          93s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          93s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          39s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          52s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          41s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          93s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          52s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          99s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          99s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          45s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          58s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          47s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          99s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          58s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE    IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          101s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          101s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          47s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          60s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          49s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          101s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          60s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds-amd64 created
daemonset.apps/kube-flannel-ds-arm64 created
daemonset.apps/kube-flannel-ds-arm created
daemonset.apps/kube-flannel-ds-ppc64le created
daemonset.apps/kube-flannel-ds-s390x created
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending    0          2m23s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending    0          2m23s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          89s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          102s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          91s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          0/1     Init:0/1   0          3s      10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          2m23s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          102s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged configured
clusterrole.rbac.authorization.k8s.io/flannel unchanged
clusterrolebinding.rbac.authorization.k8s.io/flannel unchanged
serviceaccount/flannel unchanged
configmap/kube-flannel-cfg unchanged
daemonset.apps/kube-flannel-ds-amd64 unchanged
daemonset.apps/kube-flannel-ds-arm64 unchanged
daemonset.apps/kube-flannel-ds-arm unchanged
daemonset.apps/kube-flannel-ds-ppc64le unchanged
daemonset.apps/kube-flannel-ds-s390x unchanged
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m35s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m35s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          101s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          114s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          103s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          15s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m35s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          114s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m39s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m39s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          105s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          118s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          107s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          19s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m39s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          118s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending   0          2m42s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Pending   0          2m42s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          108s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m1s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          110s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          22s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m42s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m1s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Pending             0          2m46s   <none>      <none>       <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     ContainerCreating   0          2m46s   <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          112s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          2m5s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          114s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          26s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          2m46s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          2m5s    10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     ContainerCreating   0          2m50s   <none>       k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             0/1     Running             0          2m50s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          116s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          2m9s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          118s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          30s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          2m50s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          2m9s    10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m53s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m53s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          119s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          33s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m53s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m12s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m55s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m55s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          35s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m55s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          2m57s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          2m57s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          37s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          2m57s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m16s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             0/1     Running   0          3m      10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          3m      10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m19s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          40s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          3m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m19s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          3m3s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          3m3s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          2m9s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          2m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          2m11s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          43s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          3m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          2m22s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          4m30s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          4m30s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          3m36s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          3m49s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          3m38s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             0          11s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          2m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          4m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-c78r7                     0/1     ContainerCreating   0          11s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          3m49s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          4m34s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          4m34s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m40s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m53s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m42s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running   0          15s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          2m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          4m34s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running   0          15s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m53s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE    VERSION
k8s-master   Ready    master   5m3s   v1.16.3
k8s-slave1   Ready    <none>   24s    v1.16.3
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m52s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m52s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          5m58s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m11s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m33s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m32s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          2s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m52s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          2s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m33s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m11s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m54s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m54s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m13s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m35s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m34s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          4s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m54s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          4s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m35s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m13s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m56s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m56s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m15s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m4s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m37s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m36s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          6s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m56s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          6s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m37s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m15s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running             0          6m57s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running             0          6m57s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          6m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          6m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          6m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running             1          2m38s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running             0          4m37s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1            0          7s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running             0          6m57s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     0/1     ContainerCreating   0          7s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running             0          2m38s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          6m16s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          6m59s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          6m59s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m5s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m18s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m7s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m40s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m39s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          9s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          6m59s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          9s      10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m40s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m18s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          7m      10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          7m      10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m19s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m41s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m40s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          10s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          7m      10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          10s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m41s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m19s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS     RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running    0          7m2s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running    0          7m2s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running    0          6m8s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running    0          6m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running    0          6m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running    1          2m43s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running    0          4m42s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     Init:0/1   0          12s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running    0          7m2s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running    0          12s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running    0          2m43s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running    0          6m21s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS            RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running           0          7m4s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running           0          7m4s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running           0          6m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running           0          6m23s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running           0          6m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running           1          2m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running           0          4m44s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          0/1     PodInitializing   0          14s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running           0          7m4s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running           0          14s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running           0          2m45s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running           0          6m23s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-2mwj9             1/1     Running   0          7m6s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-5644d7b6d9-pktlz             1/1     Running   0          7m6s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          6m12s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          6m25s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          6m14s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-lgqv8          1/1     Running   1          2m47s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-flannel-ds-amd64-ntrwg          1/1     Running   0          4m46s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-vhf6w          1/1     Running   0          16s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-8xwsz                     1/1     Running   0          7m6s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-9qcfg                     1/1     Running   0          16s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-c78r7                     1/1     Running   0          2m47s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          6m25s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   7m33s   v1.16.3
k8s-slave1   Ready    <none>   2m54s   v1.16.3
k8s-slave2   Ready    <none>   23s     v1.16.3
thierry@k8s-master:~$ 







===============================================================================
===============================================================================

APPENDIX

===============================================================================
===============================================================================




======================================
2.1 - Concepts - nodes and controllers
======================================


2.1.1 Nodes
===========

As seen above, a node is a worker machine in Kubernetes. A node may be a VM or 
physical machine (we then speak of a 'bare metal deployment'), depending on the 
cluster. Each node contains the services necessary to run pods and is managed 
by the master components. The services on a node include:
    - the container runtime,
    - kubelet
    - kube-proxy.


a) Node Status
==============

A node’s status contains the following information:
    - Addresses
    - Conditions
    - Capacity and Allocatable
    - Info

Node status and other details about a node can be displayed using below 
command:

$ kubectl describe node <insert-node-name-here>

Addresses
=========

The usage of these fields varies depending on your cloud provider or bare metal 
configuration.

    HostName:   The hostname as reported by the node’s kernel. Can be 
                overridden via the kubelet --hostname-override parameter.
    ExternalIP: Typically the IP address of the node that is externally 
                routable (available from outside the cluster).
    InternalIP: Typically the IP address of the node that is routable only 
                within the cluster.

Conditions
==========

The conditions field describes the status of all Running nodes. Examples of 
conditions include:

Node                Condition
Ready	            True if the node is healthy and ready to accept pods,
                    False if the node is not healthy and is not accepting pods, 
                    Unknown if the node controller has not heard from the node 
                      in the last node-monitor-grace-period (default is 40 
                      seconds)
MemoryPressure	    True if pressure exists on the node memory – that is, if 
                      the node memory is low; 
                    False otherwise
PIDPressure	        True if pressure exists on the processes – that is, if 
                      there are too many processes on the node; 
                    False otherwise
DiskPressure	    True if pressure exists on the disk size – that is, if 
                      the disk capacity is low; 
                    False otherwise
NetworkUnavailable	True if the network for the node is not correctly 
                      configured, 
                    False otherwise

The node condition is represented as a JSON object. For example, the following 
response describes a healthy node.

"conditions": [
  {
    "type": "Ready",
    "status": "True",
    "reason": "KubeletReady",
    "message": "kubelet is posting ready status",
    "lastHeartbeatTime": "2019-06-05T18:38:35Z",
    "lastTransitionTime": "2019-06-05T11:41:27Z"
  }
]


If the Status of the Ready condition remains Unknown or False for longer than 
the pod-eviction-timeout, an argument is passed to the kube-controller-manager 
and all the Pods on the node are scheduled for deletion by the Node Controller. 
The default eviction timeout duration is five minutes. In some cases when the 
node is unreachable, the apiserver is unable to communicate with the kubelet 
on the node. The decision to delete the pods cannot be communicated to the 
kubelet until communication with the apiserver is re-established. In the 
meantime, the pods that are scheduled for deletion may continue to run on the 
partitioned node.

In versions of Kubernetes prior to 1.5, the node controller would force delete 
these unreachable pods from the apiserver. However, in 1.5 and higher, the 
node controller does not force delete pods until it is confirmed that they 
have stopped running in the cluster. You can see the pods that might be 
running on an unreachable node as being in the Terminating or Unknown state. 
In cases where Kubernetes cannot deduce from the underlying infrastructure if 
a node has permanently left a cluster, the cluster administrator may need to 
delete the node object by hand. Deleting the node object from Kubernetes 
causes all the Pod objects running on the node to be deleted from the 
apiserver, and frees up their names.

In version 1.12, TaintNodesByCondition feature is promoted to beta, so node 
lifecycle controller automatically creates taints that represent conditions. 
Similarly the scheduler ignores conditions when considering a Node; instead it 
looks at the Node’s taints and a Pod’s tolerations.

Now users can choose between the old scheduling model and a new, more flexible 
scheduling model. A Pod that does not have any tolerations gets scheduled 
according to the old model. But a Pod that tolerates the taints of a particular 
Node can be scheduled on that Node.

    Caution: Enabling this feature creates a small delay between the time when 
      a condition is observed and when a taint is created. This delay is 
      usually less than one second, but it can increase the number of Pods 
      that are successfully scheduled but rejected by the kubelet.

Capacity and Allocatable
========================

Describes the resources available on the node: CPU, memory and the maximum 
number of pods that can be scheduled onto the node.

The fields in the capacity block indicate the total amount of resources that 
a Node has. The allocatable block indicates the amount of resources on a Node 
that is available to be consumed by normal Pods.

You may read more about capacity and allocatable resources while learning how 
to reserve compute resources on a Node.

Info
====

Describes general information about the node, such as kernel version, 
Kubernetes version (kubelet and kube-proxy version), Docker version (if used), 
and OS name. This information is gathered by Kubelet from the node.

b) Management
=============

Unlike pods and services, a node is not inherently created by Kubernetes: it 
is created externally by cloud providers like Google Compute Engine, or it 
exists in your pool of physical or virtual machines. So when Kubernetes 
creates a node, it creates an object that represents the node. After creation, 
Kubernetes checks whether the node is valid or not. For example, if you try to 
create a node from the following content:

{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}

Kubernetes creates a node object internally (the representation), and 
validates the node by health checking based on the metadata.name field. If 
the node is valid – that is, if all necessary services are running – it is 
eligible to run a pod. Otherwise, it is ignored for any cluster activity until 
it becomes valid.

    Note: Kubernetes keeps the object for the invalid node and keeps checking 
      to see whether it becomes valid. You must explicitly delete the Node 
      object to stop this process.

Currently, there are three components that interact with the Kubernetes node 
interface: node controller, kubelet, and kubectl.

Node Controller
===============

The node controller is a Kubernetes master component which manages various 
aspects of nodes.

The node controller has multiple roles in a node’s life.

    - The first is assigning a CIDR block to the node when it is registered (if 
      CIDR assignment is turned on).

    - The second is keeping the node controller’s internal list of nodes up to 
      date with the cloud provider’s list of available machines. When running 
      in a cloud environment, whenever a node is unhealthy, the node controller 
      asks the cloud provider if the VM for that node is still available. If 
      not, the node controller deletes the node from its list of nodes.

    - The third is monitoring the nodes’ health. The node controller is 
      responsible for updating the NodeReady condition of NodeStatus to 
      ConditionUnknown when a node becomes unreachable (i.e. the node 
      controller stops receiving heartbeats for some reason, e.g. due to the 
      node being down), and then later evicting all the pods from the node 
      (using graceful termination) if the node continues to be unreachable. 
      (The default timeouts are 40s to start reporting ConditionUnknown and 
      5m after that to start evicting pods.) The node controller checks the 
      state of each node every --node-monitor-period seconds.

In versions of Kubernetes prior to 1.13, NodeStatus is the heartbeat from the 
node. Node lease feature is enabled by default since 1.14 as a beta feature 
(feature gate NodeLease, KEP-0009). When node lease feature is enabled, each 
node has an associated Lease object in kube-node-lease namespace that is 
renewed by the node periodically, and both NodeStatus and node lease are 
treated as heartbeats from the node. Node leases are renewed frequently while 
NodeStatus is reported from node to master only when there is some change or 
enough time has passed (default is 1 minute, which is longer than the default 
timeout of 40 seconds for unreachable nodes). Since node lease is much more 
lightweight than NodeStatus, this feature makes node heartbeat significantly 
cheaper from both scalability and performance perspectives.

In Kubernetes 1.4, we updated the logic of the node controller to better 
handle cases when a large number of nodes have problems with reaching the 
master (e.g. because the master has networking problem). Starting with 1.4, 
the node controller looks at the state of all nodes in the cluster when making 
a decision about pod eviction.

In most cases, node controller limits the eviction rate to --node-eviction-rate 
(default 0.1) per second, meaning it won’t evict pods from more than 1 node 
per 10 seconds.

The node eviction behavior changes when a node in a given availability zone 
becomes unhealthy. The node controller checks what percentage of nodes in the 
zone are unhealthy (NodeReady condition is ConditionUnknown or ConditionFalse) 
at the same time. If the fraction of unhealthy nodes is at least 
--unhealthy-zone-threshold (default 0.55) then the eviction rate is reduced: 
if the cluster is small (i.e. has less than or equal to 
--large-cluster-size-threshold nodes - default 50) then evictions are stopped, 
otherwise the eviction rate is reduced to --secondary-node-eviction-rate 
(default 0.01) per second. The reason these policies are implemented per 
availability zone is because one availability zone might become partitioned 
from the master while the others remain connected. If your cluster does not 
span multiple cloud provider availability zones, then there is only one 
availability zone (the whole cluster).

A key reason for spreading your nodes across availability zones is so that the 
workload can be shifted to healthy zones when one entire zone goes down. 
Therefore, if all nodes in a zone are unhealthy then node controller evicts at 
the normal rate --node-eviction-rate. The corner case is when all zones are 
completely unhealthy (i.e. there are no healthy nodes in the cluster). In such 
case, the node controller assumes that there’s some problem with master 
connectivity and stops all evictions until some connectivity is restored.

Starting in Kubernetes 1.6, the NodeController is also responsible for evicting 
pods that are running on nodes with NoExecute taints, when the pods do not 
tolerate the taints. Additionally, as an alpha feature that is disabled by 
default, the NodeController is responsible for adding taints corresponding 
to node problems like node unreachable or not ready. See this documentation 
for details about NoExecute taints and the alpha feature.

Starting in version 1.8, the node controller can be made responsible for 
creating taints that represent Node conditions. This is an alpha feature of 
version 1.8.

Self-Registration of Nodes
==========================

When the kubelet flag --register-node is true (the default), the kubelet will 
attempt to register itself with the API server. This is the preferred pattern, 
used by most distros.

For self-registration, the kubelet is started with the following options:

    --kubeconfig - Path to credentials to authenticate itself to the apiserver.
    --cloud-provider - How to talk to a cloud provider to read metadata about 
        itself.
    --register-node - Automatically register with the API server.
    --register-with-taints - Register the node with the given list of taints 
        (comma separated <key>=<value>:<effect>). No-op if register-node is 
        false.
    --node-ip - IP address of the node.
    --node-labels - Labels to add when registering the node in the cluster 
        (see label restrictions enforced by the NodeRestriction admission 
        plugin in 1.13+).
    --node-status-update-frequency - Specifies how often kubelet posts node 
        status to master.

When the Node authorization mode and NodeRestriction admission plugin are 
enabled, kubelets are only authorized to create/modify their own Node resource.


Manual Node Administration
==========================

A cluster administrator can create and modify node objects.

If the administrator wishes to create node objects manually, set the kubelet 
flag --register-node=false.

The administrator can modify node resources (regardless of the setting of 
--register-node). Modifications include setting labels on the node and marking 
it unschedulable.

Labels on nodes can be used in conjunction with node selectors on pods to 
control scheduling, e.g. to constrain a pod to only be eligible to run on a 
subset of the nodes.

Marking a node as unschedulable prevents new pods from being scheduled to that 
node, but does not affect any existing pods on the node. This is useful as a 
preparatory step before a node reboot, etc. For example, to mark a node 
unschedulable, run this command:


kubectl cordon $NODENAME
========================
    Note: Pods created by a DaemonSet controller bypass the Kubernetes 
        scheduler and do not respect the unschedulable attribute on a node. 
        This assumes that daemons belong on the machine even if it is being 
        drained of applications while it prepares for a reboot.


Node capacity
=============

The capacity of the node (number of cpus and amount of memory) is part of the 
node object. Normally, nodes register themselves and report their capacity 
when creating the node object. If you are doing manual node administration, 
then you need to set node capacity when adding a node.

The Kubernetes scheduler ensures that there are enough resources for all the 
pods on a node. It checks that the sum of the requests of containers on the 
node is no greater than the node capacity. It includes all containers started 
by the kubelet, but not containers started directly by the container runtime 
nor any process running outside of the containers.

If you want to explicitly reserve resources for non-Pod processes, follow this 
tutorial to reserve resources for system daemons.


