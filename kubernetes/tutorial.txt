#!/usr/bin/env bash


###############################################################################
#
# Learn Kubernetes Basics - Part 0 - pre-requisites
#
###############################################################################


This tutorial mixes elements from the official Kubernetes site 'get started' 
sections (many of these), and also from Dada's blog (specifically for setting 
up the Kubernetes clusters on VMs running on a laptop):
    https://kubernetes.io/docs/tutorials/kubernetes-basics/
    https://www.dadall.info/article658/preparer-virtualbox-pour-kubernetes

Here are the identified pre-requisites to run this tutorial and actually
learn something from this experience:

    - have a linux laptop, with a 'admin' account (i.e. need to have
      the sudo privilege). Ubuntu will be perfect for beginners.
    - have curl, git and virtualbox installed

Also, you will find several resources in the directory:

    - flannel.yml - a kubernetes network model
    - server.js   - a node.js example application used as of part 3
    - Dockerfile  - to build the container for the application
    - load-balancer-example.yaml - to dsitribute teh app on several pods
    - several VirtualBox VM images in order to build the Kubernetes cluster,
      located in the "VM_images" sub-directory


This is it. Nothing else is needed... except the desire to learn :-)



###############################################################################
#
# Learn Kubernetes Basics - Part 1 - Kubernetes Basics
#
###############################################################################


=====================
1.1 Kubernetes Basics
=====================

This tutorial provides a walkthrough of the basics of the Kubernetes cluster
orchestration system. Each module contains some background information on
major Kubernetes features and concepts. You will actually deploy and manage 
a simple cluster and its containerized applications by yourself.

Following the tutorial steps, you can learn to:

  - Deploy a Kubernetes cluster
  - Deploy a containerized application on a cluster.
  - Scale the deployment.
  - Update the containerized application with a new software version.
  - Debug the containerized application.

I purposedly copied some reference documentation from the official Kubernetes
site on design and architecture (section 1.3 + APPENDIX 1) because it is 
examplary of the philosophy underlying the whole Kubernetes project: clarity of 
purpose translated in the ability of distributed teams to efficiently 
contributes into a larger goal, native and forceful openness of the design, 
resilience-by-design deeply embeded in the implementation of every layer. It 
is REMARKABLE and should be a source of inspiration in many Orange 
endeavours...



===================================
1.2 What can Kubernetes do for you?
===================================

With modern web services, users expect applications to be available 24/7, and
developers expect to deploy new versions of those applications several times
a day.

Kubernetes coordinates a highly available cluster of computers that are 
connected to work as a single unit. The abstractions in Kubernetes allow 
you to deploy containerized applications to a cluster without tying them 
specifically to individual machines. To make use of this new model of 
deployment, applications need to be packaged in a way that decouples them 
from individual hosts: they need to be containerized. 

    KUBERNETES CAN MANAGE CONTAINERIZED APPLICATIONS ONLY

Containerized applications are more flexible and available than in past 
deployment models, where applications were installed directly onto specific 
machines as packages deeply integrated into the host. Kubernetes automates the 
distribution and scheduling of application containers across a cluster in 
a more efficient way.


Kubernetes is a production-ready, open source platform designed
with Google's accumulated experience in container orchestration, combined 
with best-of-breed ideas from the community.


A Kubernetes cluster consists of two types of resources:

    - The Master coordinates the cluster
    - Nodes are the workers that run applications


(picture: cluster Diagram)


The Master is responsible for managing the cluster. The master coordinates 
all activities in your cluster, such as scheduling applications, 
maintaining applications' desired state, scaling applications, and rolling 
out new updates.

A node is a VM or a physical computer that serves as a worker machine in 
a Kubernetes cluster. Each node has a Kubelet, which is an agent for 
managing the node and communicating with the Kubernetes master. The node 
should also have tools for handling container operations, such as Docker or 
rkt. A Kubernetes cluster that handles production traffic should have a 
minimum of three nodes.

Masters manage the cluster and the nodes are used to host the running 
applications.

When you deploy applications on Kubernetes, you tell the master to start 
the application containers. The master schedules the containers to run on 
the cluster's nodes. The nodes communicate with the master using the 
Kubernetes API, which the master exposes. End users can also use the 
Kubernetes API directly to interact with the cluster.

A Kubernetes cluster can be deployed on either physical or virtual 
machines. In our case, I explained in Part I how to deploy a Kubernetes 
cluster in such a way.



========================================
1.3 - Kubernetes Design and Architecture
========================================


1.3.1 - Scope
=============

Kubernetes is a platform for deploying and managing containers. Kubernetes 
provides a container runtime, container orchestration, container-centric 
infrastructure orchestration, self-healing mechanisms such as health checking 
and re-scheduling, and service discovery and load balancing.

Kubernetes aspires to be an extensible, pluggable, building-block OSS 
platform and toolkit. Therefore, architecturally, we want Kubernetes to be 
built as a collection of pluggable components and layers, with the ability to 
use alternative schedulers, controllers, storage systems, and distribution 
mechanisms, and we're evolving its current code in that direction. 
Furthermore, we want others to be able to extend Kubernetes functionality, 
such as with higher-level PaaS functionality or multi-cluster layers, without 
modification of core Kubernetes source. Therefore, its API isn't just (or 
even necessarily mainly) targeted at end users, but at tool and extension 
developers. Its APIs are intended to serve as the foundation for an open 
ecosystem of tools, automation systems, and higher-level API layers. 
Consequently, there are no "internal" inter-component APIs. All APIs are 
visible and available, including the APIs used by the scheduler, the node 
controller, the replication-controller manager, Kubelet's API, etc. There's 
no glass to break -- in order to handle more complex use cases, one can just 
access the lower-level APIs in a fully transparent, composable manner.


1.3.2 - Goals
=============

The project is committed to the following (aspirational) design ideals:

    - Portable. Kubernetes runs everywhere -- public cloud, private cloud, 
      bare metal, laptop -- with consistent behavior so that applications and 
      tools are portable throughout the ecosystem as well as between 
      development and production environments.
    - General-purpose. Kubernetes should run all major categories of 
      workloads to enable you to run all of your workloads on a single 
      infrastructure, stateless and stateful, microservices and monoliths, 
      services and batch, greenfield and legacy.
    - Meet users partway. Kubernetes doesn’t just cater to purely greenfield 
      cloud-native applications, nor does it meet all users where they are. 
      It focuses on deployment and management of microservices and 
      cloud-native applications, but provides some mechanisms to facilitate 
      migration of monolithic and legacy applications.
    - Flexible. Kubernetes functionality can be consumed a la carte and (in 
      most cases) Kubernetes does not prevent you from using your own 
      solutions in lieu of built-in functionality.
    - Extensible. Kubernetes enables you to integrate it into your 
      environment and to add the additional capabilities you need, by 
      exposing the same interfaces used by built-in functionality.
    - Automatable. Kubernetes aims to dramatically reduce the burden of 
      manual operations. It supports both declarative control by specifying 
      users’ desired intent via its API, as well as imperative control to 
      support higher-level orchestration and automation. The declarative 
      approach is key to the system’s self-healing and autonomic capabilities.
    - Advance the state of the art. While Kubernetes intends to support 
      non-cloud-native applications, it also aspires to advance the 
      cloud-native and DevOps state of the art, such as in the participation 
      of applications in their own management. However, in doing so, we 
      strive not to force applications to lock themselves into Kubernetes 
      APIs, which is, for example, why we prefer configuration over 
      convention in the downward API. Additionally, Kubernetes is not bound 
      by the lowest common denominator of systems upon which it depends, such 
      as container runtimes and cloud providers. An example where we pushed 
      the envelope of what was achievable was in its IP per Pod networking 
      model.


1.3.3 - Architecture
====================

A running Kubernetes cluster contains node agents (kubelet) and a cluster 
control plane (AKA master), with cluster state backed by a distributed storage 
system (etcd).

(image: Kubernetes architecture)


a) Cluster control plane (AKA master)
=====================================

The Kubernetes control plane is split into a set of components, which can all 
run on a single master node, or can be replicated in order to support 
high-availability clusters, or can even be run on Kubernetes itself 
(AKA self-hosted).

Kubernetes provides a REST API supporting primarily CRUD operations on 
(mostly) persistent resources, which serve as the hub of its control plane. 
Kubernetes’s API provides IaaS-like container-centric primitives such as Pods, 
Services, and Ingress, and also lifecycle APIs to support orchestration 
(self-healing, scaling, updates, termination) of common types of workloads, 
such as ReplicaSet (simple fungible/stateless app manager), Deployment 
(orchestrates updates of stateless apps), Job (batch), CronJob (cron), 
DaemonSet (cluster services), and StatefulSet (stateful apps). We 
deliberately decoupled service naming/discovery and load balancing from 
application implementation, since the latter is diverse and open-ended.

Both user clients and components containing asynchronous controllers interact 
with the same API resources, which serve as coordination points, common 
intermediate representation, and shared state. Most resources contain 
metadata, including labels and annotations, fully elaborated desired state 
(spec), including default values, and observed state (status).

Controllers work continuously to drive the actual state towards the desired 
state, while reporting back the currently observed state for users and for 
other controllers.

While the controllers are level-based (as described here and here) to 
maximize fault tolerance, they typically watch for changes to relevant 
resources in order to minimize reaction latency and redundant work. This 
enables decentralized and decoupled choreography-like coordination without a 
message bus.


API Server
==========

The API server serves up the Kubernetes API. It is intended to be a relatively 
simple server, with most/all business logic implemented in separate components 
or in plug-ins. It mainly processes REST operations, validates them, and 
updates the corresponding objects in etcd (and perhaps eventually other 
stores). Note that, for a number of reasons, Kubernetes deliberately does not 
support atomic transactions across multiple resources.

Kubernetes cannot function without this basic API machinery, which includes:

    - REST semantics, watch, durability and consistency guarantees, API 
      versioning, defaulting, and validation
    - Built-in admission-control semantics, synchronous admission-control 
      hooks, and asynchronous resource initialization
    - API registration and discovery

Additionally, the API server acts as the gateway to the cluster. By 
definition, the API server must be accessible by clients from outside the 
cluster, whereas the nodes, and certainly containers, may not be. Clients 
authenticate the API server and also use it as a bastion and proxy/tunnel to 
nodes and pods (and services).


Cluster state store
===================

All persistent cluster state is stored in an instance of etcd. This provides 
a way to store configuration data reliably. With watch support, coordinating 
components can be notified very quickly of changes.


Controller-Manager Server
=========================

Most other cluster-level functions are currently performed by a separate 
process, called the Controller Manager. It performs both lifecycle functions 
(e.g., namespace creation and lifecycle, event garbage collection, 
terminated-pod garbage collection, cascading-deletion garbage collection, 
node garbage collection) and API business logic (e.g., scaling of pods 
controlled by a ReplicaSet).

The application management and composition layer, providing self-healing, 
scaling, application lifecycle management, service discovery, routing, and 
service binding and provisioning.

These functions may eventually be split into separate components to make them 
more easily extended or replaced.


Scheduler
=========

Kubernetes enables users to ask a cluster to run a set of containers. The 
scheduler component automatically chooses hosts to run those containers on.

The scheduler watches for unscheduled pods and binds them to nodes via the 
/binding pod subresource API, according to the availability of the requested 
resources, quality of service requirements, affinity and anti-affinity 
specifications, and other constraints.

Kubernetes supports user-provided schedulers and multiple concurrent cluster 
schedulers, using the shared-state approach pioneered by Omega. In addition to 
the disadvantages of pessimistic concurrency described by the Omega paper, 
two-level scheduling models that hide information from the upper-level 
schedulers need to implement all of the same features in the lower-level 
scheduler as required by all upper-layer schedulers in order to ensure that 
their scheduling requests can be satisfied by available desired resources.


b) The Kubernetes Node
======================

A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may 
be either a virtual or a physical machine, depending on the cluster. Each Node 
is managed by the Master. A Node can have multiple pods, and the Kubernetes 
master automatically handles scheduling the pods across the Nodes in the 
cluster. The Master's automatic scheduling takes into account the available 
resources on each Node.

Every Kubernetes Node runs at least:

    - Kubelet, a process responsible for communication between the Kubernetes 
      Master and the Node; it manages the Pods and the containers running on a 
      machine.
    - A container runtime (Docker in our case) responsible for pulling the 
      container image from a registry, unpacking the container, and running the 
      application.

Containers should only be scheduled together in a single Pod if they are 
tightly coupled and need to share resources such as disk.

Key Kubernetes commands (via kubectl):

You can use Kubectl command-line interface to get information about deployed 
applications and their environments. The most common operations can be done 
with the following kubectl commands:

    kubectl get - list resources
    kubectl describe - show detailed information about a resource
    kubectl logs - print the logs from a container in a pod
    kubectl exec - execute a command on a container in a pod

You can use these commands to see when applications were deployed, what their 
current statuses are, where they are running and what their configurations are.

Now that we know more about our cluster components and the command line, let's 
explore our application.

The Kubernetes node runs the services necessary to host application containers 
and be managed from the master systems :


Kubelet
=======

The most important and most prominent controller in Kubernetes is the Kubelet,
which is the primary implementer of the Pod and Node APIs that drive the 
container execution layer. Without these APIs, Kubernetes would just be a 
CRUD-oriented REST application framework backed by a key-value store (and 
perhaps the API machinery will eventually be spun out as an independent 
project).

Kubernetes executes isolated application containers as its default, native 
mode of execution, as opposed to processes and traditional operating-system 
packages. Not only are application containers isolated from each other, but 
they are also isolated from the hosts on which they execute, which is critical 
to decoupling management of individual applications from each other and from 
management of the underlying cluster physical/virtual infrastructure.

Kubernetes provides Pods that can host multiple containers and storage volumes 
as its fundamental execution primitive in order to facilitate packaging a 
single application per container, decoupling deployment-time concerns from 
build-time concerns, and migration from physical/virtual machines. The Pod 
primitive is key to glean the primary benefits of deployment on modern cloud 
platforms, such as Kubernetes.

API admission control may reject pods or add additional scheduling constraints 
to them, but Kubelet is the final arbiter of what pods can and cannot run on 
a given node, not the schedulers or DaemonSets.

Kubelet also currently links in the cAdvisor resource monitoring agent.


Container runtime
=================

Each node runs a container runtime, which is responsible for downloading 
images and running containers.

Kubelet does not link in the base container runtime. Instead, we're defining 
a Container Runtime Interface to control the underlying runtime and facilitate 
pluggability of that layer. This decoupling is needed in order to maintain 
clear component boundaries, facilitate testing, and facilitate pluggability. 
Runtimes supported today, either upstream or by forks, include at least docker 
(for Linux and Windows), rkt, cri-o, and frakti.


Kube Proxy
==========

The service abstraction provides a way to group pods under a common access 
policy (e.g., load-balanced). The implementation of this creates a virtual 
IP which clients can access and which is transparently proxied to the pods in 
a Service. Each node runs a kube-proxy process which programs iptables rules 
to trap access to service IPs and redirect them to the correct backends. This 
provides a highly-available load-balancing solution with low performance 
overhead by balancing client traffic from a node on that same node.

Service endpoints are found primarily via DNS.


Add-ons and other dependencies
=================================

A number of components, called add-ons typically run on Kubernetes itself:
    - DNS
    - Ingress controller
    - Heapster (resource monitoring)
    - Dashboard (GUI)


c) Pods
=======

When you deploy an application on a cluster, Kubernetes creates a Pod to host 
your application instance. A Pod is a Kubernetes abstraction that represents a 
group of one or more application containers, and some shared resources for 
those containers. Those resources include:

    - Shared storage, as Volumes
    - Networking, as a unique cluster IP address
    - Information about how to run each container, such as the container image 
      version or specific ports to use

A Pod models an application-specific "logical host" and can contain different 
application containers which are relatively tightly coupled. For example, a Pod 
might include both the container with your Node.js app as well as a different 
container that feeds the data to be published by the Node.js webserver. The 
containers in a Pod share an IP Address and port space, are always co-located 
and co-scheduled, and run in a shared context on the same Node.

Pods are the atomic unit on the Kubernetes platform. When we create a 
Deployment on Kubernetes, that Deployment creates Pods with containers inside 
them (as opposed to creating containers directly). Each Pod is tied to the Node 
where it is scheduled, and remains there until termination (according to 
restart policy) or deletion. In case of a Node failure, identical Pods are 
scheduled on other available Nodes in the cluster.



c) Labels and Selector
================================

Everything is about labels!!!

e) Service
==========



e) Deployment, StatefulSet, DaemonSet
=====================================










###############################################################################
#
# Learn Kubernetes Basics - Part 2 - Create a cluster
#
###############################################################################


Let's now refine a bit few concepts on the node, and get these concepts in 
action in setting up and deploying a cluster!


======================================
2.1 - Naming conventions & unique data
======================================


In this section, we will build step by step the infrastructure on which we 
will deploy a Kubernetes cluster. We intentionaly make every step manual, so 
that you can appreciate the full process at leat once in your lifetime. In 
real production world, most of these steps are automated and you would simply 
"push a button".

The steps are:
    - building an initial  VM image ("K8s BARE") with all the prerequisites 
      installed (Ubuntu OS, docker, kubernetes)
    - renaming a VM to be the master node ("K8s master - cluster not 
      deployed"), derived from "K8S BARE"
    - renaming two 'slave' VMs ("K8s slave - cluster not deployed") derived 
      from "K8s BARE"
    - initialize and deploy a Kubernetes on the master, and join the two 
      slaves into the cluster (all VMs are then called "cluster deployed")
    - setup a dashboard and access it

You may skip this step and use directly the "cluster deployed" images in order 
to skip this step and go directly to part 3.


###
###      WARNING
###
### Few informations will be unique to each deployement and you need to 
### carefully copy and save these informations in order to adapt the commands 
### as shown in the tutorial (i.e. replace the tutorial's info with yours) and 
### run your own version of the commands:
###
### 1) the ip@ of the VMs
###
### In the tutorial, I used (copied from a real execution):
###    master:  192.168.0.63
###    slave 1: 192.168.0.116
###    slave 2: 192.168.0.117
### The ip@ granted to each VM by the host machine will vary from execution of
### this tutorial to another, so please collect the ip@as you will start the 
### VMs and keep this information for the whole duration of the tutorial.
###
### 2) the token required for a node to join into the cluster
###
### In the following examples, I used:
###
### $ kubeadm join 192.168.0.63:6443 --token q3nz0g.n2l9ow787j7enj8j --discovery-token-ca-cert-hash sha256:191a8bc561c244591b428dd3a8b82f0b26d34afeed4115626c876c97a6839729
###
###


===========================
2.3 - Building the first VM
===========================

(you can skip this phase and use directly the "K8s BARE" image)

Nothing fancy here: we've built from the Ubuntu Server 18.04 LTS, with:
  - two CPUs (it is mandatory to run a Kubernetes master)
  - 2048 MB memory
  - two network interfaces:
       * one NAT (which is default setting) for internet connectivity
       * one configured in a "Bridge Adapter" mode, pointing on the most 
         relevant network adapter (in my case: the Ethernet card on the 
         desktop, or the wifi card on the laptop)

(image "K8s BARE - 0 - Description")
(image "K8s BARE - 1 - Memory")
(image "K8s BARE - 2 - CPU")
(image "K8s BARE - 3 - 1st Network interface")
(image "K8s BARE - 4 - 2nd Network interface")

In order to make this VM a generic basis on which we can run any type of k8s
nodes, we then need to log as root and (in this order):
  - to install docker
  - to disable the swap
  - to install kubernetes

For convenience, in this tutorial, we will configure the VMs by connecting 
into them via ssh from terminals running on the host machine, using the 
default admin profile (login=thierry, password=thierry) (Secure, huh!!!). 
There are few advantages doing so, namely the fact that the prompt then tells 
on which machine you are actually logged, which is very convenient when you 
will have 3 VMs running at the same time.

To do so, you need to identify the ip@ of the VM: it will be displayed on 
your first loging into the VM from the VM window:

(image "K8s BARE - 5 - Login from the VM")

Open a terminal and - as a regular user on the host machine, ssh into the VM:
(in my case, the hostname of my laptop is 'laptop' and my login is 'tso')

tso@laptop:~$ ssh thierry@192.168.0.111

The first time you connect on a remote machine, you're asked to confirm: 
confirm by typing 'yes' and then enter the password:

The authenticity of host '192.168.0.111 (192.168.0.111)' can't be established.
ECDSA key fingerprint is SHA256:blStegSimd9FZS74HYnmTW4CxvNY0gI2LDP7YCcbuzY.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '192.168.0.111' (ECDSA) to the list of known hosts.
thierry@192.168.0.111's password: 

You are now connected on the VM and your prompt will show it:

(K8s BARE - 6 - Login from a terminal)

thierry@k8s_node:~$

You are logged as 'thierry' (the name of the account on the VM) and not 
anymore as 'tso' (the name of the account on the host machine) and you are on 
'k8s_node'* and not on 'laptop' anymore.

*: the initial name of the machine is whatever you have specified when 
building the fist image from the Ubuntu OS in VirtualBox. I used the generic 
name 'k8s_node' because the same VM will later be used to configure both 
master and slave nodes.


We must do the following steps as 'root':

thierry@k8s_node:~$ sudo su

The prompt becomes 'root@k8s_node:/#' and we can continue the next steps.

We will install docker: collect the certificates, add the signing key to the 
local apt repository, add the docker repository... and finally install docker:

root@k8s_node:/# apt-get install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common
root@k8s_node:/# curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb https://download.docker.com/linux/debian stretch stable"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y docker-ce

Then we must disable the swap: we do so immediately with the "swappoff" 
command:

root@k8s_node:/# swapoff -a

and we also prevent the swap to be re-activated at next boot, by editing the 
fstab file (/etc/fstab) and disable the line indicating the swap partition 
by commenting it (i.e. add a '#' at the beginning of the line). Thus, the 
original file changes from:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
/swap.img	none	swap	sw	0	0

to:

UUID=96b5fd56-93ab-4512-a2da-c89f56a73da3 / ext4 defaults 0 0
#/swap.img	none	swap	sw	0	0


We will then install Kubernetes: collect the certificates, add the signing 
key to the local apt repository, add the google kubernetes repository... and 
finally install docker:

root@k8s_node:/# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
root@k8s_node:/# add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
root@k8s_node:/# apt-get update
root@k8s_node:/# apt-get install -y kubelet kubeadm kubectl


Here we are !!!
The generic VM is setup, and we can use this basis for crafting teh master 
and slave nodes. As a matter of precaution, you should log out, go in 
VirtualBox and shutdown the VM (softly), take a snapshot of the VM and export 
it as and appliance under the name of  "K8s BARE".


=========================================
2.4 - Building the master and slave nodes
=========================================

In this section, we will configure one master node and two slave nodes 
from "K8s BARE". To do so, we will clone K8s_BARE (which VirtualBox does very
easily).

In order to reduce the memory footprint of the tutorial, choose 'linked clone'
rather than 'full clone', since each VM virtual disk is more than 3.5GB (and
the laptop does not have infinite storage).

(K8s BARE - 7 - Cloning)

And you should get something looking like this:

(K8s BARE - 8 - Cloned machines)

Start the three VMs in VirtualBox, and as shown in the previous section, 
log in, identify the ip@, and connect on these VMs from terminals running on 
the host machine.

Since almost all prerequisites were already installed in the previous phase, 
the task here merely consists in renaming the machines so that a) you know on
which machine you are logged, and b) you prevent naming conflicts within the
kubernetes cluster later on (since every node must have a unique name in the 
cluster).

So we will edit two files: /etc/hosts and /etc/hostname

Log into the first VM (in our case 'ssh thierry@192.168.0.63) and edit the
file /etc/hostname: replace the previous host name (k8s-node) with the new
host name 'k8s-master'.

Then edit the file /etc/hosts and add the new name 'k8s-master after
'127.0.0.1   localhost'.

After edition, it should look like this:

thierry@k8s-master:~$ cat /etc/hostname 
k8s-master

thierry@k8s-master:~$ cat /etc/hosts
127.0.0.1 localhost k8s-master
127.0.1.1 k8s_node

Log out and log in again, and the prompt should look like:

thierry@k8s-master:~$ 

Then do so on the two other machines (respectively at 192.168.0.116 and 
192.168.0.117) with the new host names 'k8s-slave1' and 'k8s-slave2'. The 
prompt on these two machines should look like:

thierry@k8s-slave1:~$ 

and

thierry@k8s-slave2:~$ 

Interestingly, the prompt will always tell you on which machine and which 
account you are a given moment in time, which will be important in order not 
to do too many mistakes while we will configure the Kubernetes cluster.

That's it! We now have 3 machines running on the laptop, with all the 
prerequisites installed and the right configuration to make them a master 
and two slaves.

We can test the connectivity between the machines on the laptop (ping from 
one machine to the other) to check that there is no problem, and then go to
the next section.


=====================================
2.5 - Configure and start the cluster
=====================================

To start with, we will initiate the cluster on the master node.

SSH onto the master node and log as root. To initiate the cluster, we will use 
'kubeadm' with the 'init' command. We specify two arguments:
    - specify the 'internal cluster network' which will enable the nodes to 
      communicate one with the other. This internal network uses the second 
      network interface declared in VirtualBox (the one in bridge mode, and
      use a dedicated internal IP range (10.244.0.0-255 in this example).
    - specifiy that the master node must advertise its own IP@ in order to 
      enable the slave nodes to first connect to the master node.

root@k8s-master:/# kubeadm init --pod-network-cidr=10.244.0.0/16 \
      --apiserver-advertise-address=192.168.0.63

The whole setup can take a bit of time, and it shoudl conclude with displaying 
the following message (truncated):

[init] Using Kubernetes version: v1.17.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[...]
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.63:6443 --token x4xmhe.jgkyetw64skcjuzu \
    --discovery-token-ca-cert-hash sha256:93a7075d9d3c2e527ebd05b6bbd13c1b618a00d6585fd46b47d68c22acafa0a3 


Here you are: your Kubernetes master has initialized successfully!
You can see that the message displayed during the initialisation process 
includes three very important directives:
    - you need to create a configuration file to be able to operate the 
      cluster as a regulaer user (i.e. not root)
    - you need to actually deploy the network which will connect all the pods 
      together, and you will have to choose one of the possible network 
      configurations.
    - you are given the information needed for a slave node to join the 
      cluster by giving the right token and the public key of the master.

So, let's follow these directives:

The next step is to configure the 'regular' user (i.e. not root) on the 
master node who will manage the cluster. First we logout as 'root' but we 
stay logged as 'thierry' on the master, and as a 'thierry' we enter the 
commands given in the initialization message:

thierry@k8s-master:~$ mkdir -p $HOME/.kube
thierry@k8s-master:~$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
thierry@k8s-master:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Then we must deploy the network within the cluster. For the sake of 
simplicity, we will use the 'flannel' network configuration (not because it 
is better than others, but because I know it is simple to make it work).

thierry@k8s-master:~$ kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

Once you have received this message, the master will need a bit of time to 
process the deployment as it will actually spawn noew pods in charge of 
operating the network. You can see the progress by asking several times to 
show the pods running on the master with the command:
$ kubectl get pods --all-namespaces -o wide

See the results at several seconds distance, and observe that the pods status
evolves as their get from 'Pending' to 'ContainerCreating' and finally to 
'Running'.

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Pending   0          2m58s   <none>      <none>       <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          14s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          2m58s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m12s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS              RESTARTS   AGE     IP          NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     ContainerCreating   0          3m4s    <none>      k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running             0          20s     10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running             0          3m4s    10.0.2.15   k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running             0          3m18s   10.0.2.15   k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             0/1     Running   0          3m7s    10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             0/1     Running   0          3m7s    10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          23s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m7s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m21s   10.0.2.15    k8s-master   <none>           <none>
thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             1/1     Running   0          3m16s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             1/1     Running   0          3m16s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          32s     10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          3m16s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          3m30s   10.0.2.15    k8s-master   <none>           <none>


This is actually interesting as it reveals one key nature of Kubernetes: it
will enable you to run containerized applications, but it relies itself on 
launching containers (i.e. containerized applications) to do its job. As your
can see above, it launches a DNS application wich will serve to connect pods
within the cluster: this DNS application is itself running in a container, 
managed as a pod.

Looking at the list of the pods, you see that Kubernetes launched several 
applications: an API server, a proxy, a scheduler, a datastore (ETCD), two
DNS... and a controller manager. We will see abit later all these functions.

To check that the cluster is alive, you can run the version command:

thierry@k8s-master:~$ kubectl version
Client Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:20:10Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"17", GitVersion:"v1.17.0", GitCommit:"70132b0f130acc0bed193d9ba59dd186f0e634cf", GitTreeState:"clean", BuildDate:"2019-12-07T21:12:17Z", GoVersion:"go1.13.4", Compiler:"gc", Platform:"linux/amd64"}

and view the cluster details:

thierry@k8s-master:~$ kubectl cluster-info
Kubernetes master is running at https://192.168.0.63:6443
KubeDNS is running at https://192.168.0.63:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

Nevertheless, the master is still alone; you have not yet joined a slave into
the cluster. You can see all the nodes in the cluster with the command
'kubectl get nodes':

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   5m40s   v1.17.0

So it's time to join the two slaves into the cluster. To do so, we need to 
log into the slave nodes (with ssh), as 'root', and use the token and key 
which were displayed at the end of the cluster initialisation message:

root@k8s-slave1:/# kubeadm join 192.168.0.63:6443 --token fwt4d1.u6h91bmd5c3uko6e \
    --discovery-token-ca-cert-hash sha256:2e801168b30996a3f2a1dfb2b447a36b359f2522b2f36f9f9983468b4f278b94 

and then:

root@k8s-slave2:/# kubeadm join 192.168.0.63:6443 --token fwt4d1.u6h91bmd5c3uko6e \
    --discovery-token-ca-cert-hash sha256:2e801168b30996a3f2a1dfb2b447a36b359f2522b2f36f9f9983468b4f278b94 


Come back to the master node and check the status of the cluster. Check the 
number of nodes as they get enlisted and active into the cluster:

thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   5m40s   v1.17.0
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m46s   v1.17.0
k8s-slave1   NotReady   <none>   3s      v1.17.0
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m49s   v1.17.0
k8s-slave1   NotReady   <none>   6s      v1.17.0
k8s-slave2   NotReady   <none>   3s      v1.17.0
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS     ROLES    AGE     VERSION
k8s-master   Ready      master   6m53s   v1.17.0
k8s-slave1   NotReady   <none>   10s     v1.17.0
k8s-slave2   NotReady   <none>   7s      v1.17.0
thierry@k8s-master:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   7m25s   v1.17.0
k8s-slave1   Ready    <none>   42s     v1.17.0
k8s-slave2   Ready    <none>   39s     v1.17.0

Now, let's look at the pods running on the cluster: you can see that some 
pods are now running on the slave nodes as well:

thierry@k8s-master:~$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-pfglx             1/1     Running   0          7m47s   10.244.0.3   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-phjl4             1/1     Running   0          7m47s   10.244.0.2   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                      1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master            1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master   1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-4kg8s          1/1     Running   0          80s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-flannel-ds-amd64-5zvqf          1/1     Running   0          5m3s    10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-flannel-ds-amd64-ljggs          1/1     Running   0          84s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-8c7rf                     1/1     Running   0          84s     10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-proxy-9454v                     1/1     Running   0          80s     10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-w2srw                     1/1     Running   0          7m47s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-scheduler-k8s-master            1/1     Running   0          8m1s    10.0.2.15    k8s-master   <none>           <none>


YES!!! The cluster is up and running and the master is actually scheduling 
pods on the slave nodes. At this moment, the pods are only Kubernetes' 
components, but this is already showing the principles by which Kubernetes
will schedule your own application on the pods.

Now we still to enable the host machine to actually manage the Kubernetes
cluster, and in order to do so, we must get back to the host and create the 
same .kube directory with the same config file (using the same commands as the
ones we used to create the directory and file on the master node):

tso@laptop:~$ mkdir -p $HOME/.kube
tso@laptop:~$ cd .kube
tso@laptop:~/.kube$ touch config

Here you must copy the content of the config file on the master and paste it
without changing anythin in the config file on your host machine. Once it is
done, continue:

tso@laptop:~/.kube$ cd ..
tso@laptop:~$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

Here you: you now can manage the Kubernetes cluster from the host machine:

tso@laptop:~$ kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   16m     v1.17.0
k8s-slave1   Ready    <none>   10m     v1.17.0
k8s-slave2   Ready    <none>   9m57s   v1.17.0
tso@laptop:~$ kubectl get services -o wide
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   16m   <none>


=========================
2.6 - Setup the dashboard   ---  NOT COMPLETED ---
=========================


FROM:https://github.com/kubernetes/dashboard/blob/master/README.md

tso@laptop:~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml
secret/kubernetes-dashboard-certs created
serviceaccount/kubernetes-dashboard created
role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created
deployment.apps/kubernetes-dashboard created
service/kubernetes-dashboard created

tso@laptop:~$ kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   8m25s

tso@laptop:~$ kubectl get pods -A -o wide
NAMESPACE     NAME                                    READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES
kube-system   coredns-6955765f44-lzwgk                1/1     Running   0          8m10s   10.244.0.2   k8s-master   <none>           <none>
kube-system   coredns-6955765f44-wxqnn                1/1     Running   0          8m10s   10.244.0.3   k8s-master   <none>           <none>
kube-system   etcd-k8s-master                         1/1     Running   0          8m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-apiserver-k8s-master               1/1     Running   0          8m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-controller-manager-k8s-master      1/1     Running   0          8m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-bq9mz                        1/1     Running   0          8m10s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kube-proxy-rl4lh                        1/1     Running   0          3m47s   10.0.2.15    k8s-slave2   <none>           <none>
kube-system   kube-proxy-zstqk                        1/1     Running   0          3m57s   10.0.2.15    k8s-slave1   <none>           <none>
kube-system   kube-scheduler-k8s-master               1/1     Running   0          8m22s   10.0.2.15    k8s-master   <none>           <none>
kube-system   kubernetes-dashboard-7c54d59f66-h6fk5   1/1     Running   0          59s     10.244.1.2   k8s-slave1   <none>           <none>

On a different terminal:

tso@laptop:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Access the dashboard on the following URL on the browser of the host machine:
http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/

=> Error trying to reach service: 'dial tcp 10.244.1.2:8443: i/o timeout'











FROM: https://stackoverflow.com/questions/59006501/how-can-i-remotely-access-kubernetes-dashboard-with-token


tso@laptop:~$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created


Start the proxy on a different terminal (or with &):

tso@laptop:~$ kubectl proxy&
Starting to serve on 127.0.0.1:8001

Create a secret:

tso@laptop:~$ kubectl create serviceaccount tso-admin-dashboard
serviceaccount/tso-admin-dashboard created

tso@laptop:~$ kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=default:tso-admin-dashboard
clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin created

tso@laptop:~$ kubectl get secret
NAME                              TYPE                                  DATA   AGE
default-token-mgdlc               kubernetes.io/service-account-token   3      80m
tso-admin-dashboard-token-tpg8g   kubernetes.io/service-account-token   3      70s

tso@laptop:~$ kubectl describe secret tso-admin-dashboard
Name:         tso-admin-dashboard-token-tpg8g
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: tso-admin-dashboard
              kubernetes.io/service-account.uid: bc98e5ad-a40c-4f55-a0cf-5d01d658bbde

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjRubHJqM29NQjFRUDlDM1N2Ykhoa04xU3JmQlVxS1pOV2RtQ1EzcUZyS0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6InRzby1hZG1pbi1kYXNoYm9hcmQtdG9rZW4tdHBnOGciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoidHNvLWFkbWluLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImJjOThlNWFkLWE0MGMtNGY1NS1hMGNmLTVkMDFkNjU4YmJkZSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OnRzby1hZG1pbi1kYXNoYm9hcmQifQ.hsx0v4MfkEkAE7GEc1ry3DKuyvT1LEASLaSgbfnISELLxAVxG_9IzJWcOzErgAkff-w4KsinaRONSzdNFxr78IDhVWS-kmRiVVCsQ6cdAZ00u0dpRoQn6l8tFbnAAIA79NFTeBtiSmdOcRyoN8bV-XkwqIsK3tqRQp0SebwxT4SyV2OKiLeGmq_quAML_KfixriMR1tfT_7oasIzezvDk9KepGil0H1xk6GYzKwENvfdsXqyou5Gft50QvQcoBXzlEt9-ga-iJYBKqrmuIfIWgK6gQir08LEkNENPe_z4isit4diMTLMmY9v26qwngDrgP_tQfT-gQuQLxyN8VlKjA

Then establish a tunnel from the host to the master:

tso@laptop:~$ ssh -L 9999:127.0.0.1:8001 -N -f -l thierry 192.168.0.63
thierry@192.168.0.63's password: 

Once the tunnel is up and running, you should be able to view the dashboard 
from the browser on hour host machine at the following url:










































The cluster is running but it is very difficult to see what is actually 
happening, so it is time to setup the dashboad which visualizes the status 
of the cluster.

Start with login into the master and launch the installation command for the 
dashboard:




root@k8s-master:/# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/head.yaml
namespace/kubernetes-dashboard-head created
serviceaccount/kubernetes-dashboard-head created
service/kubernetes-dashboard-head created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard-head created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-head created
deployment.apps/kubernetes-dashboard-head created
service/dashboard-metrics-scraper-head created
deployment.apps/dashboard-metrics-scraper-head created

As you can see, Kubernetes pulls the resources from GitHub and starts pods 
to run the dashboard application (which is a web server). Check where the 
pods are running:

tso@laptop:~$ kubectl get pods --all-namespaces -o wide | grep dashb
kubernetes-dashboard-head   dashboard-metrics-scraper-head-7cc7d9bb4b-9cps7   1/1     Running   0          91s   10.244.1.2   k8s-slave1   <none>           <none>
kubernetes-dashboard-head   kubernetes-dashboard-head-5c87564c95-6qnkz        1/1     Running   0          91s   10.244.2.2   k8s-slave2   <none>           <none>

The workload is distributed over the two slave nodes. In order to access this
dasboard, we now need to creae users with teh right roles and authorisations
(i.e. profiles with the rights in RBAC mode)

Let's now check the services which are running on the cluster:

tso@laptop:~$ kubectl get services -A -o wide
NAMESPACE                   NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE   SELECTOR
default                     kubernetes                       ClusterIP   10.96.0.1       <none>        443/TCP                  24m   <none>
kube-system                 kube-dns                         ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   24m   k8s-app=kube-dns
kubernetes-dashboard-head   dashboard-metrics-scraper-head   ClusterIP   10.96.52.67     <none>        8000/TCP                 56s   k8s-app=dashboard-metrics-scraper-head
kubernetes-dashboard-head   kubernetes-dashboard-head        ClusterIP   10.96.220.146   <none>        443/TCP                  56s   k8s-app=kubernetes-dashboard-head

You now need to create an admin account for the dashboard and the 
corresponding role:

Crate a 'admin-user.yaml' file with the following content:

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Then create a 'admin-role.yaml' file with the following content:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

And finally load this account and the role into the cluster:

tso@laptop:~$ kubectl apply -f admin-user.yaml
serviceaccount/admin-user created
tso@laptop:~$ kubectl apply -f admin-role.yaml
clusterrolebinding.rbac.authorization.k8s.io/admin-user created

You now want to enable the host machine to connect to the dashboad: in order 
to do so, you first need to enable the host to 'see' the dashboard service: we
will achieve this by asking Kubernetes to proxy the dashboard service back to 
the host. Since the corresponding command will mobilise the terminal, execute 
it in aonther terminal window:

tso@laptop:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Since recent versions, the access to the dashboard has been more secured, so we need to get some secrets:







thierry@k8s-master:~/.kube$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-tnh22
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: ab769855-b29a-4cce-8271-edfca5986080

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjRubHJqM29NQjFRUDlDM1N2Ykhoa04xU3JmQlVxS1pOV2RtQ1EzcUZyS0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLXRuaDIyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhYjc2OTg1NS1iMjlhLTRjY2UtODI3MS1lZGZjYTU5ODYwODAiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.Ew1jWNMnuw7jHTwsjsa6pWjW9oKc7vBHXj4ZFvrdVJyZZ_dtTh1HL0QIS9rtY7vEgUs2cIDWWcmF5ZtEWqHr63i8w-T3Ahd1SDdVKhNrt6ujzLsRhJZXlFDu6zpKMtc4QCx_7Q5a4mTiPqKnt1bP20rEWftBHRfbH74gYAzSVHILp7t3Ft_H8Jw5HsPoiM5rgWohOGWZB9mUkC5TWs3zdO8TFR3oYhbiUmmnivcfRVA4oetzhIl6azIkmmjlTUuSXCp9NhzGCVMctAkVHBUNqvacRSQ-Xjs1FiFGJZOXM2rrOyvk_o6AR_lxbHanZ6I1J6_7DXxnh66t7gxZ6ZGm-A








tso@laptop:~$ kubectl -n kube-system get secret
NAME                                             TYPE                                  DATA   AGE
admin-user-token-tnh22                           kubernetes.io/service-account-token   3      17m
attachdetach-controller-token-crb7t              kubernetes.io/service-account-token   3      46m
bootstrap-signer-token-sz769                     kubernetes.io/service-account-token   3      46m
bootstrap-token-fwt4d1                           bootstrap.kubernetes.io/token         7      46m
certificate-controller-token-7tf5m               kubernetes.io/service-account-token   3      46m
clusterrole-aggregation-controller-token-2qwgz   kubernetes.io/service-account-token   3      46m
coredns-token-mrrdk                              kubernetes.io/service-account-token   3      46m
cronjob-controller-token-8h6ts                   kubernetes.io/service-account-token   3      46m
daemon-set-controller-token-rvv6w                kubernetes.io/service-account-token   3      46m
default-token-f6vz6                              kubernetes.io/service-account-token   3      46m
deployment-controller-token-bmsrc                kubernetes.io/service-account-token   3      46m
disruption-controller-token-b5w9w                kubernetes.io/service-account-token   3      46m
endpoint-controller-token-fs7xj                  kubernetes.io/service-account-token   3      46m
expand-controller-token-rpqlf                    kubernetes.io/service-account-token   3      46m
flannel-token-5c6hf                              kubernetes.io/service-account-token   3      43m
generic-garbage-collector-token-cwnkm            kubernetes.io/service-account-token   3      46m
horizontal-pod-autoscaler-token-hjpvg            kubernetes.io/service-account-token   3      46m
job-controller-token-m59nt                       kubernetes.io/service-account-token   3      46m
kube-proxy-token-d5c6d                           kubernetes.io/service-account-token   3      46m
namespace-controller-token-9hpbf                 kubernetes.io/service-account-token   3      46m
node-controller-token-g5snf                      kubernetes.io/service-account-token   3      46m
persistent-volume-binder-token-dkkbt             kubernetes.io/service-account-token   3      46m
pod-garbage-collector-token-vzfcg                kubernetes.io/service-account-token   3      46m
pv-protection-controller-token-8jghj             kubernetes.io/service-account-token   3      46m
pvc-protection-controller-token-9c8pn            kubernetes.io/service-account-token   3      46m
replicaset-controller-token-rtbzk                kubernetes.io/service-account-token   3      46m
replication-controller-token-gww9h               kubernetes.io/service-account-token   3      46m
resourcequota-controller-token-sncjn             kubernetes.io/service-account-token   3      46m
service-account-controller-token-8pbfm           kubernetes.io/service-account-token   3      46m
service-controller-token-dnxxt                   kubernetes.io/service-account-token   3      46m
statefulset-controller-token-p6tcc               kubernetes.io/service-account-token   3      46m
token-cleaner-token-s9ddr                        kubernetes.io/service-account-token   3      46m
ttl-controller-token-tpgtb                       kubernetes.io/service-account-token   3      46m

tso@laptop:~$ kubectl -n kube-system describe secret deployment-controller-token-bmsrc
Name:         deployment-controller-token-bmsrc
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: deployment-controller
              kubernetes.io/service-account.uid: caafb505-91ab-4d95-922c-51da30181d7d

Type:  kubernetes.io/service-account-token

Data
====
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjRubHJqM29NQjFRUDlDM1N2Ykhoa04xU3JmQlVxS1pOV2RtQ1EzcUZyS0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tYm1zcmMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiY2FhZmI1MDUtOTFhYi00ZDk1LTkyMmMtNTFkYTMwMTgxZDdkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.ePmuSfi582vSGgE1DUwqp5CacDMLXTNF7MJyqNyJWClTCBKRovMRsBR99SxoYzM1KXk2Doka5cn03ZSfdTXIdQJLUaFOf85-NlGoKA9YcDvq8wFSzb_debqpnuUh75cRPzxKgbi4agM2u3mP6_ZwP4RbUgKe0wlm1oQikVPvfpLjGZea0x6LEsoAJlGzlleclhno02d78AqHql_MqdLAMwfpgNu2iwrVt_domTgaBWwAp5YETe1wqP446QGvNHcQhtcK6wJ3cMMGEy7oP36yB2eayzbS7P4jQUrv9NuK-F9BfhfpyBOAJVw1d7RbUqX54FdHgm1ojvO1HhSRVe7Fxw
ca.crt:     1025 bytes


ALTERNATIVE : get the token in a single command:

tso@laptop:~$ kubectl -n kube-system describe secret $(
  kubectl -n kube-system get secret | \
  awk '/^deployment-controller-token-/{print $1}'
) | \
awk '$1=="token:"{print $2}'
eyJhbGciOiJSUzI1NiIsImtpZCI6IjRubHJqM29NQjFRUDlDM1N2Ykhoa04xU3JmQlVxS1pOV2RtQ1EzcUZyS0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZXBsb3ltZW50LWNvbnRyb2xsZXItdG9rZW4tYm1zcmMiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVwbG95bWVudC1jb250cm9sbGVyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiY2FhZmI1MDUtOTFhYi00ZDk1LTkyMmMtNTFkYTMwMTgxZDdkIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmUtc3lzdGVtOmRlcGxveW1lbnQtY29udHJvbGxlciJ9.ePmuSfi582vSGgE1DUwqp5CacDMLXTNF7MJyqNyJWClTCBKRovMRsBR99SxoYzM1KXk2Doka5cn03ZSfdTXIdQJLUaFOf85-NlGoKA9YcDvq8wFSzb_debqpnuUh75cRPzxKgbi4agM2u3mP6_ZwP4RbUgKe0wlm1oQikVPvfpLjGZea0x6LEsoAJlGzlleclhno02d78AqHql_MqdLAMwfpgNu2iwrVt_domTgaBWwAp5YETe1wqP446QGvNHcQhtcK6wJ3cMMGEy7oP36yB2eayzbS7P4jQUrv9NuK-F9BfhfpyBOAJVw1d7RbUqX54FdHgm1ojvO1HhSRVe7Fxw

We shall use this token when we want to log into the dashboard: you now can 
access the dashboard from your browser (running on the host) at this url:
http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
Since the welcome page requets a login: choose the 'Token' option and paste 
the token above into the field.






Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :


Name:         admin-user-token-g4h4m
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: 2124843b-2815-47f5-a558-cb5956bd61d7

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IkUyTVprdjJuMGJ1NkVyeFlweFVHd2RtWHR2ZHFDS1JFb21SMDNQNEY3UlUifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWc0aDRtIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyMTI0ODQzYi0yODE1LTQ3ZjUtYTU1OC1jYjU5NTZiZDYxZDciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.JmnyHQCNRqT90GhMub82keXrhJGi5cOVrKdPoM1Psw6s7qDBRpOobthUVw1KJ5abnFgTnEhVmRW52HcF4dCg2aPcKa5PcFidXtXVdyFXHM2-VUVYDw-97yeVKm9degPES11rs5bcnGGX0Fp7TAN38IhwmDDVcmcR4S7EftnOk1_RkG77Fuauu4mDYDO6kzm9ySrdMP_eF2XsoZr_1JLGucGmMDKaAryRAHPTl11t-i45mE_6NtxGdYo1eukPqKpvKF7-fQz-0Bbg3Pzavoko-nrv1TPZECPAj_cXZk3wptoYzg-Ye_h_dua0ue6CUanE1SY60u0kDmPK8aXGJfZUaw


Copy the token (yeah, I know, its long...)


Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 thierry@192.168.0.63

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://192.168.0.108:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all 

=> ça ne marche pas: ce qui s'affiche est un record de failure.

    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
=============================================================================
=============================================================================
NOTES FOR COMPLETING THE DASHBOARD SECTION
=============================================================================
=============================================================================

 
Installing the dashboard:


To install the Dashboard, run the following command from the master:

$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml

Kubernetes va aller chercher la configuration nécessaire la mise à en place du Dashboard directement depuis son dépôt Github et le faire apparaître dans la liste des pods de votre cluster.

$  kubectl get pods --all-namespaces -o wide | grep dashb
kube-system   kubernetes-dashboard-77fd78f978-f8p9l   1/1     Running   0          60s     10.244.1.230   k8snode1    <none>

Il est  "Running", ça veut dire qu'il est disponible, mais pas encore accessible.
Créez un compte utilisateur
Créez un fichier admin-user.yaml avec ce qui suit dedans :

apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system

Puis créez le rôle qui lui sera attaché : admin-role.yaml

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system

Chargez ces configurations dans le cluster :

kubectl apply -f admin-user.yaml
kubectl apply -f admin-role.yaml

Récupérer le token de connexion
Pour vous connecter au Dashboard, en plus d'avoir le pod et un utilisateur, il vous faut le token qui va bien. Pour le récupérer :

dada@k8smaster:~$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
Name:         admin-user-token-b8qmq
Namespace:    kube-system
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: admin-user
              kubernetes.io/service-account.uid: a8a600b1-e010-11e8-96ec-0800273c4560

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJl.........

J'ai tronqué l'affichage du token. Il est d'une longueur dingue. Copiez-le dans un coin maintenant que vous l'avez.
Accéder au Dashboard
Le Dashboard n'est par défaut pas accessible en dehors du cluster. On peut cependant tricher en passant par un proxy et un tunnel SSH.
Le proxy
Ouvrez un nouveau terminal branché sur votre master et tapez la commande suivante :

dada@k8smaster:~$ kubectl proxy
Starting to serve on 127.0.0.1:8001

Le tunnel SSH
Depuis votre PC hôte, lancez le tunnel :

dada@dada-laptop:~$ ssh -L 8001:127.0.0.1:8001 dada@IP_DU_MASTER

Affichez le fameux tant attendu
Si tout s'est bien passé jusqu'ici, vous deviez pouvoir accéder au Dashboard via cette url :

http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/overview?namespace=_all

Et voir ceci : 


Vous voici avec une belle interface pour admirer le comportement de votre cluster k8s. Foncez cliquer absolument partout et chercher le pourquoi du comment de telles options à tel endroit !





###############################################################################
#
# Learn Kubernetes Basics - Part 3 - Deploy an app
#
###############################################################################



Once you have a running Kubernetes cluster, you can deploy your containerized 
applications on top of it. To do so, you create a Kubernetes Deployment 
configuration. The Deployment instructs Kubernetes how to create and update 
instances of your application. Once you've created a Deployment, the 
Kubernetes master schedules mentioned application instances onto individual 
Nodes in the cluster.

Once the application instances are created, a Kubernetes Deployment Controller 
continuously monitors those instances. If the Node hosting an instance goes 
down or is deleted, the Deployment controller replaces the instance with an 
instance on another Node in the cluster. This provides a self-healing 
mechanism to address machine failure or maintenance.

In a pre-orchestration world, installation scripts would often be used to 
start applications, but they did not allow recovery from machine failure. By 
both creating your application instances and keeping them running across 
Nodes, Kubernetes Deployments provide a fundamentally different approach to 
application management.


============================================
3.1 - Deploying your first app on Kubernetes
============================================


Kubernetes Deployments
======================

Once you have a running Kubernetes cluster, you can deploy your containerized 
applications on top of it. To do so, you create what is called a "Kubernetes 
Deployment". The Deployment maerializes through a text file which defines the 
target state of your application (which application - i.e. which Docker 
container or which set of containers which compose your application -, on how 
many nodes - in order to bring resilience - or other criterias your application 
should respect once it is actually deployed on the cluster). This text file 
instructs Kubernetes how to create and update instances of your application 
(actually, it tells the Kubernetes Controller Manager to spawn a Deployment 
Controller which will read this text file: the Deployment Controller will tell 
the Kubernetes master how to 'schedules' mentioned application instances onto 
individual Nodes in the cluster, each node carrying the containers composing 
your application).

Once the application instances are created, the Kubernetes Deployment 
Controller continuously monitors those instances. If the Node hosting an 
instance goes down or is deleted, the Deployment controller replaces the 
instance with an instance on another Node in the cluster. This provides a 
self-healing mechanism to address machine failure or maintenance: since the 
text file describes the desired target state of the deployment, the Deployment 
controller actually detects that there is a deviation of the real deployment 
vs. the described target, and it reacts by deploying more pods to available 
nodes.

In a pre-orchestration world, installation scripts would often be used to start 
applications, but they did not allow recovery from machine failure. By both 
creating your application instances and keeping them running across Nodes, 
Kubernetes Deployments provide a fundamentally different approach to 
application management.


Deploying your first app on Kubernetes
======================================

You can create and manage a Deployment by using the Kubernetes command line 
interface, Kubectl. Kubectl uses the Kubernetes API to interact with the 
cluster. In this module, you'll learn the most common Kubectl commands needed 
to create Deployments that run your applications on a Kubernetes cluster.

When you create a Deployment, you'll need to specify the container image for 
your application and the number of replicas that you want to run. You can 
change that information later by updating your Deployment; Modules 5 and 6 of 
this tutorial discuss how you can scale and update your Deployments.

Applications need to be packaged into one of the supported container formats in 
order to be deployed on Kubernetes: here we will sue Docker.

For your first Deployment, you'll use a Node.js application packaged in a 
Docker container. 

===============================================================================
===============================================================================
TSO : TO BE CORRECTED 
(If you didn't already try creating a Node.js application and 
deploying it using a container, you can do that first by following the 
instructions from the Hello Minikube tutorial).
===============================================================================
===============================================================================

Let’s deploy our first app on Kubernetes with the 'kubectl create deployment'
command. We need to provide the deployment name and app image location 
(include the full repository url for images hosted outside Docker hub).

$ kubectl create deployment kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1

Great! You just deployed your first application by creating a deployment. This 
performed a few things for you:

    - searched for a suitable node where an instance of the application could 
      be run (we have only 1 available node)
    - scheduled the application to run on that Node
    - configured the cluster to reschedule the instance on a new Node when 
      needed

To list your deployments use the get deployments command:

$ kubectl get deployments

We see that there is 1 deployment running a single instance of your app. The 
instance is running inside a Docker container on your node.



(file: load-balancer-example.yaml)
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/name: load-balancer-example
  name: hello-world
spec:
  replicas: 5
  selector:
    matchLabels:
      app.kubernetes.io/name: load-balancer-example
  template:
    metadata:
      labels:
        app.kubernetes.io/name: load-balancer-example
    spec:
      containers:
      - image: gcr.io/google-samples/node-hello:1.0
        name: hello-world
        ports:
        - containerPort: 8080


$ kubectl apply -f ./yaml_config/load-balancer-example.yaml
(ou encore: $ kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml)

The preceding command creates a Deployment object and an associated ReplicaSet 
object. The ReplicaSet has five Pods, each of which runs the Hello World 
application.

Display information about the Deployment:

$ kubectl get deployments hello-world
$ kubectl describe deployments hello-world

Display information about your ReplicaSet objects:

$ kubectl get replicasets
$ kubectl describe replicasets

Create a Service object that exposes the deployment:

$ kubectl expose deployment hello-world --type=LoadBalancer --name=my-service

Display information about the Service:

$ kubectl get services my-service
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)    AGE
my-service   LoadBalancer   10.3.245.137   104.198.205.71   8080/TCP   54s

    Note: If the external IP address is shown as <pending>, wait for a minute and 
        enter the same command again.

Display detailed information about the Service:

$ kubectl describe services my-service
Name:           my-service
Namespace:      default
Labels:         app.kubernetes.io/name=load-balancer-example
Annotations:    <none>
Selector:       app.kubernetes.io/name=load-balancer-example
Type:           LoadBalancer
IP:             10.3.245.137
LoadBalancer Ingress:   104.198.205.71
Port:           <unset> 8080/TCP
NodePort:       <unset> 32377/TCP
Endpoints:      10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more...
Session Affinity:   None
Events:         <none>

Make a note of the external IP address (LoadBalancer Ingress) exposed by your 
service. In this example, the external IP address is 104.198.205.71. Also note 
the value of Port and NodePort. In this example, the Port is 8080 and the 
NodePort is 32377.

In the preceding output, you can see that the service has several endpoints: 
10.0.0.6:8080,10.0.1.6:8080,10.0.1.7:8080 + 2 more. These are internal 
addresses of the pods that are running the Hello World application. To verify 
these are pod addresses, enter this command:

$ kubectl get pods --output=wide
NAME                         ...  IP         NODE
hello-world-2895499144-1jaz9 ...  10.0.1.6   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-2e5uh ...  10.0.1.8   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-9m4h1 ...  10.0.0.6   gke-cluster-1-default-pool-e0b8d269-5v7a
hello-world-2895499144-o4z13 ...  10.0.1.7   gke-cluster-1-default-pool-e0b8d269-1afc
hello-world-2895499144-segjf ...  10.0.2.5   gke-cluster-1-default-pool-e0b8d269-cpuc

Use the external IP address (LoadBalancer Ingress) to access the Hello World 
application:

$ curl http://<external-ip>:<port>
    where <external-ip> is the external IP address (LoadBalancer Ingress) of 
    your Service, and <port> is the value of Port in your Service description. 
(sur l'exemple préscedent: $ curl http://104.198.205.71:8080)

The response to a successful request is a hello message:

> Hello Kubernetes!

Cleaning up
===========

To delete the Service, enter this command:

$ kubectl delete services my-service

To delete the Deployment, the ReplicaSet, and the Pods that are running the 
Hello World application, enter this command:

$ kubectl delete deployment hello-world

==============================================================================
==============================================================================
==============================================================================
==============================================================================
==============================================================================


ALTERNATIVE PLUS INTERESSANTE - REDIS DISTRIBUE SUR LE CLUSTER


Example: Deploying PHP Guestbook application with Redis

This tutorial shows you how to build and deploy a simple, multi-tier web 
application using Kubernetes and Docker. This example consists of the 
following components:

    - A single-instance Redis master to store guestbook entries
    - Multiple replicated Redis instances to serve reads
    - Multiple web frontend instances

x.1 - Objectives
================

    - Start up a Redis master.
    - Start up Redis slaves.
    - Start up the guestbook frontend.
    - Expose and view the Frontend Service.
    - Clean up.


x.2 - Start up the Redis Master
===============================

The guestbook application uses Redis to store its data. It writes its data to 
a Redis master instance and reads data from multiple Redis slave instances.

a. Creating the Redis Master Deployment
=======================================

The manifest file, included below, specifies a Deployment controller that runs 
a single replica Redis master Pod.

(file: application/guestbook/redis-master-deployment.yaml)

apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  replicas: 1
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: k8s.gcr.io/redis:e2e  # or just image: redis
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Launch a terminal window in the directory you downloaded the manifest files.

Apply the Redis Master Deployment from the redis-master-deployment.yaml file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml

Query the list of Pods to verify that the Redis Master Pod is running:

$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
redis-master-1068406935-3lswp   1/1       Running   0          28s

Run the following command to view the logs from the Redis Master Pod:

$ kubectl logs -f POD-NAME

    Note: Replace POD-NAME with the name of your Pod.
    Here: $ kubectl logs -f redis-master-1068406935-3lswp


b. Creating the Redis Master Service
====================================

The guestbook applications needs to communicate to the Redis master to write 
its data. You need to apply a Service to proxy the traffic to the Redis master 
Pod. A Service defines a policy to access the Pods.

(file: application/guestbook/redis-master-service.yaml)

apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend


Apply the Redis Master Service from the following redis-master-service.yaml 
file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml

Query the list of Services to verify that the Redis Master Service is running:

$ kubectl get service
NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.0.0.1     <none>        443/TCP    1m
redis-master   ClusterIP   10.0.0.151   <none>        6379/TCP   8s

    Note: This manifest file creates a Service named redis-master with a set 
        of labels that match the labels previously defined, so the Service 
        routes network traffic to the Redis master Pod.


x.3 - Start up the Redis Slaves
===============================

Although the Redis master is a single pod, you can make it highly available to 
meet traffic demands by adding replica Redis slaves.

a. Creating the Redis Slave Deployment
======================================

Deployments scale based off of the configurations set in the manifest file. In 
this case, the Deployment object specifies two replicas.

If there are not any replicas running, this Deployment would start the two 
replicas on your container cluster. Conversely, if there are more than two 
replicas are running, it would scale down until two replicas are running.

(file: application/guestbook/redis-slave-deployment.yaml)

apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: redis-slave
  labels:
    app: redis
spec:
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  replicas: 2
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google_samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # Using `GET_HOSTS_FROM=dns` requires your cluster to
          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in
          # service launched automatically. However, if the cluster you are using
          # does not have a built-in DNS service, you can instead
          # access an environment variable to find the master
          # service's host. To do so, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apply the Redis Slave Deployment from the redis-slave-deployment.yaml file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml

Query the list of Pods to verify that the Redis Slave Pods are running:

$ kubectl get pods
NAME                            READY     STATUS              RESTARTS   AGE
redis-master-1068406935-3lswp   1/1       Running             0          1m
redis-slave-2005841000-fpvqc    0/1       ContainerCreating   0          6s
redis-slave-2005841000-phfv9    0/1       ContainerCreating   0          6s


b. Creating the Redis Slave Service
===================================

The guestbook application needs to communicate to Redis slaves to read data. 
To make the Redis slaves discoverable, you need to set up a Service. A Service 
provides transparent load balancing to a set of Pods.

(file: application/guestbook/redis-slave-service.yaml)

apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apply the Redis Slave Service from the following redis-slave-service.yaml 
file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml

Query the list of Services to verify that the Redis slave service is running:

$ kubectl get services
NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
kubernetes     ClusterIP   10.0.0.1     <none>        443/TCP    2m
redis-master   ClusterIP   10.0.0.151   <none>        6379/TCP   1m
redis-slave    ClusterIP   10.0.0.223   <none>        6379/TCP   6s


x.4 - Set up and Expose the Guestbook Frontend
==============================================

The guestbook application has a web frontend serving the HTTP requests written 
in PHP. It is configured to connect to the redis-master Service for write 
requests and the redis-slave service for Read requests.

a. Creating the Guestbook Frontend Deployment
=============================================

(file: application/guestbook/frontend-deployment.yaml)

apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: frontend
  labels:
    app: guestbook
spec:
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  replicas: 3
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v4
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # Using `GET_HOSTS_FROM=dns` requires your cluster to
          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in
          # service launched automatically. However, if the cluster you are using
          # does not have a built-in DNS service, you can instead
          # access an environment variable to find the master
          # service's host. To do so, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 80

Apply the frontend Deployment from the frontend-deployment.yaml file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml

Query the list of Pods to verify that the three frontend replicas are running:

$ kubectl get pods -l app=guestbook -l tier=frontend
NAME                        READY     STATUS    RESTARTS   AGE
frontend-3823415956-dsvc5   1/1       Running   0          54s
frontend-3823415956-k22zn   1/1       Running   0          54s
frontend-3823415956-w9gbt   1/1       Running   0          54s


b. Creating the Frontend Service
================================

The redis-slave and redis-master Services you applied are only accessible 
within the container cluster because the default type for a Service is 
ClusterIP. ClusterIP provides a single IP address for the set of Pods the 
Service is pointing to. This IP address is accessible only within the cluster.

If you want guests to be able to access your guestbook, you must configure the 
frontend Service to be externally visible, so a client can request the Service 
from outside the container cluster. We will expose Services through NodePort.

    Note: Some cloud providers, like Google Compute Engine or Google 
        Kubernetes Engine, support external load balancers. If your cloud 
        provider supports load balancers and you want to use it, simply delete 
        or comment out type: NodePort, and uncomment type: LoadBalancer.

(file: application/guestbook/frontend-service.yaml)

apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # comment or delete the following line if you want to use a LoadBalancer
  type: NodePort 
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apply the frontend Service from the frontend-service.yaml file:

$ kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml

Query the list of Services to verify that the frontend Service is running:

$ kubectl get services
NAME           TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.0.0.112   <none>        80:31323/TCP   6s
kubernetes     ClusterIP   10.0.0.1     <none>        443/TCP        4m
redis-master   ClusterIP   10.0.0.151   <none>        6379/TCP       2m
redis-slave    ClusterIP   10.0.0.223   <none>        6379/TCP       1m


c. Viewing the Frontend Service via NodePort
============================================

If you deployed this application to a local cluster, you need to find the IP 
address to view your Guestbook. Run the following command to get the IP 
address for the frontend Service.

$ minikube service frontend --url
http://192.168.99.100:31323

Copy the IP address, and load the page in your browser to view your guestbook.


d. Viewing the Frontend Service via LoadBalancer
================================================

If you deployed the frontend-service.yaml manifest with type: LoadBalancer you 
need to find the IP address to view your Guestbook. Run the following command 
to get the IP address for the frontend Service.

$ kubectl get service frontend
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP        PORT(S)        AGE
frontend   ClusterIP   10.51.242.136   109.197.92.229     80:32372/TCP   1m

Copy the external IP address, and load the page in your browser to view your 
guestbook.


x.5 - Scale the Web Frontend
============================

Scaling up or down is easy because your servers are defined as a Service that 
uses a Deployment controller.

Run the following command to scale up the number of frontend Pods:

$ kubectl scale deployment frontend --replicas=5

Query the list of Pods to verify the number of frontend Pods running:

$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
frontend-3823415956-70qj5       1/1       Running   0          5s
frontend-3823415956-dsvc5       1/1       Running   0          54m
frontend-3823415956-k22zn       1/1       Running   0          54m
frontend-3823415956-w9gbt       1/1       Running   0          54m
frontend-3823415956-x2pld       1/1       Running   0          5s
redis-master-1068406935-3lswp   1/1       Running   0          56m
redis-slave-2005841000-fpvqc    1/1       Running   0          55m
redis-slave-2005841000-phfv9    1/1       Running   0          55m

Run the following command to scale down the number of frontend Pods:

$ kubectl scale deployment frontend --replicas=2

Query the list of Pods to verify the number of frontend Pods running:

$ kubectl get pods
NAME                            READY     STATUS    RESTARTS   AGE
frontend-3823415956-k22zn       1/1       Running   0          1h
frontend-3823415956-w9gbt       1/1       Running   0          1h
redis-master-1068406935-3lswp   1/1       Running   0          1h
redis-slave-2005841000-fpvqc    1/1       Running   0          1h
redis-slave-2005841000-phfv9    1/1       Running   0          1h


x.6 - Cleaning up
=================

Deleting the Deployments and Services also deletes any running Pods. Use 
labels to delete multiple resources with one command.

Run the following commands to delete all Pods, Deployments, and Services.

$ kubectl delete deployment -l app=redis
deployment.apps "redis-master" deleted
deployment.apps "redis-slave" deleted

$ kubectl delete service -l app=redis
service "redis-master" deleted
service "redis-slave" deleted

$ kubectl delete deployment -l app=guestbook
deployment.apps "frontend" deleted

$ kubectl delete service -l app=guestbook
service "frontend" deleted

Query the list of Pods to verify that no Pods are running:

$ kubectl get pods
No resources found.



















==============================================================================
==============================================================================
==============================================================================
==============================================================================
==============================================================================
==============================================================================
==============================================================================




###############################################################################
#
# Learn Kubernetes Basics - Part 4 - Explore your app
#
###############################################################################





###############################################################################
#
# Learn Kubernetes Basics - Part 5 - Expose Your App Publicly
#
###############################################################################



5.1 - Overview of Kubernetes Services
=====================================

Kubernetes Pods are mortal. Pods in fact have a lifecycle. When a worker node 
dies, the Pods running on the Node are also lost. A ReplicaSet might then 
dynamically drive the cluster back to desired state via creation of new Pods 
to keep your application running. As another example, consider an 
image-processing backend with 3 replicas. Those replicas are exchangeable; the 
front-end system should not care about backend replicas or even if a Pod is 
lost and recreated. That said, each Pod in a Kubernetes cluster has a unique IP 
address, even Pods on the same Node, so there needs to be a way of 
automatically reconciling changes among Pods so that your applications continue 
to function.

A Service in Kubernetes is an abstraction which defines a logical set of Pods 
and a policy by which to access them. Services enable a loose coupling between 
dependent Pods. A Service is defined using YAML (preferred) or JSON, like all 
Kubernetes objects. The set of Pods targeted by a Service is usually determined 
by a LabelSelector (see below for why you might want a Service without 
including selector in the spec): a LabelSelector is a the way Kubernetes will 
identify the right pods

Although each Pod has a unique IP address, those IPs are not exposed outside 
the cluster without a Service. Services allow your applications to receive 
traffic (from outside or from other applications runnig on the same cluster). 
Services can be exposed in different ways by specifying a type in the 
ServiceSpec:

    - ClusterIP (default) - Exposes the Service on an internal IP in the 
        cluster. This type makes the Service only reachable from within the 
        cluster.
    - NodePort - Exposes the Service on the same port of each selected Node in 
        the cluster using NAT. Makes a Service accessible from outside the 
        cluster using <NodeIP>:<NodePort>. Superset of ClusterIP.
    - LoadBalancer - Creates an external load balancer in the current cloud 
        (if supported) and assigns a fixed, external IP to the Service. 
        Superset of NodePort.
    - ExternalName - Exposes the Service using an arbitrary name (specified by 
        externalName in the spec) by returning a CNAME record with the name. No 
        proxy is used. This type requires v1.7 or higher of kube-dns.


    NB: more information about the different types of Services can be found in 
        the 'Using Source IP' tutorial. Also see 'Connecting Applications with 
        Services'.

Additionally, note that there are some use cases with Services that involve not 
defining selector in the spec. A Service created without selector will also not 
create the corresponding Endpoints object. This allows users to manually map a 
Service to specific endpoints. Another possibility why there may be no selector 
is you are strictly using type: ExternalName.


5.2 - Services and Labels
=========================

A Service routes traffic across a set of Pods. Services are the abstraction 
that allow pods to die and replicate in Kubernetes without impacting your 
application. Discovery and routing among dependent Pods (such as the frontend 
and backend components in an application) is handled by Kubernetes Services.

Services match a set of Pods using labels and selectors, a grouping primitive 
that allows logical operation on objects in Kubernetes. Labels are key/value 
pairs attached to objects and can be used in any number of ways:

    - Designate objects for development, test, and production
    - Embed version tags
    - Classify an object using tags

You can create a Service at the same time you create a Deployment by using
--expose in kubectl.


Labels can be attached to objects at creation time or later on. They can be 
modified at any time. Let's expose our application now using a Service and 
apply some labels.




###############################################################################
#
# Learn Kubernetes Basics - Part 6 - Scale Your App
#
###############################################################################


6.1 - Scaling an application
============================

In the previous modules we created a Deployment, and then exposed it publicly 
via a Service. The Deployment created only one Pod for running our application. 
When traffic increases, we will need to scale the application to keep up with 
user demand.

You can create from the start a Deployment with multiple instances using the 
--replicas parameter for the kubectl run command

Scaling out a Deployment will ensure new Pods are created and scheduled to 
Nodes with available resources. Scaling will increase the number of Pods to the 
new desired state. Kubernetes also supports autoscaling of Pods, but it is 
outside of the scope of this tutorial. Scaling to zero is also possible, and it 
will terminate all Pods of the specified Deployment.

Running multiple instances of an application will require a way to distribute 
the traffic to all of them. Services have an integrated load-balancer that will 
distribute network traffic to all Pods of an exposed Deployment. Services will 
monitor continuously the running Pods using endpoints, to ensure the traffic is 
sent only to available Pods.

Scaling is accomplished by changing the number of replicas in a Deployment.

Once you have multiple instances of an Application running, you would be able 
to do Rolling updates without downtime. We'll cover that in the next module. 
Now, let's go to the online terminal and scale our application.



###############################################################################
#
# Learn Kubernetes Basics - Part 7 - Update Your App
#
###############################################################################


7.1 - Updating an application
=============================

Users expect applications to be available all the time and developers are 
expected to deploy new versions of them several times a day. In Kubernetes this 
is done with rolling updates. Rolling updates allow Deployments' update to take 
place with zero downtime by incrementally updating Pods instances with new 
ones. The new Pods will be scheduled on Nodes with available resources.

In the previous module we scaled our application to run multiple instances. 
This is a requirement for performing updates without affecting application 
availability. By default, the maximum number of Pods that can be unavailable 
during the update and the maximum number of new Pods that can be created, is 
one. Both options can be configured to either numbers or percentages (of Pods). 
In Kubernetes, updates are versioned and any Deployment update can be reverted 
to previous (stable) version.


7.2 - Rolling updates overview
==============================

Similar to application Scaling, if a Deployment is exposed publicly, the 
Service will load-balance the traffic only to available Pods during the update. 
An available Pod is an instance that is available to the users of the 
application.

Rolling updates allow the following actions:

    - Promote an application from one environment to another (via container 
        image updates)
    - Rollback to previous versions
    - Continuous Integration and Continuous Delivery of applications with zero 
        downtime

If a Deployment is exposed publicly, the Service will load-balance the traffic 
only to available Pods during the update.

