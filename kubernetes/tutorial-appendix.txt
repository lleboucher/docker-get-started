

===============================================================================
===============================================================================

APPENDIX 1 - Kubernetes Node

===============================================================================
===============================================================================


A node is a worker machine in Kubernetes. A node may be a VM or 
physical machine (we then speak of a 'bare metal deployment'), depending on the 
cluster. Each node contains the services necessary to run pods and is managed 
by the master components. The services on a node include:
    - the container runtime,
    - kubelet
    - kube-proxy.


===============
1 - Node Status
===============

A node’s status contains the following information:
    - Addresses
    - Conditions
    - Capacity and Allocatable
    - Info

Node status and other details about a node can be displayed using below 
command:

$ kubectl describe node <insert-node-name-here>

1.1 - Addresses
===============

The usage of these fields varies depending on your cloud provider or bare metal 
configuration.

    HostName:   The hostname as reported by the node’s kernel. Can be 
                overridden via the kubelet --hostname-override parameter.
    ExternalIP: Typically the IP address of the node that is externally 
                routable (available from outside the cluster).
    InternalIP: Typically the IP address of the node that is routable only 
                within the cluster.

1.2 - Conditions
================

The conditions field describes the status of all Running nodes. Examples of 
conditions include:

Node                Condition
Ready	            True if the node is healthy and ready to accept pods,
                    False if the node is not healthy and is not accepting pods, 
                    Unknown if the node controller has not heard from the node 
                      in the last node-monitor-grace-period (default is 40 
                      seconds)
MemoryPressure	    True if pressure exists on the node memory – that is, if 
                      the node memory is low; 
                    False otherwise
PIDPressure	        True if pressure exists on the processes – that is, if 
                      there are too many processes on the node; 
                    False otherwise
DiskPressure	    True if pressure exists on the disk size – that is, if 
                      the disk capacity is low; 
                    False otherwise
NetworkUnavailable	True if the network for the node is not correctly 
                      configured, 
                    False otherwise

The node condition is represented as a JSON object. For example, the following 
response describes a healthy node.

"conditions": [
  {
    "type": "Ready",
    "status": "True",
    "reason": "KubeletReady",
    "message": "kubelet is posting ready status",
    "lastHeartbeatTime": "2019-06-05T18:38:35Z",
    "lastTransitionTime": "2019-06-05T11:41:27Z"
  }
]


If the Status of the Ready condition remains Unknown or False for longer than 
the pod-eviction-timeout, an argument is passed to the kube-controller-manager 
and all the Pods on the node are scheduled for deletion by the Node Controller. 
The default eviction timeout duration is five minutes. In some cases when the 
node is unreachable, the apiserver is unable to communicate with the kubelet 
on the node. The decision to delete the pods cannot be communicated to the 
kubelet until communication with the apiserver is re-established. In the 
meantime, the pods that are scheduled for deletion may continue to run on the 
partitioned node.

In versions of Kubernetes prior to 1.5, the node controller would force delete 
these unreachable pods from the apiserver. However, in 1.5 and higher, the 
node controller does not force delete pods until it is confirmed that they 
have stopped running in the cluster. You can see the pods that might be 
running on an unreachable node as being in the Terminating or Unknown state. 
In cases where Kubernetes cannot deduce from the underlying infrastructure if 
a node has permanently left a cluster, the cluster administrator may need to 
delete the node object by hand. Deleting the node object from Kubernetes 
causes all the Pod objects running on the node to be deleted from the 
apiserver, and frees up their names.

In version 1.12, TaintNodesByCondition feature is promoted to beta, so node 
lifecycle controller automatically creates taints that represent conditions. 
Similarly the scheduler ignores conditions when considering a Node; instead it 
looks at the Node’s taints and a Pod’s tolerations.

Now users can choose between the old scheduling model and a new, more flexible 
scheduling model. A Pod that does not have any tolerations gets scheduled 
according to the old model. But a Pod that tolerates the taints of a particular 
Node can be scheduled on that Node.

    Caution: Enabling this feature creates a small delay between the time when 
      a condition is observed and when a taint is created. This delay is 
      usually less than one second, but it can increase the number of Pods 
      that are successfully scheduled but rejected by the kubelet.

1.3 - Capacity and Allocatable
==============================

Describes the resources available on the node: CPU, memory and the maximum 
number of pods that can be scheduled onto the node.

The fields in the capacity block indicate the total amount of resources that 
a Node has. The allocatable block indicates the amount of resources on a Node 
that is available to be consumed by normal Pods.

You may read more about capacity and allocatable resources while learning how 
to reserve compute resources on a Node.


1.4 - Info
==========

Describes general information about the node, such as kernel version, 
Kubernetes version (kubelet and kube-proxy version), Docker version (if used), 
and OS name. This information is gathered by Kubelet from the node.

=============
2. Management
=============

Unlike pods and services, a node is not inherently created by Kubernetes: it 
is created externally by cloud providers like Google Compute Engine, or it 
exists in your pool of physical or virtual machines. So when Kubernetes 
creates a node, it creates an object that represents the node. After creation, 
Kubernetes checks whether the node is valid or not. For example, if you try to 
create a node from the following content:

{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}

Kubernetes creates a node object internally (the representation), and 
validates the node by health checking based on the metadata.name field. If 
the node is valid – that is, if all necessary services are running – it is 
eligible to run a pod. Otherwise, it is ignored for any cluster activity until 
it becomes valid.

    Note: Kubernetes keeps the object for the invalid node and keeps checking 
      to see whether it becomes valid. You must explicitly delete the Node 
      object to stop this process.

Currently, there are three components that interact with the Kubernetes node 
interface: node controller, kubelet, and kubectl.

2.1 - Node Controller
=====================

The node controller is a Kubernetes master component which manages various 
aspects of nodes.

The node controller has multiple roles in a node’s life.

    - The first is assigning a CIDR block to the node when it is registered (if 
      CIDR assignment is turned on).

    - The second is keeping the node controller’s internal list of nodes up to 
      date with the cloud provider’s list of available machines. When running 
      in a cloud environment, whenever a node is unhealthy, the node controller 
      asks the cloud provider if the VM for that node is still available. If 
      not, the node controller deletes the node from its list of nodes.

    - The third is monitoring the nodes’ health. The node controller is 
      responsible for updating the NodeReady condition of NodeStatus to 
      ConditionUnknown when a node becomes unreachable (i.e. the node 
      controller stops receiving heartbeats for some reason, e.g. due to the 
      node being down), and then later evicting all the pods from the node 
      (using graceful termination) if the node continues to be unreachable. 
      (The default timeouts are 40s to start reporting ConditionUnknown and 
      5m after that to start evicting pods.) The node controller checks the 
      state of each node every --node-monitor-period seconds.

2.1.1 - Heartbeats
==================

Heartbeats, sent by Kubernetes nodes, help determine the availability of a node. There are two forms of heartbeats: updates of NodeStatus and the Lease object. Each Node has an associated Lease object in the kube-node-lease namespace . Lease is a lightweight resource, which improves the performance of the node heartbeats as the cluster scales.

The kubelet is responsible for creating and updating the NodeStatus and a Lease object.

    The kubelet updates the NodeStatus either when there is change in status, or if there has been no update for a configured interval. The default interval for NodeStatus updates is 5 minutes (much longer than the 40 second default timeout for unreachable nodes).
    The kubelet creates and then updates its Lease object every 10 seconds (the default update interval). Lease updates occur independently from the NodeStatus updates.
    

2.1.2- Reliability
==================

In Kubernetes 1.4, we updated the logic of the node controller to better 
handle cases when a large number of nodes have problems with reaching the 
master (e.g. because the master has networking problem). Starting with 1.4, 
the node controller looks at the state of all nodes in the cluster when making 
a decision about pod eviction.

In most cases, node controller limits the eviction rate to --node-eviction-rate 
(default 0.1) per second, meaning it won’t evict pods from more than 1 node 
per 10 seconds.

The node eviction behavior changes when a node in a given availability zone 
becomes unhealthy. The node controller checks what percentage of nodes in the 
zone are unhealthy (NodeReady condition is ConditionUnknown or ConditionFalse) 
at the same time. If the fraction of unhealthy nodes is at least 
--unhealthy-zone-threshold (default 0.55) then the eviction rate is reduced: 
if the cluster is small (i.e. has less than or equal to 
--large-cluster-size-threshold nodes - default 50) then evictions are stopped, 
otherwise the eviction rate is reduced to --secondary-node-eviction-rate 
(default 0.01) per second. The reason these policies are implemented per 
availability zone is because one availability zone might become partitioned 
from the master while the others remain connected. If your cluster does not 
span multiple cloud provider availability zones, then there is only one 
availability zone (the whole cluster).

A key reason for spreading your nodes across availability zones is so that the 
workload can be shifted to healthy zones when one entire zone goes down. 
Therefore, if all nodes in a zone are unhealthy then node controller evicts at 
the normal rate --node-eviction-rate. The corner case is when all zones are 
completely unhealthy (i.e. there are no healthy nodes in the cluster). In such 
case, the node controller assumes that there’s some problem with master 
connectivity and stops all evictions until some connectivity is restored.

Starting in Kubernetes 1.6, the NodeController is also responsible for evicting 
pods that are running on nodes with NoExecute taints, when the pods do not 
tolerate the taints. Additionally, as an alpha feature that is disabled by 
default, the NodeController is responsible for adding taints corresponding 
to node problems like node unreachable or not ready. See this documentation 
for details about NoExecute taints and the alpha feature.

Starting in version 1.8, the node controller can be made responsible for 
creating taints that represent Node conditions. This is an alpha feature of 
version 1.8.

2.2 - Self-Registration of Nodes
================================

When the kubelet flag --register-node is true (the default), the kubelet will 
attempt to register itself with the API server. This is the preferred pattern, 
used by most distros.

For self-registration, the kubelet is started with the following options:

    --kubeconfig - Path to credentials to authenticate itself to the apiserver.
    --cloud-provider - How to talk to a cloud provider to read metadata about 
        itself.
    --register-node - Automatically register with the API server.
    --register-with-taints - Register the node with the given list of taints 
        (comma separated <key>=<value>:<effect>). No-op if register-node is 
        false.
    --node-ip - IP address of the node.
    --node-labels - Labels to add when registering the node in the cluster 
        (see label restrictions enforced by the NodeRestriction admission 
        plugin in 1.13+).
    --node-status-update-frequency - Specifies how often kubelet posts node 
        status to master.

When the Node authorization mode and NodeRestriction admission plugin are 
enabled, kubelets are only authorized to create/modify their own Node resource.


2.2.1 - Manual Node Administration
==================================

A cluster administrator can create and modify node objects.

If the administrator wishes to create node objects manually, set the kubelet 
flag --register-node=false.

The administrator can modify node resources (regardless of the setting of 
--register-node). Modifications include setting labels on the node and marking 
it unschedulable.

Labels on nodes can be used in conjunction with node selectors on pods to 
control scheduling, e.g. to constrain a pod to only be eligible to run on a 
subset of the nodes.

Marking a node as unschedulable prevents new pods from being scheduled to that 
node, but does not affect any existing pods on the node. This is useful as a 
preparatory step before a node reboot, etc. For example, to mark a node 
unschedulable, run this command:

$ kubectl cordon $NODENAME

    Note: Pods created by a DaemonSet controller bypass the Kubernetes 
        scheduler and do not respect the unschedulable attribute on a node. 
        This assumes that daemons belong on the machine even if it is being 
        drained of applications while it prepares for a reboot.


2.3 - Node capacity
===================

The capacity of the node (number of cpus and amount of memory) is part of the 
node object. Normally, nodes register themselves and report their capacity 
when creating the node object. If you are doing manual node administration, 
then you need to set node capacity when adding a node.

The Kubernetes scheduler ensures that there are enough resources for all the 
pods on a node. It checks that the sum of the requests of containers on the 
node is no greater than the node capacity. It includes all containers started 
by the kubelet, but not containers started directly by the container runtime 
nor any process running outside of the containers.

If you want to explicitly reserve resources for non-Pod processes, follow this 
tutorial to reserve resources for system daemons.




===============================================================================
===============================================================================

APPENDIX 2 - Master-Node Communication

===============================================================================
===============================================================================

This document catalogs the communication paths between the master (really the apiserver) and the Kubernetes cluster. The intent is to allow users to customize their installation to harden the network configuration such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud provider).


2.1 - Cluster to Master
=======================

All communication paths from the cluster to the master terminate at the apiserver (none of the other master components are designed to expose remote services). In a typical deployment, the apiserver is configured to listen for remote connections on a secure HTTPS port (443) with one or more forms of client authentication enabled. One or more forms of authorization should be enabled, especially if anonymous requests or service account tokens are allowed.

Nodes should be provisioned with the public root certificate for the cluster such that they can connect securely to the apiserver along with valid client credentials. For example, on a default GKE deployment, the client credentials provided to the kubelet are in the form of a client certificate. See kubelet TLS bootstrapping for automated provisioning of kubelet client certificates.

Pods that wish to connect to the apiserver can do so securely by leveraging a service account so that Kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in all namespaces) is configured with a virtual IP address that is redirected (via kube-proxy) to the HTTPS endpoint on the apiserver.

The master components also communicate with the cluster apiserver over the secure port.

As a result, the default operating mode for connections from the cluster (nodes and pods running on the nodes) to the master is secured by default and can run over untrusted and/or public networks.


2.2 - Master to Cluster
=======================

There are two primary communication paths from the master (apiserver) to the cluster. The first is from the apiserver to the kubelet process which runs on each node in the cluster. The second is from the apiserver to any node, pod, or service through the apiserver’s proxy functionality.


2.2.1 - apiserver to kubelet
============================

The connections from the apiserver to the kubelet are used for:

    Fetching logs for pods.
    Attaching (through kubectl) to running pods.
    Providing the kubelet’s port-forwarding functionality.

These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.

To verify this connection, use the --kubelet-certificate-authority flag to provide the apiserver with a root certificate bundle to use to verify the kubelet’s serving certificate.

If that is not possible, use SSH tunneling between the apiserver and kubelet if required to avoid connecting over an untrusted or public network.

Finally, Kubelet authentication and/or authorization should be enabled to secure the kubelet API.


2.2.2 - apiserver to nodes, pods, and services
==============================================

The connections from the apiserver to a node, pod, or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod, or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials so while the connection will be encrypted, it will not provide any guarantees of integrity. These connections are not currently safe to run over untrusted and/or public networks.


2.2.3 - SSH Tunnels
===================

Kubernetes supports SSH tunnels to protect the Master -> Cluster communication paths. In this configuration, the apiserver initiates an SSH tunnel to each node in the cluster (connecting to the ssh server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or service through the tunnel. This tunnel ensures that the traffic is not exposed outside of the network in which the nodes are running.

SSH tunnels are currently deprecated so you shouldn’t opt to use them unless you know what you are doing. A replacement for this communication channel is being designed.


===============================================================================
===============================================================================

APPENDIX 3 - Controllers

===============================================================================
===============================================================================


In robotics and automation, a control loop is a non-terminating loop that regulates the state of a system.

Here is one example of a control loop: a thermostat in a room.

When you set the temperature, that’s telling the thermostat about your desired state. The actual room temperature is the current state. The thermostat acts to bring the current state closer to the desired state, by turning equipment on or off.

In Kubernetes, controllers are control loops that watch the state of your cluster , then make or request changes where needed. Each controller tries to move the current cluster state closer to the desired state.


3.1 - Controller pattern
========================

A controller tracks at least one Kubernetes resource type. These objects have a spec field that represents the desired state. The controller(s) for that resource are responsible for making the current state come closer to that desired state.

The controller might carry the action out itself; more commonly, in Kubernetes, a controller will send messages to the API server that have useful side effects. You’ll see examples of this below.


3.1.1 - Control via API server
==============================

The Job controller is an example of a Kubernetes built-in controller. Built-in controllers manage state by interacting with the cluster API server.

Job is a Kubernetes resource that runs a Pod , or perhaps several Pods, to carry out a task and then stop.

(Once scheduled, Pod objects become part of the desired state for a kubelet).

When the Job controller sees a new task it makes sure that, somewhere in your cluster, the kubelets on a set of Nodes are running the right number of Pods to get the work done. The Job controller does not run any Pods or containers itself. Instead, the Job controller tells the API server to create or remove Pods. Other components in the control plane act on the new information (there are new Pods to schedule and run), and eventually the work is done.

After you create a new Job, the desired state is for that Job to be completed. The Job controller makes the current state for that Job be nearer to your desired state: creating Pods that do the work you wanted for that Job, so that the Job is closer to completion.

Controllers also update the objects that configure them. For example: once the work is done for a Job, the Job controller updates that Job object to mark it Finished.

(This is a bit like how some thermostats turn a light off to indicate that your room is now at the temperature you set).


3.1.2 - Direct control
======================

By contrast with Job, some controllers need to make changes to things outside of your cluster.

For example, if you use a control loop to make sure there are enough Nodes in your cluster, then that controller needs something outside the current cluster to set up new Nodes when needed.

Controllers that interact with external state find their desired state from the API server, then communicate directly with an external system to bring the current state closer in line.

(There actually is a controller that horizontally scales the nodes in your cluster. See Cluster autoscaling).


3.2 - Desired versus current state
==================================

Kubernetes takes a cloud-native view of systems, and is able to handle constant change.

Your cluster could be changing at any point as work happens and control loops automatically fix failures. This means that, potentially, your cluster never reaches a stable state.

As long as the controllers for your cluster are running and able to make useful changes, it doesn’t matter if the overall state is or is not stable.


3.3 - Design
============

As a tenet of its design, Kubernetes uses lots of controllers that each manage a particular aspect of cluster state. Most commonly, a particular control loop (controller) uses one kind of resource as its desired state, and has a different kind of resource that it manages to make that desired state happen.

It’s useful to have simple controllers rather than one, monolithic set of control loops that are interlinked. Controllers can fail, so Kubernetes is designed to allow for that.

For example: a controller for Jobs tracks Job objects (to discover new work) and Pod object (to run the Jobs, and then to see when the work is finished). In this case something else creates the Jobs, whereas the Job controller creates Pods.

    Note:

    There can be several controllers that create or update the same kind of object. Behind the scenes, Kubernetes controllers make sure that they only pay attention to the resources linked to their controlling resource.

    For example, you can have Deployments and Jobs; these both create Pods. The Job controller does not delete the Pods that your Deployment created, because there is information (labels ) the controllers can use to tell those Pods apart.


3.4 - Ways of running controllers
=================================

Kubernetes comes with a set of built-in controllers that run inside the kube-controller-manager . These built-in controllers provide important core behaviors.

The Deployment controller and Job controller are examples of controllers that come as part of Kubernetes itself (“built-in” controllers). Kubernetes lets you run a resilient control plane, so that if any of the built-in controllers were to fail, another part of the control plane will take over the work.

You can find controllers that run outside the control plane, to extend Kubernetes. Or, if you want, you can write a new controller yourself. You can run your own controller as a set of Pods, or externally to Kubernetes. What fits best will depend on what that particular controller does.


3.5 - What's next
=================

    Read about the Kubernetes control plane
    Discover some of the basic Kubernetes objects
    Learn more about the Kubernetes API
    If you want to write your own controller, see Extension Patterns in Extending Kubernetes.







===============================================================================
===============================================================================

APPENDIX 4 - Concepts Underlying the Cloud Controller Manager

===============================================================================
===============================================================================

The cloud controller manager (CCM) concept (not to be confused with the binary) was originally created to allow cloud specific vendor code and the Kubernetes core to evolve independent of one another. The cloud controller manager runs alongside other master components such as the Kubernetes controller manager, the API server, and scheduler. It can also be started as a Kubernetes addon, in which case it runs on top of Kubernetes.

The cloud controller manager’s design is based on a plugin mechanism that allows new cloud providers to integrate with Kubernetes easily by using plugins. There are plans in place for on-boarding new cloud providers on Kubernetes and for migrating cloud providers from the old model to the new CCM model.

This document discusses the concepts behind the cloud controller manager and gives details about its associated functions.

Here’s the architecture of a Kubernetes cluster without the cloud controller manager:

(image: Cloud Controller Manager)



4.1 - Design
============

In the preceding diagram, Kubernetes and the cloud provider are integrated through several different components:

    Kubelet
    Kubernetes controller manager
    Kubernetes API server

The CCM consolidates all of the cloud-dependent logic from the preceding three components to create a single point of integration with the cloud. The new architecture with the CCM looks like this:

(image: Cloud Controller Manager)


4.2 - Components of the CCM
===========================

The CCM breaks away some of the functionality of Kubernetes controller manager (KCM) and runs it as a separate process. Specifically, it breaks away those controllers in the KCM that are cloud dependent. The KCM has the following cloud dependent controller loops:

    Node controller
    Volume controller
    Route controller
    Service controller

In version 1.9, the CCM runs the following controllers from the preceding list:

    Node controller
    Route controller
    Service controller

    Note: Volume controller was deliberately chosen to not be a part of CCM. Due to the complexity involved and due to the existing efforts to abstract away vendor specific volume logic, it was decided that volume controller will not be moved to CCM.

The original plan to support volumes using CCM was to use Flex volumes to support pluggable volumes. However, a competing effort known as CSI is being planned to replace Flex.

Considering these dynamics, we decided to have an intermediate stop gap measure until CSI becomes ready.


4.3 - Functions of the CCM
==========================

The CCM inherits its functions from components of Kubernetes that are dependent on a cloud provider. This section is structured based on those components.

4.3.1 - Kubernetes controller manager
=====================================

The majority of the CCM’s functions are derived from the KCM. As mentioned in the previous section, the CCM runs the following control loops:

    Node controller
    Route controller
    Service controller

4.3.1.1 - Node controller

The Node controller is responsible for initializing a node by obtaining information about the nodes running in the cluster from the cloud provider. The node controller performs the following functions:

    Initialize a node with cloud specific zone/region labels.
    Initialize a node with cloud specific instance details, for example, type and size.
    Obtain the node’s network addresses and hostname.
    In case a node becomes unresponsive, check the cloud to see if the node has been deleted from the cloud. If the node has been deleted from the cloud, delete the Kubernetes Node object.

4.3.1.2 - Route controller

The Route controller is responsible for configuring routes in the cloud appropriately so that containers on different nodes in the Kubernetes cluster can communicate with each other. The route controller is only applicable for Google Compute Engine clusters.
Service Controller

The Service controller is responsible for listening to service create, update, and delete events. Based on the current state of the services in Kubernetes, it configures cloud load balancers (such as ELB , Google LB, or Oracle Cloud Infrastructure LB) to reflect the state of the services in Kubernetes. Additionally, it ensures that service backends for cloud load balancers are up to date.

4.3.2 - Kubelet
===============

The Node controller contains the cloud-dependent functionality of the kubelet. Prior to the introduction of the CCM, the kubelet was responsible for initializing a node with cloud-specific details such as IP addresses, region/zone labels and instance type information. The introduction of the CCM has moved this initialization operation from the kubelet into the CCM.

In this new model, the kubelet initializes a node without cloud-specific information. However, it adds a taint to the newly created node that makes the node unschedulable until the CCM initializes the node with cloud-specific information. It then removes this taint.


4.4 - Plugin mechanism
======================

The cloud controller manager uses Go interfaces to allow implementations from any cloud to be plugged in. Specifically, it uses the CloudProvider Interface defined here.

The implementation of the four shared controllers highlighted above, and some scaffolding along with the shared cloudprovider interface, will stay in the Kubernetes core. Implementations specific to cloud providers will be built outside of the core and implement interfaces defined in the core.

For more information about developing plugins, see Developing Cloud Controller Manager.


4.5 - Authorization
===================

This section breaks down the access required on various API objects by the CCM to perform its operations.

4.5.1 - Node Controller

The Node controller only works with Node objects. It requires full access to get, list, create, update, patch, watch, and delete Node objects.

v1/Node:

    Get
    List
    Create
    Update
    Patch
    Watch
    Delete

4.5.2 - Route controller

The route controller listens to Node object creation and configures routes appropriately. It requires get access to Node objects.

v1/Node:

    Get

4.5.3 - Service controller

The service controller listens to Service object create, update and delete events and then configures endpoints for those Services appropriately.

To access Services, it requires list, and watch access. To update Services, it requires patch and update access.

To set up endpoints for the Services, it requires access to create, list, get, watch, and update.

v1/Service:

    List
    Get
    Watch
    Patch
    Update

4.5.4 - Others

The implementation of the core of CCM requires access to create events, and to ensure secure operation, it requires access to create ServiceAccounts.

v1/Event:

    Create
    Patch
    Update

v1/ServiceAccount:

    Create

The RBAC ClusterRole for the CCM looks like this:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cloud-controller-manager
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
  - update
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - '*'
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - ""
  resources:
  - services
  verbs:
  - list
  - patch
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - serviceaccounts
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - persistentvolumes
  verbs:
  - get
  - list
  - update
  - watch
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - create
  - get
  - list
  - watch
  - update


4.6 - Vendor Implementations
============================

The following cloud providers have implemented CCMs:

    AWS
    Azure
    BaiduCloud
    DigitalOcean
    GCP
    Linode
    OpenStack
    Oracle
    TencentCloud


4.7 - Cluster Administration
============================

Complete instructions for configuring and running the CCM are provided here (https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager).



























===============================================================================
===============================================================================

APPENDIX 2 - ReplicaSet, Deployment, StatefulSet, DaemonSet

===============================================================================
===============================================================================



==============
1 - ReplicaSet
==============


A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at 
any given time. As such, it is often used to guarantee the availability of a 
specified number of identical Pods.


1.1 - How a ReplicaSet works
============================

A ReplicaSet is defined with fields, including a selector that specifies how 
to identify Pods it can acquire, a number of replicas indicating how many Pods 
it should be maintaining, and a pod template specifying the data of new Pods 
it should create to meet the number of replicas criteria. A ReplicaSet then 
fulfills its purpose by creating and deleting Pods as needed to reach the 
desired number. When a ReplicaSet needs to create new Pods, it uses its Pod 
template.

The link a ReplicaSet has to its Pods is via the Pods’ metadata.ownerReferences 
field, which specifies what resource the current object is owned by. All Pods 
acquired by a ReplicaSet have their owning ReplicaSet’s identifying 
information within their ownerReferences field. It’s through this link that 
the ReplicaSet knows of the state of the Pods it is maintaining and plans 
accordingly.

A ReplicaSet identifies new Pods to acquire by using its selector. If there is 
a Pod that has no OwnerReference or the OwnerReference is not a Controller and 
it matches a ReplicaSet’s selector, it will be immediately acquired by said 
ReplicaSet.


1.2 - When to use a ReplicaSet
==============================

A ReplicaSet ensures that a specified number of pod replicas are running at 
any given time. However, a Deployment is a higher-level concept that manages 
ReplicaSets and provides declarative updates to Pods along with a lot of other 
useful features. Therefore, we recommend using Deployments instead of directly 
using ReplicaSets, unless you require custom update orchestration or don’t 
require updates at all.

This actually means that you may never need to manipulate ReplicaSet objects: 
use a Deployment instead, and define your application in the spec section.

File: controllers/frontend.yaml

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3


Saving this manifest into frontend.yaml and submitting it to a Kubernetes 
cluster will create the defined ReplicaSet and the Pods that it manages.

$ kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml

You can then get the current ReplicaSets deployed:

$ kubectl get rs

And see the frontend one you created:

NAME       DESIRED   CURRENT   READY   AGE
frontend   3         3         3       6s

You can also check on the state of the replicaset:

$ kubectl describe rs/frontend

And you will see output similar to:

Name:		      frontend
Namespace:	      default
Selector:	      tier=frontend,tier in (frontend)
Labels:		      app=guestbook
                  tier=frontend
Annotations:	  <none>
Replicas:	      3 current / 3 desired
Pods Status:	  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:         app=guestbook
                  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         80/TCP
    Requests:
      cpu:        100m
      memory:     100Mi
    Environment:
      GET_HOSTS_FROM:   dns
    Mounts:             <none>
  Volumes:              <none>
Events:
  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----                -------------    --------    ------            -------
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-qhloh
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
  1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-9si5l

  
And lastly you can check for the Pods brought up:

$vkubectl get Pods

You should see Pod information similar to:

NAME             READY     STATUS    RESTARTS   AGE
frontend-9si5l   1/1       Running   0          1m
frontend-dnjpy   1/1       Running   0          1m
frontend-qhloh   1/1       Running   0          1m


You can also verify that the owner reference of these pods is set to the 
frontend ReplicaSet. To do this, get the yaml of one of the Pods running:

$ kubectl get pods frontend-9si5l -o yaml

The output will look similar to this, with the frontend ReplicaSet’s info set 
in the metadata’s ownerReferences field:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: 2019-01-31T17:20:41Z
  generateName: frontend-
  labels:
    tier: frontend
  name: frontend-9si5l
  namespace: default
  ownerReferences:
  - apiVersion: extensions/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: ReplicaSet
    name: frontend
    uid: 892a2330-257c-11e9-aecd-025000000001
...

Non-Template Pod acquisitions

While you can create bare Pods with no problems, it is strongly recommended to 
make sure that the bare Pods do not have labels which match the selector of 
one of your ReplicaSets. The reason for this is because a ReplicaSet is not 
limited to owning Pods specified by its template– it can acquire other Pods 
in the manner specified in the previous sections.

Take the previous frontend ReplicaSet example, and the Pods specified in the 
following manifest:

File: pods/pod-rs.yaml

apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: hello1
    image: gcr.io/google-samples/hello-app:2.0

---

apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    tier: frontend
spec:
  containers:
  - name: hello2
    image: gcr.io/google-samples/hello-app:1.0


As those Pods do not have a Controller (or any object) as their owner reference 
and match the selector of the frontend ReplicaSet, they will immediately be 
acquired by it.

Suppose you create the Pods after the frontend ReplicaSet has been deployed and 
has set up its initial Pod replicas to fulfill its replica count requirement:

$ kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml

The new Pods will be acquired by the ReplicaSet, and then immediately 
terminated as the ReplicaSet would be over its desired count.

Fetching the Pods:

$ kubectl get Pods

The output shows that the new Pods are either already terminated, or in the 
process of being terminated:

NAME             READY   STATUS        RESTARTS   AGE
frontend-9si5l   1/1     Running       0          1m
frontend-dnjpy   1/1     Running       0          1m
frontend-qhloh   1/1     Running       0          1m
pod2             0/1     Terminating   0          4s

If you create the Pods first:

$ kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml

And then create the ReplicaSet however:

$ kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml

You shall see that the ReplicaSet has acquired the Pods and has only created 
new ones according to its spec until the number of its new Pods and the 
original matches its desired count. As fetching the Pods:

$ kubectl get Pods

Will reveal in its output:

NAME             READY   STATUS    RESTARTS   AGE
frontend-pxj4r   1/1     Running   0          5s
pod1             1/1     Running   0          13s
pod2             1/1     Running   0          13s

In this manner, a ReplicaSet can own a non-homogenous set of Pods.


1.3 - Writing a ReplicaSet manifest
===================================

As with all other Kubernetes API objects, a ReplicaSet needs the apiVersion, 
kind, and metadata fields. For ReplicaSets, the kind is always just ReplicaSet. 
In Kubernetes 1.9 the API version apps/v1 on the ReplicaSet kind is the current 
version and is enabled by default. The API version apps/v1beta2 is deprecated. 
Refer to the first lines of the frontend.yaml example for guidance.

A ReplicaSet also needs a .spec section.

a) Pod Template
===============

The .spec.template is a pod template which is also required to have labels in 
place. In our frontend.yaml example we had one label: tier: frontend. Be 
careful not to overlap with the selectors of other controllers, lest they try 
to adopt this Pod.

For the template’s restart policy field, .spec.template.spec.restartPolicy, 
the only allowed value is Always, which is the default.

b) Pod Selector
===============

The .spec.selector field is a label selector. As discussed earlier these are 
the labels used to identify potential Pods to acquire. In our frontend.yaml 
example, the selector was:

matchLabels:
	tier: frontend

In the ReplicaSet, .spec.template.metadata.labels must match spec.selector, or 
it will be rejected by the API.

    Note: For 2 ReplicaSets specifying the same .spec.selector but different 
      .spec.template.metadata.labels and .spec.template.spec fields, each 
      ReplicaSet ignores the Pods created by the other ReplicaSet.

c) Replicas
===========

You can specify how many Pods should run concurrently by setting 
.spec.replicas. The ReplicaSet will create/delete its Pods to match this number.

If you do not specify .spec.replicas, then it defaults to 1.


1.4 - Working with ReplicaSets
==============================


a) Deleting a ReplicaSet and its Pods
=====================================

To delete a ReplicaSet and all of its Pods, use kubectl delete. The Garbage 
collector automatically deletes all of the dependent Pods by default.

When using the REST API or the client-go library, you must set 
propagationPolicy to Background or Foreground in the -d option. For example:

$ kubectl proxy --port=8080
$ curl -X DELETE  'localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Foreground"}' \
> -H "Content-Type: application/json"


b) Deleting just a ReplicaSet
=============================

You can delete a ReplicaSet without affecting any of its Pods using kubectl 
delete with the --cascade=false option. When using the REST API or the 
client-go library, you must set propagationPolicy to Orphan. For example:

$ kubectl proxy --port=8080
$ curl -X DELETE  'localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend' \
> -d '{"kind":"DeleteOptions","apiVersion":"v1","propagationPolicy":"Orphan"}' \
> -H "Content-Type: application/json"

Once the original is deleted, you can create a new ReplicaSet to replace it. 
As long as the old and new .spec.selector are the same, then the new one will 
adopt the old Pods. However, it will not make any effort to make existing Pods 
match a new, different pod template. To update Pods to a new spec in a 
controlled way, use a Deployment, as ReplicaSets do not support a rolling 
update directly.


c) Isolating Pods from a ReplicaSet
===================================

You can remove Pods from a ReplicaSet by changing their labels. This technique 
may be used to remove Pods from service for debugging, data recovery, etc. Pods 
that are removed in this way will be replaced automatically (assuming that 
the number of replicas is not also changed).


d) Scaling a ReplicaSet
=======================

A ReplicaSet can be easily scaled up or down by simply updating the 
.spec.replicas field. The ReplicaSet controller ensures that a desired number 
of Pods with a matching label selector are available and operational.


e) ReplicaSet as a Horizontal Pod Autoscaler Target
===================================================

A ReplicaSet can also be a target for Horizontal Pod Autoscalers (HPA). That 
is, a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA 
targeting the ReplicaSet we created in the previous example

File: controllers/hpa-rs.yaml

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50

Saving this manifest into hpa-rs.yaml and submitting it to a Kubernetes 
cluster should create the defined HPA that autoscales the target ReplicaSet 
depending on the CPU usage of the replicated Pods.

$ kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml

Alternatively, you can use the kubectl autoscale command to accomplish the 
same (and it’s easier!)

$ kubectl autoscale rs frontend --max=10


1.5 - Alternatives to ReplicaSet
================================


a) Deployment (recommended)
===========================
 
Deployment is an object which can own ReplicaSets and update them and their 
Pods via declarative, server-side rolling updates. While ReplicaSets can be 
used independently, today they’re mainly used by Deployments as a mechanism to
orchestrate Pod creation, deletion and updates. When you use Deployments you 
don’t have to worry about managing the ReplicaSets that they create. 
Deployments own and manage their ReplicaSets. As such, it is recommended to 
use Deployments when you want ReplicaSets.


b) Bare Pods
============

Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods 
that are deleted or terminated for any reason, such as in the case of node 
failure or disruptive node maintenance, such as a kernel upgrade. For this 
reason, we recommend that you use a ReplicaSet even if your application 
requires only a single Pod. Think of it similarly to a process supervisor, 
only it supervises multiple Pods across multiple nodes instead of individual
processes on a single node. A ReplicaSet delegates local container restarts to 
some agent on the node (for example, Kubelet or Docker).


c) Job
======

Use a Job instead of a ReplicaSet for Pods that are expected to terminate on 
their own (that is, batch jobs).


d) DaemonSet
============

Use a DaemonSet instead of a ReplicaSet for Pods that provide a machine-level
function, such as machine monitoring or machine logging. These Pods have a 
lifetime that is tied to a machine lifetime: the Pod needs to be running on 
the machine before other Pods start, and are safe to terminate when the 
machine is otherwise ready to be rebooted/shutdown.


e) ReplicationController
========================

ReplicaSets are the successors to ReplicationControllers. The two serve the 
same purpose, and behave similarly, except that a ReplicationController does 
not support set-based selector requirements as described in the labels user 
guide. As such, ReplicaSets are preferred over ReplicationControllers.




===============
2 - Deployments
===============


A Deployment provides declarative updates for Pods and ReplicaSets.

You describe a desired state in a Deployment, and the Deployment Controller 
changes the actual state to the desired state at a controlled rate. You can 
define Deployments to create new ReplicaSets, or to remove existing 
Deployments and adopt all their resources with new Deployments.

    Note: Do not manage ReplicaSets owned by a Deployment. Consider opening an 
      issue in the main Kubernetes repository if your use case is not covered 
      below.


2.1 - Use Case
==============

The following are typical use cases for Deployments:

    - Create a Deployment to rollout a ReplicaSet. The ReplicaSet creates Pods 
      in the background. Check the status of the rollout to see if it succeeds 
      or not.
    - Declare the new state of the Pods by updating the PodTemplateSpec of the
      Deployment. A new ReplicaSet is created and the Deployment manages 
      moving the Pods from the old ReplicaSet to the new one at a controlled 
      rate. Each new ReplicaSet updates the revision of the Deployment.
    - Rollback to an earlier Deployment revision if the current state of the
      Deployment is not stable. Each rollback updates the revision of the 
      Deployment.
    - Scale up the Deployment to facilitate more load.
    - Pause the Deployment to apply multiple fixes to its PodTemplateSpec and 
      then resume it to start a new rollout.
    - Use the status of the Deployment as an indicator that a rollout has stuck.
    - Clean up older ReplicaSets that you don’t need anymore.


2.2 - Creating a Deployment
===========================

The following is an example of a Deployment. It creates a ReplicaSet to bring 
up three nginx Pods:

File: controllers/nginx-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80

In this example:

    - A Deployment named nginx-deployment is created, indicated by the 
      .metadata.name field.
    - The Deployment creates three replicated Pods, indicated by the replicas 
      field.

    - The selector field defines how the Deployment finds which Pods to manage. 
      In this case, you simply select a label that is defined in the Pod 
      template (app: nginx). However, more sophisticated selection rules are 
      possible, as long as the Pod template itself satisfies the rule.

        Note: The matchLabels field is a map of {key,value} pairs. A single 
          {key,value} in the matchLabels map is equivalent to an element of 
          matchExpressions, whose key field is “key” the operator is “In”, and 
          the values array contains only “value”. All of the requirements, 
          from both matchLabels and matchExpressions, must be satisfied in 
          order to match.

    - The template field contains the following sub-fields:
        * The Pods are labeled app: nginx using the labels field.
        * The Pod template’s specification, or .template.spec field, indicates 
          that the Pods run one container, nginx, which runs the nginx Docker 
          Hub image at version 1.7.9.
        * Create one container and name it nginx using the name field.


Follow the steps given below to create the above Deployment:

Before you begin, make sure your Kubernetes cluster is up and running.

a) Create the Deployment
========================

Create the Deployment by running the following command:

    Note: You may specify the –record flag to write the command executed in 
        the resource annotation kubernetes.io/change-cause. It is useful for 
        future introspection. For example, to see the commands executed in 
        each Deployment revision.

$ kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml

Run kubectl get deployments to check if the Deployment was created. If the 
Deployment is still being created, the output is similar to the following:

NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s

When you inspect the Deployments in your cluster, the following fields are 
displayed:

    - NAME lists the names of the Deployments in the cluster.
    - DESIRED displays the desired number of replicas of the application, 
      which you define when you create the Deployment. This is the desired 
      state.
    - CURRENT displays how many replicas are currently running.
    - UP-TO-DATE displays the number of replicas that have been updated to 
      achieve the desired state.
    - AVAILABLE displays how many replicas of the application are available 
      to your users.
    - AGE displays the amount of time that the application has been running.

Notice how the number of desired replicas is 3 according to .spec.replicas 
field.

To see the Deployment rollout status, run the followig command:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment.apps/nginx-deployment successfully rolled out

Run the following command again a few seconds later:

$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s

Notice that the Deployment has created all three replicas, and all replicas 
are up-to-date (they contain the latest Pod template) and available.

To see the ReplicaSet (rs) created by the Deployment, run the follwing command:

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s

Notice that the name of the ReplicaSet is always formatted as 
[DEPLOYMENT-NAME]-[RANDOM-STRING]. The random string is randomly generated and 
uses the pod-template-hash as a seed.

To see the labels automatically generated for each Pod, run the following command:

$ kubectl get pods --show-labels
NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453

The created ReplicaSet ensures that there are three nginx Pods.

    Note: You must specify an appropriate selector and Pod template labels 
      in a Deployment (in this case, app: nginx). Do not overlap labels or 
      selectors with other controllers (including other Deployments and 
      StatefulSets). Kubernetes doesn’t stop you from overlapping, and if 
      multiple controllers have overlapping selectors those controllers might 
      conflict and behave unexpectedly.

b) Pod-template-hash label
==========================

    Note: Do not change this label.

The pod-template-hash label is added by the Deployment controller to every 
ReplicaSet that a Deployment creates or adopts.

This label ensures that child ReplicaSets of a Deployment do not overlap. It 
is generated by hashing the PodTemplate of the ReplicaSet and using the 
resulting hash as the label value that is added to the ReplicaSet selector, 
Pod template labels, and in any existing Pods that the ReplicaSet might have.


2.3 - Updating a Deployment
===========================


a) Updating the deployment
==========================

Note: A Deployment’s rollout is triggered if and only if the Deployment’s 
      Pod template (that is, .spec.template) is changed, for example if the 
      labels or container images of the template are updated. Other updates, 
      such as scaling the Deployment, do not trigger a rollout.

Follow the steps given below to update your Deployment:

Let’s update the nginx Pods to use the nginx:1.9.1 image instead of the 
nginx:1.7.9 image.

$ kubectl --record deployment.apps/nginx-deployment set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1

or simply use the following command:

$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 --record
deployment.apps/nginx-deployment image updated

Alternatively, you can edit the Deployment and change 
.spec.template.spec.containers[0].image from nginx:1.7.9 to nginx:1.9.1:

$ kubectl edit deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment edited

To see the rollout status, run:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
... and finally it shows:
deployment.apps/nginx-deployment successfully rolled out


b) Get more details on your updated Deployment:
===============================================

After the rollout succeeds, you can view the Deployment by running:
$ kubectl get deployments
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           36s

Run the following command to see that the Deployment updated the Pods by 
creating a new ReplicaSet and scaling it up to 3 replicas, as well as scaling 
down the old ReplicaSet to 0 replicas.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s

Running 'get pods' should now show only the new Pods:

$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s

Next time you want to update these Pods, you only need to update the 
Deployment’s Pod template again.

Deployment ensures that only a certain number of Pods are down while they are 
being updated. By default, it ensures that at least 75% of the desired number 
of Pods are up (25% max unavailable).

Deployment also ensures that only a certain number of Pods are created above 
the desired number of Pods. By default, it ensures that at most 125% of the 
desired number of Pods are up (25% max surge).

For example, if you look at the above Deployment closely, you will see that 
it first created a new Pod, then deleted some old Pods, and created new ones. 
It does not kill old Pods until a sufficient number of new Pods have come up, 
and does not create new Pods until a sufficient number of old Pods have been 
killed. It makes sure that at least 2 Pods are available and that at max 4 
Pods in total are available.

c) Get details of your Deployment:
==================================

$ kubectl describe deployments
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
Labels:  app=nginx
 Containers:
  nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0

Here you see that when you first created the Deployment, it created a 
ReplicaSet (nginx-deployment-2035384211) and scaled it up to 3 replicas 
directly. When you updated the Deployment, it created a new ReplicaSet 
(nginx-deployment-1564180365) and scaled it up to 1 and then scaled down the 
old ReplicaSet to 2, so that at least 2 Pods were available and at most 4 Pods 
were created at all times. It then continued scaling up and down the new and 
the old ReplicaSet, with the same rolling update strategy. Finally, you’ll 
have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is 
scaled down to 0.


d) Rollover (aka multiple updates in-flight)
============================================

Each time a new Deployment is observed by the Deployment controller, a 
ReplicaSet is created to bring up the desired Pods. If the Deployment is 
updated, the existing ReplicaSet that controls Pods whose labels match 
.spec.selector but whose template does not match .spec.template are scaled 
down. Eventually, the new ReplicaSet is scaled to .spec.replicas and all old 
ReplicaSets is scaled to 0.

If you update a Deployment while an existing rollout is in progress, the 
Deployment creates a new ReplicaSet as per the update and start scaling that 
up, and rolls over the ReplicaSet that it was scaling up previously – it will 
add it to its list of old ReplicaSets and start scaling it down.

For example, suppose you create a Deployment to create 5 replicas of 
nginx:1.7.9, but then update the Deployment to create 5 replicas of 
nginx:1.9.1, when only 3 replicas of nginx:1.7.9 had been created. In that 
case, the Deployment immediately starts killing the 3 nginx:1.7.9 Pods that it 
had created, and starts creating nginx:1.9.1 Pods. It does not wait for the 5 
replicas of nginx:1.7.9 to be created before changing course.

e) Label selector updates
=========================

It is generally discouraged to make label selector updates and it is suggested 
to plan your selectors up front. In any case, if you need to perform a label 
selector update, exercise great caution and make sure you have grasped all of 
the implications.

    Note: In API version apps/v1, a Deployment’s label selector is immutable 
      after it gets created.

    - Selector additions require the Pod template labels in the Deployment 
      spec to be updated with the new label too, otherwise a validation error 
      is returned. This change is a non-overlapping one, meaning that the new 
      selector does not select ReplicaSets and Pods created with the old 
      selector, resulting in orphaning all old ReplicaSets and creating a new 
      ReplicaSet.
    - Selector updates changes the existing value in a selector key – result 
      in the same behavior as additions.
    - Selector removals removes an existing key from the Deployment selector – 
      do not require any changes in the Pod template labels. Existing 
      ReplicaSets are not orphaned, and a new ReplicaSet is not created, but 
      note that the removed label still exists in any existing Pods and 
      ReplicaSets.


2.4 - Rolling Back a Deployment
===============================

Sometimes, you may want to rollback a Deployment; for example, when the 
Deployment is not stable, such as crash looping. By default, all of the 
Deployment’s rollout history is kept in the system so that you can rollback 
anytime you want (you can change that by modifying revision history limit).

    Note: A Deployment’s revision is created when a Deployment’s rollout is 
      triggered. This means that the new revision is created if and only if 
      the Deployment’s Pod template (.spec.template) is changed, for example 
      if you update the labels or container images of the template. Other 
      updates, such as scaling the Deployment, do not create a Deployment 
      revision, so that you can facilitate simultaneous manual- or 
      auto-scaling. This means that when you roll back to an earlier revision, 
      only the Deployment’s Pod template part is rolled back.

Suppose that you made a typo while updating the Deployment, by putting the 
image name as nginx:1.91 instead of nginx:1.9.1:

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true
deployment.apps/nginx-deployment image updated

The rollout gets stuck. You can verify it by checking the rollout status:

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 1 out of 3 new replicas have been updated...

Press Ctrl-C to stop the above rollout status watch. For more information on 
stuck rollouts, read more here.

You see that the number of old replicas (nginx-deployment-1564180365 and 
nginx-deployment-2035384211) is 2, and new replicas 
(nginx-deployment-3066724191) is 1.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s

Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is 
stuck in an image pull loop.

$ kubectl get pods
NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s

    Note: The Deployment controller stops the bad rollout automatically, and 
      stops scaling up the new ReplicaSet. This depends on the rollingUpdate 
      parameters (maxUnavailable specifically) that you have specified. 
      Kubernetes by default sets the value to 25%.

a) Get the description of the Deployment:
=========================================

$ kubectl describe deployment
Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1

To fix this, you need to rollback to a previous revision of Deployment that is 
stable.

b) Checking Rollout History of a Deployment
===========================================

Follow the steps given below to check the rollout history:

First, check the revisions of this Deployment:

$ kubectl rollout history deployment.v1.apps/nginx-deployment
deployments "nginx-deployment"
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true

CHANGE-CAUSE is copied from the Deployment annotation 
kubernetes.io/change-cause to its revisions upon creation. You can specify the
CHANGE-CAUSE message by:
    - Annotating the Deployment with kubectl annotate 
      deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"
    - Append the --record flag to save the kubectl command that is making 
      changes to the resource.
    - Manually editing the manifest of the resource.

To see the details of each revision, run:

$ kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
deployments "nginx-deployment" revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
  Containers:
    nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      <none>
  No volumes.


c) Rolling Back to a Previous Revision
======================================

Follow the steps given below to rollback the Deployment from the current 
version to the previous version, which is version 2.

Now you’ve decided to undo the current rollout and rollback to the previous 
revision:

$ kubectl rollout undo deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment

Alternatively, you can rollback to a specific revision by specifying it with 
--to-revision:

$ kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2
deployment.apps/nginx-deployment

For more details about rollout related commands, read kubectl rollout.

The Deployment is now rolled back to a previous stable revision. As you can 
see, a DeploymentRollback event for rolling back to revision 2 is generated 
from Deployment controller.


To check if the rollback was successful and the Deployment is running as expected, run:

$ kubectl get deployment nginx-deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m

Get the description of the Deployment:

$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment "nginx-deployment" to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0


2.5 - Scaling a Deployment
==========================

You can scale a Deployment by using the following command:

$ kubectl scale deployment.v1.apps/nginx-deployment --replicas=10
deployment.apps/nginx-deployment scaled

Assuming horizontal Pod autoscaling is enabled in your cluster, you can setup 
an autoscaler for your Deployment and choose the minimum and maximum number of 
Pods you want to run based on the CPU utilization of your existing Pods.

$ kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80
deployment.apps/nginx-deployment scaled

a) Proportional scaling

RollingUpdate Deployments support running multiple versions of an application 
at the same time. When you or an autoscaler scales a RollingUpdate Deployment 
that is in the middle of a rollout (either in progress or paused), the 
Deployment controller balances the additional replicas in the existing active 
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called 
proportional scaling.

For example, you are running a Deployment with 10 replicas, maxSurge=3, and 
maxUnavailable=2.
Ensure that the 10 replicas in your Deployment are running.

$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s

You update to a new image which happens to be unresolvable from inside the 
cluster.

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:sometag
deployment.apps/nginx-deployment image updated

The image update starts a new rollout with ReplicaSet 
nginx-deployment-1989198191, but it’s blocked due to the maxUnavailable 
requirement that you mentioned above. Check out the rollout status:

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m

Then a new scaling request for the Deployment comes along. The autoscaler 
increments the Deployment replicas to 15. The Deployment controller needs to 
decide where to add these new 5 replicas. If you weren’t using proportional 
scaling, all 5 of them would be added in the new ReplicaSet. With proportional 
scaling, you spread the additional replicas across all ReplicaSets. Bigger 
proportions go to the ReplicaSets with the most replicas and lower proportions 
go to ReplicaSets with less replicas. Any leftovers are added to the 
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not 
scaled up.

In our example above, 3 replicas are added to the old ReplicaSet and 2 
replicas are added to the new ReplicaSet. The rollout process should 
eventually move all replicas to the new ReplicaSet, assuming the new replicas 
become healthy. To confirm this, run:

$ kubectl get deploy
NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m

The rollout status confirms how the replicas were added to each ReplicaSet.

$ kubectl get rs
NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m


2.6 - Pausing and Resuming a Deployment
=======================================

You can pause a Deployment before triggering one or more updates and then 
resume it. This allows you to apply multiple fixes in between pausing and 
resuming without triggering unnecessary rollouts.

For example, with a Deployment that was just created: Get the Deployment details:

$ kubectl get deploy
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m

Get the rollout status:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m

Pause by running the following command:

$ kubectl rollout pause deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment paused

Then update the image of the Deployment:

$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1
deployment.apps/nginx-deployment image updated

Notice that no new rollout started:

$ kubectl rollout history deployment.v1.apps/nginx-deployment
deployments "nginx"
REVISION  CHANGE-CAUSE
1   <none>

Get the rollout status to ensure that the Deployment is updated successfully:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m

You can make as many updates as you wish, for example, update the resources 
that will be used:

$ kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
deployment.apps/nginx-deployment resource requirements updated

The initial state of the Deployment prior to pausing it will continue its 
function, but new updates to the Deployment will not have any effect as long 
as the Deployment is paused.

Eventually, resume the Deployment and observe a new ReplicaSet coming up with 
all the new updates:

$ kubectl rollout resume deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment resumed

Watch the status of the rollout until it’s done.

$ kubectl get rs -w
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s

Get the status of the latest rollout:

$ kubectl get rs
NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s

    Note: You cannot rollback a paused Deployment until you resume it.


2.7 - Deployment status
=======================

A Deployment enters various states during its lifecycle. It can be progressing 
while rolling out a new ReplicaSet, it can be complete, or it can fail to 
progress.


a) Progressing Deployment
=========================

Kubernetes marks a Deployment as progressing when one of the following tasks 
is performed:

    - The Deployment creates a new ReplicaSet.
    - The Deployment is scaling up its newest ReplicaSet.
    - The Deployment is scaling down its older ReplicaSet(s).
    - New Pods become ready or available (ready for at least MinReadySeconds).

You can monitor the progress for a Deployment by using kubectl rollout status.


b) Complete Deployment
======================

Kubernetes marks a Deployment as complete when it has the following 
characteristics:

    - All of the replicas associated with the Deployment have been updated to 
      the latest version you’ve specified, meaning any updates you’ve 
      requested have been completed.
    - All of the replicas associated with the Deployment are available.
    - No old replicas for the Deployment are running.

You can check if a Deployment has completed by using kubectl rollout status. 
If the rollout completed successfully, kubectl rollout status returns a zero 
exit code.

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment.apps/nginx-deployment successfully rolled out
$ echo $?
0

c) Failed Deployment
====================

Your Deployment may get stuck trying to deploy its newest ReplicaSet without 
ever completing. This can occur due to some of the following factors:

    - Insufficient quota
    - Readiness probe failures
    - Image pull errors
    - Insufficient permissions
    - Limit ranges
    - Application runtime misconfiguration

One way you can detect this condition is to specify a deadline parameter in 
your Deployment spec: (.spec.progressDeadlineSeconds).
.spec.progressDeadlineSeconds denotes the number of seconds the Deployment 
controller waits before indicating (in the Deployment status) that the 
Deployment progress has stalled.

The following kubectl command sets the spec with progressDeadlineSeconds to 
make the controller report lack of progress for a Deployment after 10 minutes:

$ kubectl patch deployment.v1.apps/nginx-deployment -p \
    '{"spec":{"progressDeadlineSeconds":600}}'
deployment.apps/nginx-deployment patched

Once the deadline has been exceeded, the Deployment controller adds a 
DeploymentCondition with the following attributes to the Deployment’s 
.status.conditions:

    Type=Progressing
    Status=False
    Reason=ProgressDeadlineExceeded

See the Kubernetes API conventions for more information on status conditions.

    Note: Kubernetes takes no action on a stalled Deployment other than to 
      report a status condition with Reason=ProgressDeadlineExceeded. Higher 
      level orchestrators can take advantage of it and act accordingly, for 
      example, rollback the Deployment to its previous version.

    Note: If you pause a Deployment, Kubernetes does not check progress 
      against your specified deadline. You can safely pause a Deployment in 
      the middle of a rollout and resume without triggering the condition for 
      exceeding the deadline.

You may experience transient errors with your Deployments, either due to a 
low timeout that you have set or due to any other kind of error that can be 
treated as transient. For example, let’s suppose you have insufficient quota. 
If you describe the Deployment you will notice the following section:

$ kubectl describe deployment nginx-deployment
<...>
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
<...>

Run the following command:

$ kubectl get deployment nginx-deployment -o yaml
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set "nginx-deployment-4262182780" is progressing.
    reason: ReplicaSetUpdated
    status: "True"
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: "True"
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: 'Error creating: pods "nginx-deployment-4262182780-" is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2'
    reason: FailedCreate
    status: "True"
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2

Eventually, once the Deployment progress deadline is exceeded, Kubernetes 
updates the status and the reason for the Progressing condition:

Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate

You can address an issue of insufficient quota by scaling down your Deployment, 
by scaling down other controllers you may be running, or by increasing quota 
in your namespace. If you satisfy the quota conditions and the Deployment 
controller then completes the Deployment rollout, you’ll see the Deployment’s 
status update with a successful condition 
(Status=True and Reason=NewReplicaSetAvailable).

Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable

Type=Available with Status=True means that your Deployment has minimum 
availability. Minimum availability is dictated by the parameters specified in 
the deployment strategy. Type=Progressing with Status=True means that your 
Deployment is either in the middle of a rollout and it is progressing or that 
it has successfully completed its progress and the minimum required new 
replicas are available (see the Reason of the condition for the particulars - 
in our case Reason=NewReplicaSetAvailable means that the Deployment is 
complete).

You can check if a Deployment has failed to progress by using the command 
'kubectl rollout status': it returns a non-zero exit code if the Deployment 
has exceeded the progression deadline.

$ kubectl rollout status deployment.v1.apps/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment "nginx" exceeded its progress deadline
$ echo $?
1

d) Operating on a failed deployment
===================================

All actions that apply to a complete Deployment also apply to a failed 
Deployment. You can scale it up/down, roll back to a previous revision, or 
even pause it if you need to apply multiple tweaks in the Deployment Pod 
template.


2.8 - Clean up Policy
=====================

You can set .spec.revisionHistoryLimit field in a Deployment to specify how 
many old ReplicaSets for this Deployment you want to retain. The rest will be 
garbage-collected in the background. By default, it is 10.

    Note: Explicitly setting this field to 0, will result in cleaning up all 
      the history of your Deployment thus that Deployment will not be able to 
      roll back.


2.9 - Canary Deployment
=======================

If you want to roll out releases to a subset of users or servers using the 
Deployment, you can create multiple Deployments, one for each release, 
following the canary pattern described in managing resources.



================
3 - StatefulSets
================


StatefulSet is the workload API object used to manage stateful applications.

Manages the deployment and scaling of a set of Pods , and provides guarantees 
about the ordering and uniqueness of these Pods.

Like a Deployment, a StatefulSet manages Pods that are based on an identical 
container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity 
for each of their Pods. These pods are created from the same spec, but are not 
interchangeable: each has a persistent identifier that it maintains across any 
rescheduling.


3.1 - Using StatefulSets
========================

StatefulSets are valuable for applications that require one or more of the 
following.

    - Stable, unique network identifiers.
    - Stable, persistent storage.
    - Ordered, graceful deployment and scaling.
    - Ordered, automated rolling updates.

In the above, stable is synonymous with persistence across Pod (re)scheduling. 
If an application doesn’t require any stable identifiers or ordered deployment, 
deletion, or scaling, you should deploy your application using a workload 
object that provides a set of stateless replicas. Deployment or ReplicaSet may 
be better suited to your stateless needs.


3.2 - Limitations
=================

    - The storage for a given Pod must either be provisioned by a 
      PersistentVolume Provisioner based on the requested storage class, or 
      pre-provisioned by an admin.
    - Deleting and/or scaling a StatefulSet down will not delete the volumes 
      associated with the StatefulSet. This is done to ensure data safety, 
      which is generally more valuable than an automatic purge of all related 
      StatefulSet resources.
    - StatefulSets currently require a Headless Service to be responsible for 
      the network identity of the Pods. You are responsible for creating this 
      Service.
    - StatefulSets do not provide any guarantees on the termination of pods 
      when a StatefulSet is deleted. To achieve ordered and graceful 
      termination of the pods in the StatefulSet, it is possible to scale the 
      StatefulSet down to 0 prior to deletion.
    - When using Rolling Updates with the default Pod Management Policy 
      (OrderedReady), it’s possible to get into a broken state that requires 
      manual intervention to repair.


3.3 - Components
================

The example below demonstrates the components of a StatefulSet.

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi

In the above example:

    - A Headless Service, named nginx, is used to control the network domain.
    - The StatefulSet, named web, has a Spec that indicates that 3 replicas of 
      the nginx container will be launched in unique Pods.
    - The volumeClaimTemplates will provide stable storage using 
      PersistentVolumes provisioned by a PersistentVolume Provisioner.


3.4 - Pod Selector
==================

You must set the .spec.selector field of a StatefulSet to match the labels of 
its .spec.template.metadata.labels. Prior to Kubernetes 1.8, the 
.spec.selector field was defaulted when omitted. In 1.8 and later versions, 
failing to specify a matching Pod Selector will result in a validation error 
during StatefulSet creation.


3.5 - Pod Identity
==================

StatefulSet Pods have a unique identity that is comprised of an ordinal, a 
stable network identity, and stable storage. The identity sticks to the Pod, 
regardless of which node it’s (re)scheduled on.


a) Ordinal Index
================

For a StatefulSet with N replicas, each Pod in the StatefulSet will be 
assigned an integer ordinal, from 0 up through N-1, that is unique over the 
Set.


b) Stable Network ID
====================

Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet 
and the ordinal of the Pod. The pattern for the constructed hostname is 
$(statefulset name)-$(ordinal). The example above will create three Pods named 
web-0,web-1,web-2. A StatefulSet can use a Headless Service to control the 
domain of its Pods. The domain managed by this Service takes the form: 
$(service name).$(namespace).svc.cluster.local, where “cluster.local” is the 
cluster domain. As each Pod is created, it gets a matching DNS subdomain, 
taking the form: $(podname).$(governing service domain), where the governing 
service is defined by the serviceName field on the StatefulSet.

As mentioned in the limitations section, you are responsible for creating the 
Headless Service responsible for the network identity of the pods.

Here are some examples of choices for Cluster Domain, Service name, 
StatefulSet name, and how that affects the DNS names for the StatefulSet’s Pods.
Cluster Domain	Service (ns/name)	StatefulSet (ns/name)	                     StatefulSet Domain	Pod DNS	                      Pod Hostname
cluster.local	default/nginx	    default/web	nginx.default.svc.cluster.local	 web-{0..N-1}.nginx.default.svc.cluster.local	  web-{0..N-1}
cluster.local	foo/nginx	        foo/web	nginx.foo.svc.cluster.local	         web-{0..N-1}.nginx.foo.svc.cluster.local	      web-{0..N-1}
kube.local	    foo/nginx	        foo/web	nginx.foo.svc.kube.local	         web-{0..N-1}.nginx.foo.svc.kube.local	          web-{0..N-1}

    Note: Cluster Domain will be set to cluster.local unless otherwise 
      configured.


c) Stable Storage
=================

Kubernetes creates one PersistentVolume for each VolumeClaimTemplate. In the 
nginx example above, each Pod will receive a single PersistentVolume with a 
StorageClass of my-storage-class and 1 Gib of provisioned storage. If no 
StorageClass is specified, then the default StorageClass will be used. When a 
Pod is (re)scheduled onto a node, its volumeMounts mount the PersistentVolumes 
associated with its PersistentVolume Claims. Note that, the PersistentVolumes 
associated with the Pods’ PersistentVolume Claims are not deleted when the 
Pods, or StatefulSet are deleted. This must be done manually.


d) Pod Name Label
=================

When the StatefulSet Controller creates a Pod, it adds a label, 
statefulset.kubernetes.io/pod-name, that is set to the name of the Pod. This 
label allows you to attach a Service to a specific Pod in the StatefulSet.


3.6 - Deployment and Scaling Guarantees
=======================================

    - For a StatefulSet with N replicas, when Pods are being deployed, they 
      are created sequentially, in order from {0..N-1}.
    - When Pods are being deleted, they are terminated in reverse order, 
      from {N-1..0}.
    - Before a scaling operation is applied to a Pod, all of its predecessors 
      must be Running and Ready.
    - Before a Pod is terminated, all of its successors must be completely 
      shutdown.

The StatefulSet should not specify a pod.Spec.TerminationGracePeriodSeconds of 
0. This practice is unsafe and strongly discouraged. For further explanation, 
please refer to force deleting StatefulSet Pods.

When the nginx example above is created, three Pods will be deployed in the 
order web-0, web-1, web-2. web-1 will not be deployed before web-0 is Running 
and Ready, and web-2 will not be deployed until web-1 is Running and Ready. 
If web-0 should fail, after web-1 is Running and Ready, but before web-2 is 
launched, web-2 will not be launched until web-0 is successfully relaunched 
and becomes Running and Ready.

If a user were to scale the deployed example by patching the StatefulSet such 
that replicas=1, web-2 would be terminated first. web-1 would not be terminated 
until web-2 is fully shutdown and deleted. If web-0 were to fail after web-2 
has been terminated and is completely shutdown, but prior to web-1’s 
termination, web-1 would not be terminated until web-0 is Running and Ready.


3.7 - Update Strategies
=======================

In Kubernetes 1.7 and later, StatefulSet’s .spec.updateStrategy field allows 
you to configure and disable automated rolling updates for containers, labels, 
resource request/limits, and annotations for the Pods in a StatefulSet.


On Delete
=========

The OnDelete update strategy implements the legacy (1.6 and prior) behavior. 
When a StatefulSet’s .spec.updateStrategy.type is set to OnDelete, the 
StatefulSet controller will not automatically update the Pods in a StatefulSet. 
Users must manually delete Pods to cause the controller to create new Pods that
reflect modifications made to a StatefulSet’s .spec.template.


Rolling Updates
===============

The RollingUpdate update strategy implements automated, rolling update for the 
Pods in a StatefulSet. It is the default strategy when .spec.updateStrategy is 
left unspecified. When a StatefulSet’s .spec.updateStrategy.type is set to 
RollingUpdate, the StatefulSet controller will delete and recreate each Pod in 
the StatefulSet. It will proceed in the same order as Pod termination (from 
the largest ordinal to the smallest), updating each Pod one at a time. It will 
wait until an updated Pod is Running and Ready prior to updating its 
predecessor.


Partitions
==========

The RollingUpdate update strategy can be partitioned, by specifying a 
.spec.updateStrategy.rollingUpdate.partition. If a partition is specified, all 
Pods with an ordinal that is greater than or equal to the partition will be 
updated when the StatefulSet’s .spec.template is updated. All Pods with an 
ordinal that is less than the partition will not be updated, and, even if they 
are deleted, they will be recreated at the previous version. If a StatefulSet’s 
.spec.updateStrategy.rollingUpdate.partition is greater than its 
.spec.replicas, updates to its .spec.template will not be propagated to its 
Pods. In most cases you will not need to use a partition, but they are useful 
if you want to stage an update, roll out a canary, or perform a phased roll 
out.


Forced Rollback
===============

When using Rolling Updates with the default Pod Management Policy 
(OrderedReady), it’s possible to get into a broken state that requires manual 
intervention to repair.

If you update the Pod template to a configuration that never becomes Running 
and Ready (for example, due to a bad binary or application-level configuration 
error), StatefulSet will stop the rollout and wait.

In this state, it’s not enough to revert the Pod template to a good 
configuration. Due to a known issue, StatefulSet will continue to wait for the 
broken Pod to become Ready (which never happens) before it will attempt to 
revert it back to the working configuration.

After reverting the template, you must also delete any Pods that StatefulSet 
had already attempted to run with the bad configuration. StatefulSet will then 
begin to recreate the Pods using the reverted template.


============
4- DaemonSet
============


A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are 
added to the cluster, Pods are added to them. As nodes are removed from the 
cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up 
the Pods it created.

Some typical uses of a DaemonSet are:

    - running a cluster storage daemon, such as glusterd, ceph, on each node.
    - running a logs collection daemon on every node, such as fluentd or 
      logstash.
    - running a node monitoring daemon on every node, such as Prometheus Node 
      Exporter, Flowmill, Sysdig Agent, collectd, Dynatrace OneAgent, 
      AppDynamics Agent, Datadog agent, New Relic agent, Ganglia gmond or 
      Instana Agent.

In a simple case, one DaemonSet, covering all nodes, would be used for each 
type of daemon. A more complex setup might use multiple DaemonSets for a 
single type of daemon, but with different flags and/or different memory and 
cpu requests for different hardware types.

    - Writing a DaemonSet Spec
    - How Daemon Pods are Scheduled
    - Communicating with Daemon Pods
    - Updating a DaemonSet
    - Alternatives to DaemonSet

a) Writing a DaemonSet Spec
===========================

Create a DaemonSet

You can describe a DaemonSet in a YAML file. For example, the daemonset.yaml 
file below describes a DaemonSet that runs the fluentd-elasticsearch Docker 
image:

File: controllers/daemonset.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

Create a DaemonSet based on the YAML file:

$ kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml


b) Required Fields
==================

As with all other Kubernetes config, a DaemonSet needs apiVersion, kind, and 
metadata fields. For general information about working with config files, see 
deploying applications, configuring containers, and object management using 
kubectl documents.

A DaemonSet also needs a .spec section.


Pod Template
============

The .spec.template is one of the required fields in .spec.

The .spec.template is a pod template. It has exactly the same schema as a Pod, 
except it is nested and does not have an apiVersion or kind.

In addition to required fields for a Pod, a Pod template in a DaemonSet has to 
specify appropriate labels (see pod selector).

A Pod Template in a DaemonSet must have a RestartPolicy equal to Always, or 
be unspecified, which defaults to Always.


Pod Selector
============

The .spec.selector field is a pod selector. It works the same as the 
.spec.selector of a Job.

As of Kubernetes 1.8, you must specify a pod selector that matches the labels 
of the .spec.template. The pod selector will no longer be defaulted when left 
empty. Selector defaulting was not compatible with kubectl apply. Also, once a 
DaemonSet is created, its .spec.selector can not be mutated. Mutating the pod 
selector can lead to the unintentional orphaning of Pods, and it was found to 
be confusing to users.

The .spec.selector is an object consisting of two fields:

    - matchLabels - works the same as the .spec.selector of a 
      ReplicationController.
    - matchExpressions - allows to build more sophisticated selectors by 
      specifying key, list of values and an operator that relates the key and 
      values.

When the two are specified the result is ANDed.

If the .spec.selector is specified, it must match the 
.spec.template.metadata.labels. Config with these not matching will be rejected 
by the API.

Also you should not normally create any Pods whose labels match this selector, 
either directly, via another DaemonSet, or via another workload resource such 
as ReplicaSet. Otherwise, the DaemonSet Controller will think that those Pods 
were created by it. Kubernetes will not stop you from doing this. One case 
where you might want to do this is manually create a Pod with a different value 
on a node for testing.


Running Pods on Only Some Nodes
===============================

If you specify a .spec.template.spec.nodeSelector, then the DaemonSet 
controller will create Pods on nodes which match that node selector. Likewise 
if you specify a .spec.template.spec.affinity, then DaemonSet controller will 
create Pods on nodes which match that node affinity. If you do not specify 
either, then the DaemonSet controller will create Pods on all nodes.


How Daemon Pods are Scheduled
=============================
Scheduled by default scheduler

FEATURE STATE: Kubernetes v1.17 stable

A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the 
node that a Pod runs on is selected by the Kubernetes scheduler. However, 
DaemonSet pods are created and scheduled by the DaemonSet controller instead. 
That introduces the following issues:

    - Inconsistent Pod behavior: Normal Pods waiting to be scheduled are 
      created and in Pending state, but DaemonSet pods are not created in 
      Pending state. This is confusing to the user.
    - Pod preemption is handled by default scheduler. When preemption is 
      enabled, the DaemonSet controller will make scheduling decisions without 
      considering pod priority and preemption.

ScheduleDaemonSetPods allows you to schedule DaemonSets using the default 
scheduler instead of the DaemonSet controller, by adding the NodeAffinity term 
to the DaemonSet pods, instead of the .spec.nodeName term. The default 
scheduler is then used to bind the pod to the target host. If node affinity of 
the DaemonSet pod already exists, it is replaced. The DaemonSet controller only 
performs these operations when creating or modifying DaemonSet pods, and no 
changes are made to the spec.template of the DaemonSet.

nodeAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchFields:
      - key: metadata.name
        operator: In
        values:
        - target-host-name

In addition, node.kubernetes.io/unschedulable:NoSchedule toleration is added 
automatically to DaemonSet Pods. The default scheduler ignores unschedulable 
Nodes when scheduling DaemonSet Pods.


c) Taints and Tolerations
=========================

Although Daemon Pods respect taints and tolerations, the following tolerations 
are added to DaemonSet Pods automatically according to the related features.

Toleration Key	                         Effect	     Version	Description
node.kubernetes.io/not-ready	         NoExecute	 1.13+	    DaemonSet pods will not be evicted when there are node problems such as a network partition.
node.kubernetes.io/unreachable	         NoExecute	 1.13+	    DaemonSet pods will not be evicted when there are node problems such as a network partition.
node.kubernetes.io/disk-pressure	     NoSchedule	 1.8+	
node.kubernetes.io/memory-pressure	     NoSchedule	 1.8+	
node.kubernetes.io/unschedulable	     NoSchedule	 1.12+	    DaemonSet pods tolerate unschedulable attributes by default scheduler.
node.kubernetes.io/network-unavailable	 NoSchedule	 1.12+	    DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.


d) Communicating with Daemon Pods
=================================

Some possible patterns for communicating with Pods in a DaemonSet are:

    - Push: Pods in the DaemonSet are configured to send updates to another 
      service, such as a stats database. They do not have clients.
    - NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that 
      the pods are reachable via the node IPs. Clients know the list of node 
      IPs somehow, and know the port by convention.
    - DNS: Create a headless service with the same pod selector, and then 
      discover DaemonSets using the endpoints resource or retrieve multiple A 
      records from DNS.
    - Service: Create a service with the same Pod selector, and use the service 
      to reach a daemon on a random node. (No way to reach specific node.)


e) Updating a DaemonSet
=======================

If node labels are changed, the DaemonSet will promptly add Pods to newly 
matching nodes and delete Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates. However, Pods do not allow 
all fields to be updated. Also, the DaemonSet controller will use the original 
template the next time a node (even with the same name) is created.

You can delete a DaemonSet. If you specify --cascade=false with kubectl, then 
the Pods will be left on the nodes. If you subsequently create a new DaemonSet 
with the same selector, the new DaemonSet adopts the existing Pods. If any Pods 
need replacing the DaemonSet replaces them according to its updateStrategy.

You can perform a rolling update on a DaemonSet.


f) Alternatives to DaemonSet
============================


Init Scripts
============

It is certainly possible to run daemon processes by directly starting them on a 
node (e.g. using init, upstartd, or systemd). This is perfectly fine. However, 
there are several advantages to running such processes via a DaemonSet:

    - Ability to monitor and manage logs for daemons in the same way as 
      applications.
    - Same config language and tools (e.g. Pod templates, kubectl) for daemons 
      and applications.
    - Running daemons in containers with resource limits increases isolation 
      between daemons from app containers. However, this can also be 
      accomplished by running the daemons in a container but not in a Pod (e.g. 
      start directly via Docker).


Bare Pods
=========

It is possible to create Pods directly which specify a particular node to run 
on. However, a DaemonSet replaces Pods that are deleted or terminated for any 
reason, such as in the case of node failure or disruptive node maintenance, 
such as a kernel upgrade. For this reason, you should use a DaemonSet rather 
than creating individual Pods.


Static Pods
===========

It is possible to create Pods by writing a file to a certain directory watched 
by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot 
be managed with kubectl or other Kubernetes API clients. Static Pods do not 
depend on the apiserver, making them useful in cluster bootstrapping cases. 
Also, static Pods may be deprecated in the future.


Deployments
===========

DaemonSets are similar to Deployments in that they both create Pods, and those 
Pods have processes which are not expected to terminate (e.g. web servers, 
storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and 
down the number of replicas and rolling out updates are more important than 
controlling exactly which host the Pod runs on. Use a DaemonSet when it is 
important that a copy of a Pod always run on all or certain hosts, and when it 
needs to start before other Pods.







===============================================================================
===============================================================================

APPENDIX - STATEFULSET

===============================================================================
===============================================================================




StatefulSets are intended to be used with stateful applications and distributed 
systems. However, the administration of stateful applications and distributed 
systems on Kubernetes is a broad, complex topic. In order to demonstrate the 
basic features of a StatefulSet, and not to conflate the former topic with the 
latter, you will deploy a simple web application using a StatefulSet.

After this tutorial, you will be familiar with the following.

    How to create a StatefulSet
    How a StatefulSet manages its Pods
    How to delete a StatefulSet
    How to scale a StatefulSet
    How to update a StatefulSet’s Pods

0 - Before you begin
====================

Before you begin this tutorial, you should familiarize yourself with the 
following Kubernetes concepts.

    Pods
    Cluster DNS
    Headless Services
    PersistentVolumes
    PersistentVolume Provisioning
    StatefulSets
    kubectl CLI

This tutorial assumes that your cluster is configured to dynamically provision 
PersistentVolumes. If your cluster is not configured to do so, you will have to 
manually provision two 1 GiB volumes prior to starting this tutorial.

1 - Creating a StatefulSet
==========================

Begin by creating a StatefulSet using the example below. It is similar to the 
example presented in the StatefulSets concept. It creates a Headless Service, 
nginx, to publish the IP addresses of Pods in the StatefulSet, web.

(file: application/web/web.yaml)

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


Save the example above to a file named web.yaml

You will need to use two terminal windows. In the first terminal, use 'kubectl 
get' to watch the creation of the StatefulSet’s Pods.

$ kubectl get pods -w -l app=nginx

In the second terminal, use 'kubectl apply' to create the Headless Service and 
StatefulSet defined in web.yaml.

$ kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

The command above creates two Pods, each running an NGINX webserver. Get the 
nginx Service and the web StatefulSet to verify that they were created 
successfully.

$ kubectl get service nginx
NAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
nginx     ClusterIP    None         <none>        80/TCP    12s

$ kubectl get statefulset web
NAME      DESIRED   CURRENT   AGE
web       2         1         20s


1.1 - Ordered Pod Creation
==========================

For a StatefulSet with N replicas, when Pods are being deployed, they are 
created sequentially, in order from {0..N-1}. Examine the output of the kubectl 
get command in the first terminal. Eventually, the output will look like the 
example below.

$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       Pending             0          0s
web-0     0/1       Pending             0          0s
web-0     0/1       ContainerCreating   0          0s
web-0     1/1       Running             0          19s
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       ContainerCreating   0          0s
web-1     1/1       Running             0          18s

Notice that the web-1 Pod is not launched until the web-0 Pod is Running and 
Ready.


1.2 - Examining the Pod’s Ordinal Index
=======================================

Pods in a StatefulSet have a unique ordinal index and a stable network identity.
Get the StatefulSet’s Pods.

$ kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          1m
web-1     1/1       Running   0          1m

As mentioned in the StatefulSets concept, the Pods in a StatefulSet have a 
sticky, unique identity. This identity is based on a unique ordinal index that 
is assigned to each Pod by the StatefulSet controller. The Pods’ names take the 
form <statefulset name>-<ordinal index>. Since the web StatefulSet has two 
replicas, it creates two Pods, web-0 and web-1.


1.3 - Using Stable Network Identities
=====================================

Each Pod has a stable hostname based on its ordinal index. Use 'kubectl exec' 
to execute the hostname command in each Pod.

$ for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

Use 'kubectl run' to execute a container that provides the nslookup command 
from the dnsutils package. Using nslookup on the Pods’ hostnames, you can 
examine their in-cluster DNS addresses.

$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm  
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.6

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.6

The CNAME of the headless service points to SRV records (one for each Pod that 
is Running and Ready). The SRV records point to A record entries that contain 
the Pods’ IP addresses.

In one terminal, watch the StatefulSet’s Pods.

$ kubectl get pod -w -l app=nginx

In a second terminal, use 'kubectl delete' to delete all the Pods in the 
StatefulSet.

$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Wait for the StatefulSet to restart them, and for both Pods to transition to 
Running and Ready.

$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
[...]
$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          2s
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       ContainerCreating   0          0s
web-1     1/1       Running             0          34s

Use 'kubectl exec' and 'kubectl run' to view the Pods hostnames and in-cluster 
DNS entries.

$ for i in 0 1; do kubectl exec web-$i -- sh -c 'hostname'; done
web-0
web-1

$ kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm /bin/sh 
nslookup web-0.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx
Address 1: 10.244.1.7

nslookup web-1.nginx
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx
Address 1: 10.244.2.8

The Pods’ ordinals, hostnames, SRV records, and A record names have not 
changed, but the IP addresses associated with the Pods may have changed. In the 
cluster used for this tutorial, they have. This is why it is important not to 
configure other applications to connect to Pods in a StatefulSet by IP address.

If you need to find and connect to the active members of a StatefulSet, you 
should query the CNAME of the Headless Service 
(nginx.default.svc.cluster.local). The SRV records associated with the CNAME 
will contain only the Pods in the StatefulSet that are Running and Ready.

If your application already implements connection logic that tests for liveness 
and readiness, you can use the SRV records of the Pods 
(web-0.nginx.default.svc.cluster.local, web-1.nginx.default.svc.cluster.local), 
as they are stable, and your application will be able to discover the Pods’ 
addresses when they transition to Running and Ready.
Writing to Stable Storage


1.4 - Get the PersistentVolumeClaims for web-0 and web-1
========================================================

$ kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s

The StatefulSet controller created two PersistentVolumeClaims that are bound to 
two PersistentVolumes. As the cluster used in this tutorial is configured to 
dynamically provision PersistentVolumes, the PersistentVolumes were created and 
bound automatically.

The NGINX webservers, by default, will serve an index file at 
/usr/share/nginx/html/index.html. The volumeMounts field in the StatefulSets 
spec ensures that the /usr/share/nginx/html directory is backed by a 
PersistentVolume.

Write the Pods’ hostnames to their index.html files and verify that the NGINX 
webservers serve the hostnames.

$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done
$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

    Note:

    If you instead see 403 Forbidden responses for the above curl command, you 
    will need to fix the permissions of the directory mounted by the 
    volumeMounts (due to a bug when using hostPath volumes) with:

    $ for i in 0 1; do kubectl exec web-$i -- chmod 755 /usr/share/nginx/html; done

    before retrying the curl command above.

In one terminal, watch the StatefulSet’s Pods.

$ kubectl get pod -w -l app=nginx

In a second terminal, delete all of the StatefulSet’s Pods.

$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted

Examine the output of the 'kubectl get' command in the first terminal, and wait 
for all of the Pods to transition to Running and Ready.

$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
[...]
$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          2s
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       ContainerCreating   0          0s
web-1     1/1       Running             0          34s

Verify the web servers continue to serve their hostnames.

for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though web-0 and web-1 were rescheduled, they continue to serve their 
hostnames because the PersistentVolumes associated with their 
PersistentVolumeClaims are remounted to their volumeMounts. No matter what 
node web-0and web-1 are scheduled on, their PersistentVolumes will be mounted 
to the appropriate mount points.


2 - Scaling a StatefulSet
=========================

Scaling a StatefulSet refers to increasing or decreasing the number of 
replicas. This is accomplished by updating the replicas field. You can use 
either 'kubectl scale' or 'kubectl patch' to scale a StatefulSet.

2.1 - Scaling Up
================

In one terminal window, watch the Pods in the StatefulSet.

$ kubectl get pods -w -l app=nginx

In another terminal window, use kubectl scale to scale the number of replicas 
to 5.

$ kubectl scale sts web --replicas=5
statefulset.apps/web scaled

Examine the output of the 'kubectl get' command in the first terminal, and 
wait for the three additional Pods to transition to Running and Ready.

$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2h
web-1     1/1       Running   0          2h
[...]
$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-2     0/1       Pending             0          0s
web-2     0/1       Pending             0          0s
web-2     0/1       ContainerCreating   0          0s
web-2     1/1       Running             0          19s
web-3     0/1       Pending             0          0s
web-3     0/1       Pending             0          0s
web-3     0/1       ContainerCreating   0          0s
web-3     1/1       Running             0          18s
web-4     0/1       Pending             0          0s
web-4     0/1       Pending             0          0s
web-4     0/1       ContainerCreating   0          0s
web-4     1/1       Running             0          19s

The StatefulSet controller scaled the number of replicas. As with StatefulSet 
creation, the StatefulSet controller created each Pod sequentially with 
respect to its ordinal index, and it waited for each Pod’s predecessor to be 
Running and Ready before launching the subsequent Pod.


2.2 - Scaling Down
==================

In one terminal, watch the StatefulSet’s Pods.

$ kubectl get pods -w -l app=nginx

In another terminal, use 'kubectl patch' to scale the StatefulSet back down to 
three replicas.

$ kubectl patch sts web -p '{"spec":{"replicas":3}}'
statefulset.apps/web patched

Wait for web-4 and web-3 to transition to Terminating.

$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3h
web-1     1/1       Running             0          3h
web-2     1/1       Running             0          55s
web-3     1/1       Running             0          36s
web-4     0/1       ContainerCreating   0          18s
[...]
$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS          RESTARTS   AGE
web-4     1/1       Running         0          19s
web-4     1/1       Terminating     0          24s
web-4     1/1       Terminating     0          24s
web-3     1/1       Terminating     0          42s
web-3     1/1       Terminating     0          42s


2.3 - Ordered Pod Termination
=============================

The controller deleted one Pod at a time, in reverse order with respect to its 
ordinal index, and it waited for each to be completely shutdown before deleting 
the next.


2.4 - Get the StatefulSet’s PersistentVolumeClaims
==================================================

$ kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-2   Bound     pvc-e1125b27-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-3   Bound     pvc-e1176df6-b508-11e6-932f-42010a800002   1Gi        RWO           13h
www-web-4   Bound     pvc-e11bb5f8-b508-11e6-932f-42010a800002   1Gi        RWO           13h

There are still five PersistentVolumeClaims and five PersistentVolumes. When 
exploring a Pod’s stable storage, we saw that the PersistentVolumes mounted to 
the Pods of a StatefulSet are not deleted when the StatefulSet’s Pods are 
deleted. This is still true when Pod deletion is caused by scaling the 
StatefulSet down.


3 - Updating StatefulSets
=========================

In Kubernetes 1.7 and later, the StatefulSet controller supports automated 
updates. The strategy used is determined by the spec.updateStrategy field of 
the StatefulSet API Object. This feature can be used to upgrade the container 
images, resource requests and/or limits, labels, and annotations of the Pods in 
a StatefulSet. There are two valid update strategies, RollingUpdate and 
OnDelete.

RollingUpdate update strategy is the default for StatefulSets.


3.1 - Rolling Update
====================

The RollingUpdate update strategy will update all Pods in a StatefulSet, in 
reverse ordinal order, while respecting the StatefulSet guarantees.

Patch the web StatefulSet to apply the RollingUpdate update strategy.

$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate"}}}'
statefulset.apps/web patched

In one terminal window, patch the web StatefulSet to change the container image 
again.

$ kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"gcr.io/google_containers/nginx-slim:0.8"}]'
statefulset.apps/web patched

In another terminal, watch the Pods in the StatefulSet.

$ kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          7m
web-1     1/1       Running             0          7m
web-2     1/1       Running             0          8m
web-2     1/1       Terminating         0          8m
web-2     1/1       Terminating         0          8m
web-2     0/1       Terminating         0          8m
web-2     0/1       Terminating         0          8m
web-2     0/1       Terminating         0          8m
web-2     0/1       Terminating         0          8m
web-2     0/1       Pending             0          0s
web-2     0/1       Pending             0          0s
web-2     0/1       ContainerCreating   0          0s
web-2     1/1       Running             0          19s
web-1     1/1       Terminating         0          8m
web-1     0/1       Terminating         0          8m
web-1     0/1       Terminating         0          8m
web-1     0/1       Terminating         0          8m
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       ContainerCreating   0          0s
web-1     1/1       Running             0          6s
web-0     1/1       Terminating         0          7m
web-0     1/1       Terminating         0          7m
web-0     0/1       Terminating         0          7m
web-0     0/1       Terminating         0          7m
web-0     0/1       Terminating         0          7m
web-0     0/1       Terminating         0          7m
web-0     0/1       Pending             0          0s
web-0     0/1       Pending             0          0s
web-0     0/1       ContainerCreating   0          0s
web-0     1/1       Running             0          10s

The Pods in the StatefulSet are updated in reverse ordinal order. The 
StatefulSet controller terminates each Pod, and waits for it to transition to 
Running and Ready prior to updating the next Pod. Note that, even though the 
StatefulSet controller will not proceed to update the next Pod until its 
ordinal successor is Running and Ready, it will restore any Pod that fails 
during the update to its current version. Pods that have already received the 
update will be restored to the updated version, and Pods that have not yet 
received the update will be restored to the previous version. In this way, the 
controller attempts to continue to keep the application healthy and the update 
consistent in the presence of intermittent failures.

Get the Pods to view their container images.

$ for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8
k8s.gcr.io/nginx-slim:0.8

All the Pods in the StatefulSet are now running the previous container image.

Tip You can also use 'kubectl rollout status sts/<name>' to view the status of 
a rolling update.


3.2 - Staging an Update
=======================

You can stage an update to a StatefulSet by using the partition parameter of 
the RollingUpdate update strategy. A staged update will keep all of the Pods in 
the StatefulSet at the current version while allowing mutations to the 
StatefulSet’s .spec.template.

Patch the web StatefulSet to add a partition to the updateStrategy field.

$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'
statefulset.apps/web patched

Patch the StatefulSet again to change the container’s image.

ù kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"k8s.gcr.io/nginx-slim:0.7"}]'
statefulset.apps/web patched

Delete a Pod in the StatefulSet.

$ kubectl delete po web-2
pod "web-2" deleted

Wait for the Pod to be Running and Ready.

$ kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

$ kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

Notice that, even though the update strategy is RollingUpdate, the StatefulSet 
controller restored the Pod with its original container. This is because the 
ordinal of the Pod is less than the partition specified by the updateStrategy.


3.3 - Rolling Out a Canary
==========================

You can roll out a canary to test a modification by decrementing the partition 
you specified above.

Patch the StatefulSet to decrement the partition.

$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'
statefulset.apps/web patched

Wait for web-2 to be Running and Ready.

$ kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s

Get the Pod’s container.

$ kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.7

When you changed the partition, the StatefulSet controller automatically 
updated the web-2 Pod because the Pod’s ordinal was greater than or equal to 
the partition.

Delete the web-1 Pod.

$ kubectl delete po web-1
pod "web-1" deleted

Wait for the web-1 Pod to be Running and Ready.

$ kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          6m
web-1     0/1       Terminating         0          6m
web-2     1/1       Running             0          2m
web-1     0/1       Terminating         0          6m
web-1     0/1       Terminating         0          6m
web-1     0/1       Terminating         0          6m
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       ContainerCreating   0          0s
web-1     1/1       Running             0          18s

Get the web-1 Pods container.

$ kubectl get po web-1 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
k8s.gcr.io/nginx-slim:0.8

web-1 was restored to its original configuration because the Pod’s ordinal was 
less than the partition. When a partition is specified, all Pods with an 
ordinal that is greater than or equal to the partition will be updated when 
the StatefulSet’s .spec.template is updated. If a Pod that has an ordinal less 
than the partition is deleted or otherwise terminated, it will be restored to 
its original configuration.


3.4 - Phased Roll Outs
======================

You can perform a phased roll out (e.g. a linear, geometric, or exponential 
roll out) using a partitioned rolling update in a similar manner to how you 
rolled out a canary. To perform a phased roll out, set the partition to the 
ordinal at which you want the controller to pause the update.

The partition is currently set to 2. Set the partition to 0.

$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":0}}}}'
statefulset.apps/web patched

Wait for all of the Pods in the StatefulSet to become Running and Ready.

$ kubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          3m
web-1     0/1       ContainerCreating   0          11s
web-2     1/1       Running             0          2m
web-1     1/1       Running             0          18s
web-0     1/1       Terminating         0          3m
web-0     1/1       Terminating         0          3m
web-0     0/1       Terminating         0          3m
web-0     0/1       Terminating         0          3m
web-0     0/1       Terminating         0          3m
web-0     0/1       Terminating         0          3m
web-0     0/1       Pending             0          0s
web-0     0/1       Pending             0          0s
web-0     0/1       ContainerCreating   0          0s
web-0     1/1       Running             0          3s

Get the Pod’s containers.

$ for p in 0 1 2; do kubectl get po web-$p --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7
k8s.gcr.io/nginx-slim:0.7

By moving the partition to 0, you allowed the StatefulSet controller to 
continue the update process.


3.5 - On Delete
===============

The OnDelete update strategy implements the legacy (1.6 and prior) behavior. 
When you select this update strategy, the StatefulSet controller will not 
automatically update Pods when a modification is made to the StatefulSet’s 
.spec.template field. This strategy can be selected by setting the 
.spec.template.updateStrategy.type to OnDelete.


4 - Deleting StatefulSets
=========================

StatefulSet supports both Non-Cascading and Cascading deletion. In a 
Non-Cascading Delete, the StatefulSet’s Pods are not deleted when the 
StatefulSet is deleted. In a Cascading Delete, both the StatefulSet and its 
Pods are deleted.


4.1 - Non-Cascading Delete
==========================

In one terminal window, watch the Pods in the StatefulSet.

$ kubectl get pods -w -l app=nginx

Use 'kubectl delete' to delete the StatefulSet. Make sure to supply the 
--cascade=false parameter to the command. This parameter tells Kubernetes to 
only delete the StatefulSet, and to not delete any of its Pods.

$ kubectl delete statefulset web --cascade=false
statefulset.apps "web" deleted

Get the Pods to examine their status.

$ kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          6m
web-1     1/1       Running   0          7m
web-2     1/1       Running   0          5m

Even though web has been deleted, all of the Pods are still Running and Ready. 
Delete web-0.

$ kubectl delete pod web-0
pod "web-0" deleted

Get the StatefulSet’s Pods.

$ kubectl get pods -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-1     1/1       Running   0          10m
web-2     1/1       Running   0          7m

As the web StatefulSet has been deleted, web-0 has not been relaunched.

In one terminal, watch the StatefulSet’s Pods.

$ kubectl get pods -w -l app=nginx

In a second terminal, recreate the StatefulSet. Note that, unless you deleted 
the nginx Service (which you should not have), you will see an error 
indicating that the Service already exists.

$ kubectl apply -f web.yaml
statefulset.apps/web created
service/nginx unchanged

Ignore the error. It only indicates that an attempt was made to create the 
nginx Headless Service even though that Service already exists.

Examine the output of the kubectl get command running in the first terminal.

$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-1     1/1       Running             0          16m
web-2     1/1       Running             0          2m
[...]
$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       Pending             0          0s
web-0     0/1       Pending             0         0s
web-0     0/1       ContainerCreating   0         0s
web-0     1/1       Running             0         18s
web-2     1/1       Terminating         0         3m
web-2     0/1       Terminating         0         3m
web-2     0/1       Terminating         0         3m
web-2     0/1       Terminating         0         3m

When the web StatefulSet was recreated, it first relaunched web-0. Since web-1 
was already Running and Ready, when web-0 transitioned to Running and Ready, it 
simply adopted this Pod. Since you recreated the StatefulSet with replicas 
equal to 2, once web-0 had been recreated, and once web-1 had been determined 
to already be Running and Ready, web-2 was terminated.

Let’s take another look at the contents of the index.html file served by the 
Pods’ webservers.

$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you deleted both the StatefulSet and the web-0 Pod, it still serves 
the hostname originally entered into its index.html file. This is because the 
StatefulSet never deletes the PersistentVolumes associated with a Pod. When you 
recreated the StatefulSet and it relaunched web-0, its original 
PersistentVolume was remounted.


4.2 - Cascading Delete
======================

In one terminal window, watch the Pods in the StatefulSet.

$ kubectl get pods -w -l app=nginx

In another terminal, delete the StatefulSet again. This time, omit the 
--cascade=false parameter.

$ kubectl delete statefulset web
statefulset.apps "web" deleted

Examine the output of the kubectl get command running in the first terminal, 
and wait for all of the Pods to transition to Terminating.

$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          11m
web-1     1/1       Running   0          27m
[...]
$ kubectl get pods -w -l app=nginx
NAME      READY     STATUS        RESTARTS   AGE
web-0     1/1       Terminating   0          12m
web-1     1/1       Terminating   0          29m
web-0     0/1       Terminating   0          12m
web-0     0/1       Terminating   0          12m
web-0     0/1       Terminating   0          12m
web-1     0/1       Terminating   0          29m
web-1     0/1       Terminating   0          29m
web-1     0/1       Terminating   0          29m

As you saw in the Scaling Down section, the Pods are terminated one at a time, 
with respect to the reverse order of their ordinal indices. Before terminating 
a Pod, the StatefulSet controller waits for the Pod’s successor to be 
completely terminated.

Note that, while a cascading delete will delete the StatefulSet and its Pods, 
it will not delete the Headless Service associated with the StatefulSet. You 
must delete the nginx Service manually.

$ kubectl delete service nginx
service "nginx" deleted

Recreate the StatefulSet and Headless Service one more time.

$ kubectl apply -f web.yaml
service/nginx created
statefulset.apps/web created

When all of the StatefulSet’s Pods transition to Running and Ready, retrieve 
the contents of their index.html files.

$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done
web-0
web-1

Even though you completely deleted the StatefulSet, and all of its Pods, the 
Pods are recreated with their PersistentVolumes mounted, and web-0 and web-1 
will still serve their hostnames.

Finally delete the web StatefulSet and the nginx service.

$ kubectl delete service nginx
service "nginx" deleted
$ kubectl delete statefulset web
statefulset "web" deleted


5 - Pod Management Policy
=========================

For some distributed systems, the StatefulSet ordering guarantees are 
unnecessary and/or undesirable. These systems require only uniqueness and 
identity. To address this, in Kubernetes 1.7, we introduced 
.spec.podManagementPolicy to the StatefulSet API Object.


5.1 - OrderedReady Pod Management
=================================

OrderedReady pod management is the default for StatefulSets. It tells the 
StatefulSet controller to respect the ordering guarantees demonstrated above.
Parallel Pod Management

Parallel pod management tells the StatefulSet controller to launch or terminate 
all Pods in parallel, and not to wait for Pods to become Running and Ready or 
completely terminated prior to launching or terminating another Pod.

(FILE) application/web/web-parallel.yaml

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  podManagementPolicy: "Parallel"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


Save the example aboveto a file named web-parallel.yaml

This manifest is identical to the one you downloaded above except that the 
.spec.podManagementPolicy of the web StatefulSet is set to Parallel.

In one terminal, watch the Pods in the StatefulSet.

$ kubectl get po -l app=nginx -w

In another terminal, create the StatefulSet and Service in the manifest.

$ kubectl apply -f web-parallel.yaml
service/nginx created
statefulset.apps/web created

Examine the output of the kubectl get command that you executed in the first 
terminal.

$ckubectl get po -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       Pending             0          0s
web-0     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-1     0/1       Pending             0          0s
web-0     0/1       ContainerCreating   0          0s
web-1     0/1       ContainerCreating   0          0s
web-0     1/1       Running             0          10s
web-1     1/1       Running             0          10s

The StatefulSet controller launched both web-0 and web-1 at the same time.

Keep the second terminal open, and, in another terminal window scale the StatefulSet.

$ kubectl scale statefulset/web --replicas=4
statefulset.apps/web scaled

Examine the output of the terminal where the kubectl get command is running.

web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         0s
web-3     0/1       Pending   0         7s
web-3     0/1       ContainerCreating   0         7s
web-2     1/1       Running   0         10s
web-3     1/1       Running   0         26s

The StatefulSet controller launched two new Pods, and it did not wait for the first to become Running and Ready prior to launching the second.

Keep this terminal open, and in another terminal delete the web StatefulSet.

kubectl delete sts web

Again, examine the output of the kubectl get command running in the other terminal.

web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-3     1/1       Terminating   0         9m
web-2     1/1       Terminating   0         9m
web-1     1/1       Terminating   0         44m
web-0     1/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-2     0/1       Terminating   0         9m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-1     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-0     0/1       Terminating   0         44m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m
web-3     0/1       Terminating   0         9m

The StatefulSet controller deletes all Pods concurrently, it does not wait for 
a Pod’s ordinal successor to terminate prior to deleting that Pod.

Close the terminal where the 'kubectl get' command is running and delete the 
nginx Service.

$ kubectl delete svc nginx


5.2 - Cleaning up
=================

You will need to delete the persistent storage media for the PersistentVolumes 
used in this tutorial. Follow the necessary steps, based on your environment, 
storage configuration, and provisioning method, to ensure that all storage is 
reclaimed.




==============================================================================
==============================================================================
Ingress
==============================================================================
==============================================================================


An API object that manages external access to the services in a cluster, 
typically HTTP.

Ingress can provide load balancing, SSL termination and name-based virtual 
hosting.


x.0 - Terminology
=================

For clarity, this guide defines the following terms:

Node
    A worker machine in Kubernetes, part of a cluster.
Cluster
    A set of Nodes that run containerized applications managed by Kubernetes. 
    For this example, and in most common Kubernetes deployments, nodes in the 
    cluster are not part of the public internet.
Edge router
    A router that enforces the firewall policy for your cluster. This could be 
    a gateway managed by a cloud provider or a physical piece of hardware.
Cluster network
    A set of links, logical or physical, that facilitate communication within 
    a cluster according to the Kubernetes networking model.
Service
    A Kubernetes Service that identifies a set of Pods using label selectors. 
    Unless mentioned otherwise, Services are assumed to have virtual IPs only 
    routable within the cluster network.


x.1 - What is Ingress?
======================

Ingress exposes HTTP and HTTPS routes from outside the cluster to services 
within the cluster. Traffic routing is controlled by rules defined on the 
Ingress resource.

    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]

An Ingress can be configured to give Services externally-reachable URLs, load 
balance traffic, terminate SSL / TLS, and offer name based virtual hosting. An 
Ingress controller is responsible for fulfilling the Ingress, usually with a 
load balancer, though it may also configure your edge router or additional 
frontends to help handle the traffic.

An Ingress does not expose arbitrary ports or protocols. Exposing services 
other than HTTP and HTTPS to the internet typically uses a service of type 
Service.Type=NodePort or Service.Type=LoadBalancer.


x.2 - Prerequisites
===================

You must have an ingress controller to satisfy an Ingress. Only creating an 
Ingress resource has no effect.

You may need to deploy an Ingress controller such as ingress-nginx. You can 
choose from a number of Ingress controllers.

Ideally, all Ingress controllers should fit the reference specification. In 
reality, the various Ingress controllers operate slightly differently.

    Note: Make sure you review your Ingress controller’s documentation to 
        understand the caveats of choosing it.


x.3 - The Ingress Resource
==========================

A minimal Ingress resource example:

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /testpath
        backend:
          serviceName: test
          servicePort: 80

As with all other Kubernetes resources, an Ingress needs apiVersion, kind, and 
metadata fields. For general information about working with config files, see 
deploying applications, configuring containers, managing resources. Ingress 
frequently uses annotations to configure some options depending on the Ingress 
controller, an example of which is the rewrite-target annotation. Different 
Ingress controller support different annotations. Review the documentation for 
your choice of Ingress controller to learn which annotations are supported.

The Ingress spec has all the information needed to configure a load balancer or 
proxy server. Most importantly, it contains a list of rules matched against all 
incoming requests. Ingress resource only supports rules for directing HTTP 
traffic.


x.4 - Ingress rules
===================

Each HTTP rule contains the following information:

    - An optional host. In this example, no host is specified, so the rule 
      applies to all inbound HTTP traffic through the IP address specified. If 
      a host is provided (for example, foo.bar.com), the rules apply to that 
      host.
    - A list of paths (for example, /testpath), each of which has an associated 
      backend defined with a serviceName and servicePort. Both the host and 
      path must match the content of an incoming request before the load 
      balancer directs traffic to the referenced Service.
    - A backend is a combination of Service and port names as described in the 
      Service doc. HTTP (and HTTPS) requests to the Ingress that matches the 
      host and path of the rule are sent to the listed backend.

A default backend is often configured in an Ingress controller to service any 
requests that do not match a path in the spec.


Default Backend
===============

An Ingress with no rules sends all traffic to a single default backend. The 
default backend is typically a configuration option of the Ingress controller 
and is not specified in your Ingress resources.

If none of the hosts or paths match the HTTP request in the Ingress objects, 
the traffic is routed to your default backend.


x.5 - Types of Ingress
======================

Single Service Ingress
======================

There are existing Kubernetes concepts that allow you to expose a single 
Service (see alternatives). You can also do this with an Ingress by specifying 
a default backend with no rules.

file: service/networking/ingress.yaml

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: test-ingress
spec:
  backend:
    serviceName: testsvc
    servicePort: 80

If you create it using kubectl apply -f you should be able to view the state of 
the Ingress you just added:

$ kubectl get ingress test-ingress
NAME           HOSTS     ADDRESS           PORTS     AGE
test-ingress   *         107.178.254.228   80        59s

Where 107.178.254.228 is the IP allocated by the Ingress controller to satisfy 
this Ingress.

    Note: Ingress controllers and load balancers may take a minute or two to 
        allocate an IP address. Until that time, you often see the address 
        listed as <pending>.

Simple fanout
=============

A fanout configuration routes traffic from a single IP address to more than one 
Service, based on the HTTP URI being requested. An Ingress allows you to keep 
the number of load balancers down to a minimum. For example, a setup like:

foo.bar.com -> 178.91.123.132 -> / foo    service1:4200
                                 / bar    service2:8080

would require an Ingress such as:

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: simple-fanout-example
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /foo
        backend:
          serviceName: service1
          servicePort: 4200
      - path: /bar
        backend:
          serviceName: service2
          servicePort: 8080

When you create the Ingress with kubectl apply -f:

$ kubectl describe ingress simple-fanout-example
Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test

The Ingress controller provisions an implementation-specific load balancer that 
satisfies the Ingress, as long as the Services (service1, service2) exist. When 
it has done so, you can see the address of the load balancer at the Address 
field.

    Note: Depending on the Ingress controller you are using, you may need to 
        create a default-http-backend Service.

Name based virtual hosting
==========================

Name-based virtual hosts support routing HTTP traffic to multiple host names at 
the same IP address.

foo.bar.com --|                 |-> foo.bar.com service1:80
              | 178.91.123.132  |
bar.foo.com --|                 |-> bar.foo.com service2:80

The following Ingress tells the backing load balancer to route requests based 
on the Host header.

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: bar.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80

If you create an Ingress resource without any hosts defined in the rules, then 
any web traffic to the IP address of your Ingress controller can be matched 
without a name based virtual host being required.

For example, the following Ingress resource will route traffic requested for 
first.bar.com to service1, second.foo.com to service2, and any traffic to the 
IP address without a hostname defined in request (that is, without a request 
header being presented) to service3.

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: name-virtual-host-ingress
spec:
  rules:
  - host: first.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
  - host: second.foo.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
  - http:
      paths:
      - backend:
          serviceName: service3
          servicePort: 80

TLS
===

You can secure an Ingress by specifying a Secret that contains a TLS private 
key and certificate. Currently the Ingress only supports a single TLS port, 
443, and assumes TLS termination. If the TLS configuration section in an 
Ingress specifies different hosts, they are multiplexed on the same port 
according to the hostname specified through the SNI TLS extension (provided 
the Ingress controller supports SNI). The TLS secret must contain keys named 
tls.crt and tls.key that contain the certificate and private key to use for 
TLS. For example:

apiVersion: v1
kind: Secret
metadata:
  name: testsecret-tls
  namespace: default
data:
  tls.crt: base64 encoded cert
  tls.key: base64 encoded key
type: kubernetes.io/tls

Referencing this secret in an Ingress tells the Ingress controller to secure 
the channel from the client to the load balancer using TLS. You need to make 
sure the TLS secret you created came from a certificate that contains a Common 
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for 
sslexample.foo.com.

apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: tls-example-ingress
spec:
  tls:
  - hosts:
    - sslexample.foo.com
    secretName: testsecret-tls
  rules:
    - host: sslexample.foo.com
      http:
        paths:
        - path: /
          backend:
            serviceName: service1
            servicePort: 80

    Note: There is a gap between TLS features supported by various Ingress 
        controllers. Please refer to documentation on nginx, GCE, or any other 
        platform specific Ingress controller to understand how TLS works in 
        your environment.

Loadbalancing
=============

An Ingress controller is bootstrapped with some load balancing policy settings 
that it applies to all Ingress, such as the load balancing algorithm, backend 
weight scheme, and others. More advanced load balancing concepts (e.g. 
persistent sessions, dynamic weights) are not yet exposed through the Ingress. 
You can instead get these features through the load balancer used for a 
Service.

It’s also worth noting that even though health checks are not exposed directly 
through the Ingress, there exist parallel concepts in Kubernetes such as 
readiness probes that allow you to achieve the same end result. Please review 
the controller specific documentation to see how they handle health checks 
(nginx, GCE).


x.6 - Updating an Ingress
=========================

To update an existing Ingress to add a new Host, you can update it by editing 
the resource:

$ kubectl describe ingress test
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test

kubectl edit ingress test

This pops up an editor with the existing configuration in YAML format. Modify 
it to include the new Host:

spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - backend:
          serviceName: service1
          servicePort: 80
        path: /foo
  - host: bar.baz.com
    http:
      paths:
      - backend:
          serviceName: service2
          servicePort: 80
        path: /foo
..

After you save your changes, kubectl updates the resource in the API server, 
which tells the Ingress controller to reconfigure the load balancer.

Verify this:

$ kubectl describe ingress test
Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test

You can achieve the same outcome by invoking kubectl replace -f on a modified 
Ingress YAML file.


x.7 - Failing across availability zones
=======================================

Techniques for spreading traffic across failure domains differs between cloud 
providers. Please check the documentation of the relevant Ingress controller 
for details. You can also refer to the federation documentation for details on 
deploying Ingress in a federated cluster.

